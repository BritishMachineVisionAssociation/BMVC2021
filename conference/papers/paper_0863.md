---
layout: default_paper
id: 863
order: 98
poster_session: 2
session_id: 5
title: "Learning Non-Parametric Invariances from Data with Permanent Random Connectomes"
authors:
  - author: "Dipan Pal (Carnegie Mellon University)"
  - author: "Akshay Chawla (CMU)"
  - author: "Marios Savvides (Carnegie Mellon University)"
all_authors: "Dipan Pal, Akshay Chawla and Marios Savvides"
code: ""
keywords:
  - word: "random connections"
  - word: "invariant features"
  - word: "permanent random connectomes"
  - word: "prcns"
  - word: "nptns"
  - word: "learning invariances"
  - word: "bio-inspired networks"
paper: "papers/0863.pdf"
supp: "supp/0863_supp.zip"
abstract: "Learning non-parametric invariances directly from data remains an important open problem. In this paper, we introduce a new architectural layer for convolutional networks which is capable of learning general invariances from data itself. This layer can learn invariance to non-parametric transformations and interestingly, motivates and incorporates permanent random connectomes, thereby being called Permanent Random Connectome Non-Parametric Transformation Networks (PRC-NPTN). PRC-NPTN networks are initialized with random connections (not just weights) which are a small subset of the connections in a fully connected convolution layer. Importantly, these connections in PRC-NPTNs once initialized remain permanent throughout training and testing.  Permanent random connectomes make these architectures loosely more biologically plausible than many other mainstream network architectures which require highly ordered structures. We motivate randomly initialized connections as a simple method to learn invariance from data itself while invoking invariance towards multiple nuisance transformations simultaneously. We find that these randomly initialized permanent connections have positive effects on generalization, outperform much larger ConvNet baselines and the recently proposed Non-Parametric Transformation Network (NPTN) on benchmarks such as augmented MNIST, ETH-80 and CIFAR10, that enforce learning invariances from the data itself."
slides-id: 38934042
---
