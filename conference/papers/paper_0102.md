---
layout: default_paper
id: 102
order: 4
oral_session: 2
poster_session: 1
session_id: 1
title: "Cross-Modal Hierarchical Modelling for Fine-Grained Sketch Based Image Retrieval"
authors:
  - author: "Aneeshan Sain (University of Surrey)"
  - author: "Ayan Kumar Bhunia  (University of Surrey)"
  - author: "Yongxin Yang (University of Surrey)"
  - author: "Tao Xiang (University of Surrey)"
  - author: "Yi-Zhe Song (University of Surrey)"
all_authors: "Aneeshan Sain, Ayan Kumar Bhunia, Yongxin Yang, Tao Xiang and Yi-Zhe Song"
code: ""
keywords:
  - word: "cross-modal co-attention"
  - word: "sketch hierarchy"
  - word: "cross-modal retrieval"
  - word: "sketch based image retrieval"
paper: "papers/0102.pdf"
supp: ""
abstract: "Sketch as an image search query is an ideal alternative to text in capturing the fine-grained visual details. Prior successes on fine-grained sketch-based image retrieval (FG-SBIR) have demonstrated the importance of tackling the unique traits of sketches as opposed to photos, e.g., temporal vs. static, strokes vs. pixels, and abstract vs. pixel-perfect. In this paper, we study a further trait of sketches that has been overlooked to date, that is, they are hierarchical in terms of the levels of detail -- a person typically sketches up to various extents of detail to depict an object. This hierarchical structure is often visually distinct. In this paper,  we design a novel network that is capable of cultivating sketch-specific hierarchies and exploiting them to match sketch with photo at corresponding hierarchical levels. In particular, features from a sketch and a photo are enriched using cross-modal co-attention, coupled with hierarchical node fusion at every level to form a better embedding space to conduct retrieval. Experiments on common benchmarks show our method to outperform state-of-the-arts by a significant margin."
slides-id: 38933889
---
