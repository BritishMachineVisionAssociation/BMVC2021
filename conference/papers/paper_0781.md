---
layout: default_paper
id: 781
order: 116
poster_session: 3
session_id: 8
title: "MagnifierNet: Towards Semantic Adversary and Fusion for Person Re-identification"
authors:
  - author: "Yushi Lan (Nanyang Technological University )"
  - author: "Yuan Liu (Nanyang Technological University)"
  - author: "xinchi zhou (Sensetime Limited)"
  - author: "Tian Maoqing (Sensetime Limited)"
  - author: "Xuesen Zhang (SenseTime)"
  - author: "Shuai Yi (SenseTime Group Limited)"
  - author: "Hongsheng Li (Chinese University of Hong Kong)"
all_authors: "Yushi Lan, Yuan Liu, Xinchi Zhou, Tian Maoqing, Xuesen Zhang, Shuai Yi and Hongsheng Li"
code: "https://github.com/NIRVANALAN/magnifiernet_reid"
keywords:
  - word: "person re-identification"
  - word: "adversarial samples"
  - word: "metric learning"
  - word: "multi-task learning"
  - word: "image retrieval"
paper: "papers/0781.pdf"
supp: ""
abstract: "Although person re-identification (ReID) has achieved significant improvement recently by enforcing part alignment, it is still a challenging task when it comes to distinguishing visually similar identities or identifying the occluded person. In these scenarios, magnifying details in each part features and selectively fusing them together may provide a feasible solution. In this work, we propose MagnifierNet, a triple-branch network which accurately mines details from whole to parts. Firstly, the holistic salient features are encoded by a global branch. Secondly, to enhance detailed representation for each semantic region, the \"Semantic Adversarial Branch\" is designed to learn from dynamically generated semantic-occluded samples during training. Meanwhile, we introduce \"Semantic Fusion Branch\" to filter out irrelevant noises by selectively fusing semantic region information sequentially. To further improve feature diversity, we introduce a novel loss function \"Semantic Diversity Loss\" to remove redundant overlaps across learned semantic representations. State-of-the-art performance has been achieved on three benchmarks by large margins. Specifically, the mAP score is improved by 6% and 5% on the most challenging CUHK03-L and CUHK03-D benchmarks."
slides-id: 38934030
channel-id: "paper_116_P3_id_0781"
---
