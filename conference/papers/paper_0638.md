---
layout: default_paper
id: 638
order: 155
oral_session: 8
poster_session: 4
session_id: 10
title: "Video Region Annotation with Sparse Bounding Boxes"
authors:
  - author: "Yuzheng Xu (Kyoto University)"
  - author: "Yang Wu (Kyoto University)"
  - author: "Nur Sabrina binti Zuraimi (Kyoto University)"
  - author: "Shohei Nobuhara (Kyoto University)"
  - author: "Ko Nishino (Kyoto University)"
all_authors: "Yuzheng Xu, Yang Wu, Nur Sabrina binti Zuraimi, Shohei Nobuhara and Ko Nishino"
code: ""
keywords:
  - word: "video annotation"
  - word: "semi-automatic annotation"
  - word: "graph convolutional network"
  - word: "region boundaries"
  - word: "sparse bounding boxes"
  - word: "automatic boundary finding"
  - word: ""
paper: "papers/0638.pdf"
supp: "supp/0638_supp.zip"
abstract: "Video analysis has been moving towards more detailed interpretation (e.g. segmentation) with encouraging progresses. These tasks, however, increasingly rely on densely annotated training data both in space and time. Since such annotation is labour-intensive, few densely annotated video data with detailed region boundaries exist. This work aims to resolve this dilemma by learning to automatically generate region boundaries for all frames of a video from sparsely annotated bounding boxes of target regions. We achieve this with a Volumetric Graph Convolutional Network (VGCN), which learns to iteratively find keypoints on the region boundaries using the spatio-temporal volume of surrounding appearance and motion. The global optimization of VGCN makes it significantly stronger and generalize better than existing solutions. Experimental results using two latest datasets (one real and one synthetic), including ablation studies, demonstrate the effectiveness and superiority of our method."
slides-id: 38934010
---
