papers:
  - id: 5
    order: 10
    poster_session: 1
    session_id: 2
    title: "Foreground Mining via Contrastive Guidance for Weakly Supervised Object Localization"
    authors:
      - author: "Wonyoung Lee (Yonsei University)"
      - author: "Minsong Ki (LG Uplus)"
      - author: "Cheolhyun Mun (Yonsei university)"
      - author: "Sungpil Kho (Yonsei University)"
      - author: "Hyeran Byun (Yonsei University)"
    all_authors: "Wonyoung Lee, Minsong Ki, Cheolhyun Mun, Sungpil Kho and Hyeran Byun"
    code: "https://github.com/lwy8555/FMCG"
    keywords:
      - word: "weakly supervised object localization"
      - word: ""
    paper: "papers/0005.pdf"
    supp: "supp/0005_supp.zip"
    abstract: "Weakly supervised object localization (WSOL) locates the target object within an image using only image-level labels. Recent methods try to extend the feature activation to cover entire object regions by dropping the most discriminative parts. However, they either overextend the activation into the background or are still limited to covering the most discriminative parts. In this paper, we propose a novel WSOL framework that localizes the entire object to the right extent via contrastive learning. Our framework contains three key components: 1) scheduled region drop, 2) contrastive guidance, and 3) pairwise non-local block. The scheduled region drop progressively erases the most discriminative parts of the original feature at a region-level. The erased feature facilitates the network to discover less discriminative regions in the original feature. Then, our contrastive guidance encourages the foregrounds of the original and erased features to be closer while pushing away from each background. In this manner, the network earns the capacity to differentiate the foregrounds from backgrounds, spreading out the activation within object regions. Last but not least, we utilize the pairwise non-local block, which provides an enhanced attention map to strengthen the spatial correlations between each pixel. In conclusion, our method achieves the state-of-the-art performance on CUB-200-2011 and ImageNet benchmarks regarding Top-1 Loc, GT-Loc and MaxBoxAccV2."
  - id: 8
    order: 119
    poster_session: 2
    session_id: 5
    title: "HCV: Hierarchy-Consistency Verification for Incremental Implicitly-Refined Classification"
    authors:
      - author: "kai wang (Computer Vision Center)"
      - author: "Xialei Liu (Nankai University)"
      - author: "Luis Herranz (Computer Vision Center)"
      - author: "Joost van de Weijer (Computer Vision Center)"
    all_authors: "Kai Wang, Xialei Liu, Luis Herranz and Joost van de Weijer"
    code: "https://github.com/wangkai930418/HCV_IIRC"
    keywords:
      - word: "continual learning"
      - word: "multi-label classification"
      - word: "hierarchical classification"
      - word: ""
    paper: "papers/0008.pdf"
    supp: "supp/0008_supp.zip"
    abstract: "Human beings learn and accumulate hierarchical knowledge over their lifetime. This knowledge is associated with previous concepts for consolidation and hierarchical construction. However, current incremental learning methods lack the ability to build a concept hierarchy by associating new concepts to old ones. A more realistic setting tackling this problem is referred to as Incremental Implicitly-Refined Classification (IIRC), which simulates the recognition process from coarse-grained categories to fine-grained categories. To overcome forgetting in this benchmark, we propose Hierarchy-Consistency Verification (HCV) as an enhancement to existing continual learning methods. Our method incrementally discovers the hierarchical relations between classes. We then show how this knowledge can be exploited during both training and inference. Experiments on three setups of varying difficulty demonstrate that our HCV module improves performance of existing continual learning methods under this IIRC setting by a large margin."
  - id: 12
    order: 11
    poster_session: 1
    session_id: 2
    title: "Learning to Hide Residual for Boosting Image Compression"
    authors:
      - author: "Yi-Lun Lee (National Chiao Tung University)"
      - author: "Yen-Chung Chen (National Chiao Tung University)"
      - author: "Min-Yuan Tseng (National Chiao Tung University)"
      - author: "Yi-Hsuan Tsai (Phiar Technologies)"
      - author: "Wei-Chen Chiu (National Chiao Tung University)"
    all_authors: "Yi-Lun Lee, Yen-Chung Chen, Min-Yuan Tseng, Yi-Hsuan Tsai and Wei-Chen Chiu"
    code: ""
    keywords:
      - word: "image compression"
      - word: "image restoration"
      - word: "steganography"
    paper: "papers/0012.pdf"
    supp: "supp/0012_supp.zip"
    abstract: "Lossy compression usually leads to severe compression artifacts, such as blocking boundary, mosquito noise, and blur. Reducing compression artifacts is essential for better visual experience and quality when transmitting data under limited bandwidth, where the sender compresses an image and transmits it via a communication channel to the receiver side. To tackle this problem, most existing methods aim to directly recover details from received compressed image, instead of fully exploiting the rich information contained in the uncompressed image. In this paper, we focus on leveraging the residual information, i.e. the difference between a compressed image and its corresponding original/uncompressed one, and propose to hide the residual into the original image by a novel framework. As such, our model that considers this resultant image with the hidden information has a better ability to recover the residual caused by the compression process. Afterwards, the hidden residual could be decoded from the received image and used to boost the quality of image reconstruction on the receiver side. Extensive experiments verify the efficacy of our proposed framework in reducing compression artifacts and showing favorable performance against numerous baselines."
  - id: 14
    order: 113
    oral_session: 3
    poster_session: 2
    session_id: 3
    title: "UNIK: A Unified Framework for Real-world Skeleton-based Action Recognition"
    authors:
      - author: "Di Yang (INRIA)"
      - author: "Yaohui Wang (INRIA)"
      - author: "Antitza  Dantcheva (INRIA)"
      - author: "Lorenzo Garattoni (Toyota-Europe)"
      - author: "Gianpiero Francesca (Toyota-Europe)"
      - author: "Francois Bremond (Inria Sophia Antipolis, France)"
    all_authors: "Di Yang, Yaohui Wang, Antitza  Dantcheva, Lorenzo Garattoni, Gianpiero Francesca and Francois Bremond"
    code: "https://github.com/YangDi666/UNIK"
    keywords:
      - word: "deep learning"
      - word: "video understanding"
      - word: "action recognition"
      - word: "skeleton"
      - word: "2D pose"
      - word: "3D pose"
      - word: "graph convolution"
      - word: "attention"
      - word: "real-world"
      - word: "dataset"
    paper: "papers/0014.pdf"
    supp: "supp/0014_supp.zip"
    abstract: "Action recognition based on skeleton data has recently witnessed increasing attention and progress. State-of-the-art approaches adopting Graph Convolutional networks (GCNs) can effectively extract features on human skeletons relying on the pre-defined human topology. Despite associated progress, GCN-based methods have difficulties to generalize across domains, especially with different human topological structures. In this context, we introduce UNIK, a novel topology-free skeleton-based action recognition method that is not only effective to learn spatio-temporal features on human skeleton sequences but also able to generalize across datasets. This is achieved by learning an optimal dependency matrix from the uniform distribution based on a multi-head attention mechanism. Subsequently, to study the cross-domain generalizability of skeleton-based action recognition in real-world videos, we re-evaluate state-of-the-art approaches as well as the proposed UNIK in light of a novel Posetics dataset. This dataset is created from Kinetics-400 videos by estimating, refining and filtering poses. We provide an analysis on how much performance improves on the smaller benchmark datasets after pre-training on Posetics for the action classification task. Experimental results show that the proposed UNIK, with pre-training on Posetics, generalizes well and outperforms state-of-the-art when transferred onto four target action classification datasets: Toyota Smarthome, Penn Action, NTU-RGB+D 60 and NTU-RGB+D 120."
  - id: 16
    order: 336
    poster_session: 4
    session_id: 11
    title: "TransFusion: Cross-view Fusion with Transformer for 3D Human Pose Estimation"
    authors:
      - author: "Haoyu Ma (University of California, Irvine)"
      - author: "Liangjian Chen (University of California, Irvine)"
      - author: "Deying Kong (university of california, irvine)"
      - author: "Zhe Wang (UC-Irvine)"
      - author: "Xingwei Liu (University of California Irvine)"
      - author: "Hao Tang (University of California Irvine)"
      - author: "Xiangyi Yan (University of California, Irvine)"
      - author: "Yusheng Xie (Amazon)"
      - author: "Shih-Yao Lin (Sony Corporation of America)"
      - author: "Xiaohui Xie (University of California, Irvine)"
    all_authors: "Haoyu Ma, Liangjian Chen, Deying Kong, Zhe Wang, Xingwei Liu, Hao Tang, Xiangyi Yan, Yusheng Xie, Shih-Yao Lin and Xiaohui Xie"
    code: "https://github.com/HowieMa/TransFusion-Pose"
    keywords:
      - word: "multi-view"
      - word: "3D pose estimation"
      - word: "epipolar line"
      - word: "vision transformer"
      - word: ""
    paper: "papers/0016.pdf"
    supp: ""
    abstract: "Estimating the 2D human poses in each view is typically the first step in calibrated multi-view 3D pose estimation. But the performance of 2D pose detectors suffers from challenging situations such as occlusions and oblique viewing angles. To address these challenges, previous works derive point-to-point correspondences between different views from epipolar geometry and utilize the correspondences to merge prediction heatmaps or feature representations. Instead of post-prediction merge/calibration, here we introduce a transformer framework for multi-view 3D pose estimation, aiming at directly improving individual 2D predictors by integrating information from different views. Inspired by previous multi-modal transformers, we design a unified transformer architecture, named TransFusion, to fuse cues from both current views and neighboring views. Moreover, we propose the concept of epipolar field to encode 3D positional information into the transformer model.  The 3D position encoding guided by epipolar field provides an efficient way of encoding correspondences between pixels of different views. Experiments on Human 3.6M and Ski-Pose show that our method is more efficient and has consistent improvements compared to other fusion methods. Specifically, we achieve 25.8 mm MPJPE on Human 3.6M with only 5M parameters on 256 x 256 resolution. Source code and trained model can be found at: https://github.com/HowieMa/TransFusion-Pose. "
  - id: 31
    order: 228
    poster_session: 3
    session_id: 8
    title: "Mask-aware IoU for Anchor Assignment in Real-time Instance Segmentation"
    authors:
      - author: "Kemal Oksuz (Middle East Technical University)"
      - author: "Baris Can Cam (Middle East Technical University)"
      - author: "Fehmi Kahraman (Middle East Technical University)"
      - author: "Zeynep S Baltacı (Middle East Technical University)"
      - author: "Sinan Kalkan (Middle East Technical University)"
      - author: "Emre Akbas (Middle East Technical University)"
    all_authors: "Kemal Oksuz, Baris Can Cam, Fehmi Kahraman, Zeynep S Baltacı, Sinan Kalkan and Emre Akbas"
    code: "https://github.com/kemaloksuz/Mask-aware-IoU"
    keywords:
      - word: "instance segmentation"
      - word: "real time"
      - word: "anchor assignment"
      - word: "object detection"
      - word: ""
    paper: "papers/0031.pdf"
    supp: ""
    abstract: "This paper presents Mask-aware Intersection-over-Union (maIoU) for assigning anchor boxes as positives and negatives during training of instance segmentation methods. Unlike conventional IoU or its variants, which only considers the proximity of two boxes; maIoU consistently measures the proximity of an anchor box with not only a ground truth box but also its associated ground truth mask. Thus, additionally considering  the mask, which, in fact, represents the shape of the object, maIoU enables a more accurate supervision during training. We present the effectiveness of maIoU on a state-of-the-art (SOTA) assigner, ATSS, by replacing IoU operation by our maIoU and training YOLACT, a SOTA real-time instance segmentation method. Using ATSS with maIoU consistently outperforms (i) ATSS with IoU by ~1 mask AP, (ii) baseline YOLACT with fixed IoU threshold assigner by ~2 mask AP over different image sizes and (iii) decreases the inference time by 25% owing to using less anchors. Then, exploiting this efficiency, we devise maYOLACT, a faster and +6 AP more accurate detector than YOLACT. Our best model achieves 37.7 mask AP at 25 fps on COCO test-dev establishing a new state-of-the-art for real-time instance segmentation. Code is available at https://github.com/kemaloksuz/Mask-aware-IoU "
  - id: 35
    order: 229
    poster_session: 3
    session_id: 8
    title: "Continuous-Time Video Generation via Learning Motion Dynamics with Neural ODE"
    authors:
      - author: "Kangyeol Kim (KAIST)"
      - author: "Sunghyun  Park (KAIST)"
      - author: "Junsoo Lee (NAVER WEBTOON Ltd.)"
      - author: "Joonseok Lee (Google Research & Seoul National University)"
      - author: "Sookyung Kim (Lawrence Livermore National Laboratory)"
      - author: "Jaegul Choo (Korea Advanced Institute of Science and Technology)"
      - author: "Edward Choi (KAIST)"
    all_authors: "Kangyeol Kim, Sunghyun  Park, Junsoo Lee, Joonseok Lee, Sookyung Kim, Jaegul Choo and Edward Choi"
    code: ""
    keywords:
      - word: "video generation"
      - word: "generative model"
      - word: "neural ODE"
      - word: "motion estimation"
    paper: "papers/0035.pdf"
    supp: "supp/0035_supp.zip"
    abstract: "In order to perform unconditional video generation, we must learn the distribution of the real-world videos. In an effort to synthesize high-quality videos, various studies attempted to learn a mapping function between noise and videos, including recent efforts to separate motion distribution and appearance distribution. Previous methods, however, learn motion dynamics in discretized, fixed-interval timesteps, which is contrary to the continuous nature of motion of a physical body. In this paper, we propose a novel video generation approach that learns separate distributions for motion and appearance, the former modeled by neural ODE to learn natural motion dynamics. Specifically, we employ a two-stage approach where the first stage converts a noise vector to a sequence of keypoints in arbitrary frame rates, and the second stage synthesizes videos based on the given keypoints sequence and the appearance noise vector. Our model not only quantitatively outperforms recent baselines for video generation, but also demonstrates versatile functionality such as dynamic frame rate manipulation and motion transfer between two datasets, thus opening new doors to diverse video generation applications."
  - id: 38
    order: 12
    poster_session: 1
    session_id: 2
    title: "Learning Exposure Correction Via Consistency Modeling"
    authors:
      - author: "Ntumba Elie Nsampi (Northwestern Polytehcnical University)"
      - author: "Zhongyun Hu (Northwestern Polytechnical University)"
      - author: "Qing Wang (Northwestern Polytechnical University)"
    all_authors: "Ntumba Elie Nsampi, Zhongyun Hu and Qing Wang"
    code: "https://github.com/elientumba2019/Exposure-Correction-BMVC-2021"
    keywords:
      - word: "exposure correction"
      - word: "computational photography"
      - word: "image enhancement"
      - word: "over-exposure correction"
      - word: "under-exposure correction"
      - word: "deep learning"
    paper: "papers/0038.pdf"
    supp: "supp/0038_supp.zip"
    abstract: "Existing works on exposure correction have exclusively focused on either under-exposure or over-exposure. Recent work targeting both under-, and over-exposure achieved state of the art. However, it tends to produce images with inconsistent correction and sometimes color artifacts. In this paper, we propose a novel neural network architecture for exposure correction. The proposed network targets both under-, and over-exposure. We introduce a deep feature matching loss that enables the network to learn exposure-invariant representation in the feature space, which guarantees image exposure consistency. Moreover, we leverage a global attention mechanism to allow long-range interactions between distant pixels for exposure correction. This results in consistently corrected images, free of localized color distortions. Through extensive quantitative and qualitative experiments, we demonstrate that the proposed network outperforms the existing state-of-the-art. Our code will be made publicly available upon acceptance of the paper."
  - id: 43
    order: 13
    poster_session: 1
    session_id: 2
    title: "Approaching the Limit of Image Rescaling via Flow Guidance"
    authors:
      - author: "Shang Li (CASIA)"
      - author: "GuiXuan Zhang (CASIA)"
      - author: "Zhengxiong Luo (Institute of Automation，Chinese Academy of Sciences)"
      - author: "Jie Liu (CASIA)"
      - author: "Zhi Zeng (Institute of Automation, Chinese Academy of Sciences)"
      - author: "ShuWu Zhang (CASIA)"
    all_authors: "Shang Li, GuiXuan Zhang, Zhengxiong Luo, Jie Liu, Zhi Zeng and ShuWu Zhang"
    code: ""
    keywords:
      - word: "image rescaling"
      - word: "invertible flow"
      - word: "image super-resolution"
    paper: "papers/0043.pdf"
    supp: ""
    abstract: "Image downscaling and upscaling are two basic rescaling operations. Once the image is downscaled, it is difficult to be reconstructed via upscaling due to the loss of information. To make these two processes more compatible and improve the reconstruction performance, some efforts model them as a joint encoding-decoding task, with the constraint that the downscaled (ie encoded) low-resolution (LR) image must preserve the original visual appearance. To implement this constraint, most methods guide the downscaling module by supervising it with the bicubically downscaled LR version of the original high-resolution (HR) image. However, this bicubic LR guidance may be suboptimal for the subsequent upscaling (ie decoding) and restrict the final reconstruction performance. In this paper, instead of directly applying the LR guidance, we propose an additional invertible flow guidance module (FGM), which can transform the downscaled representation to the visually plausible image during downscaling and transform it back during upscaling. Benefiting from the invertibility of FGM, the downscaled representation could get rid of the LR guidance and would not disturb the downscaling-upscaling process. It allows us to remove the restrictions on the downscaling module and optimize the downscaling and upscaling modules in an end-to-end manner. In this way, these two modules could cooperate to maximize the HR reconstruction performance. Extensive experiments demonstrate that the proposed method can achieve state-of-the-art (SotA) performance on both downscaled and reconstructed images."
  - id: 44
    order: 337
    poster_session: 4
    session_id: 11
    title: "Text-Based Person Search with Limited Data"
    authors:
      - author: "Xiao Han (University of Surrey)"
      - author: "Sen He (University of Surrey)"
      - author: "Li Zhang (Fudan University)"
      - author: "Tao Xiang (University of Surrey)"
    all_authors: "Xiao Han, Sen He, Li Zhang and Tao Xiang"
    code: "https://github.com/BrandonHanx/TextReID"
    keywords:
      - word: "person re-identification"
      - word: "cross-modal image retrieval"
      - word: "fine-grained image retrieval"
      - word: "text-based person search"
    paper: "papers/0044.pdf"
    supp: "supp/0044_supp.zip"
    abstract: "Text-based person search (TBPS) aims at retrieving a target person from an image gallery with a descriptive text query.
Solving such a fine-grained cross-modal retrieval task is challenging, which is further hampered by the lack of large-scale datasets.
In this paper, we present a framework with two novel components to handle the problems brought by limited data. 
Firstly, to fully utilize the existing small-scale benchmarking datasets for more discriminative feature learning, we introduce a cross-modal momentum contrastive learning framework to enrich the training data for a given mini-batch. Secondly, we propose to transfer knowledge learned from existing coarse-grained large-scale datasets containing image-text pairs from drastically different problem domains to compensate for the lack of TBPS training data. A transfer learning method is designed so that useful information can be transferred despite the large domain gap.  Armed with these components, our method achieves new state of the art on the CUHK-PEDES dataset with significant improvements over the prior art in terms of Rank-1 and mAP. Our code is available at https://github.com/BrandonHanx/TextReID."
  - id: 51
    order: 120
    poster_session: 2
    session_id: 5
    title: "SuperStyleNet: Deep Image Synthesis with Superpixel Based Style Encoder"
    authors:
      - author: "Jonghyun Kim (Sungkyunkwan University)"
      - author: "Gen Li (Sungkyunkwan University)"
      - author: "Cheolkon Jung (Xidian University)"
      - author: "Joongkyu Kim (Sungkyunkwan University)"
    all_authors: "Jonghyun Kim, Gen Li, Cheolkon Jung and Joongkyu Kim"
    code: "https://github.com/BenjaminJonghyun/SuperStyleNet"
    keywords:
      - word: "image-to-image translation"
      - word: "semantic image synthesis"
      - word: "image generation"
      - word: "superpixel"
      - word: "style encoder"
      - word: "graph self-attention"
    paper: "papers/0051.pdf"
    supp: "supp/0051_supp.zip"
    abstract: "Existing methods for image synthesis utilized a style encoder based on stacks of convolutions and pooling layers to generate style codes from input images. However, the encoded vectors do not necessarily contain local information of the corresponding images since small-scale objects are tended to \"wash away\" through such downscaling procedures. In this paper, we propose deep image synthesis with superpixel based style encoder, named SuperStyleNet. First, we directly extract the style codes from the original image based on superpixels to consider local objects. Second, we recover spatial relationships in vectorized style codes based on graphical analysis. Thus, the proposed network achieves high-quality image synthesis by mapping the style codes into semantic labels. Experimental results show that the proposed method outperforms state-of-the-art ones in terms of visual quality and quantitative measurements. Furthermore, we achieve elaborate spatial style editing by adjusting style codes."
  - id: 54
    order: 230
    poster_session: 3
    session_id: 8
    title: "Human-object Interaction Detection without Alignment Supervision"
    authors:
      - author: "Mert Kilickaya (University of Amsterdam)"
      - author: "Arnold W.M. Smeulders (University of Amsterdam)"
    all_authors: "Mert Kilickaya and Arnold W.M. Smeulders"
    code: ""
    keywords:
      - word: "human-object interactions"
      - word: "visual relationship detection"
      - word: "weakly supervised learning"
      - word: "visual transformers"
    paper: "papers/0054.pdf"
    supp: "supp/0054_supp.zip"
    abstract: "The goal of this paper is Human-object Interaction (HO-I) detection. HO-I detection aims to find interacting human-objects regions and classify their interaction from an image. Researchers obtain significant improvement in recent years by relying on strong HO-I alignment supervision. HO-I alignment supervision pairs humans with their interacted objects, and then aligns human-object pair(s) with their interaction categories. Since collecting such annotation is expensive, in this paper, we propose to detect HO-I without alignment supervision. We instead rely on image-level supervision that only enumerates existing interactions within the image without pointing where they happen. Our paper makes three contributions: 1. We propose Align-Former, a visual-transformer based CNN that can detect HO-I with only image-level supervision. 2. Align-Former is equipped with HO-I align layer, that can learn to select appropriate targets to allow detector supervision. 3. We evaluate Align-Former on HICO-DET and V-COCO, and show that Align-Former outperforms existing image-level supervised HO-I detectors by a large margin (4.71 mAP improvement from 16.14 to 20.85 on HICO-DET). "
  - id: 56
    order: 231
    poster_session: 3
    session_id: 8
    title: "Segmenting Invisible Moving Objects"
    authors:
      - author: "Hala Lamdouar (University of Oxford)"
      - author: "Weidi Xie (University of Oxford)"
      - author: "Andrew Zisserman (University of Oxford)"
    all_authors: "Hala Lamdouar, Weidi Xie and Andrew Zisserman"
    code: "https://www.robots.ox.ac.uk/~vgg/research/simo/"
    keywords:
      - word: "synthetic data generation"
      - word: "motion segmentation"
      - word: "amodal segmentation"
      - word: "video camouflage breaking"
      - word: "self-attention"
    paper: "papers/0056.pdf"
    supp: "supp/0056_supp.zip"
    abstract: "Biological visual systems are exceptionally good at perceiving objects that undergo changes in appearance, pose, and position. In this paper, we aim to train a computational model with similar functionality to segment the moving objects in videos. We target the challenging cases when objects are ``invisible'' in the RGB video sequence, for example, breaking  camouflage, where visual appearance from a static scene can barely provide informative cues, or locating the objects as a whole even under partial occlusion. 
To this end, we make the following contributions: (i) In order to train a motion segmentation model, we propose a scalable pipeline for generating synthetic training data, significantly reducing the requirements for labour-intensive annotations; (ii) We introduce a dual-head architecture (hybrid of ConvNets and Transformer) that takes a sequence of optical flows as input, and learns to segment the moving objects even when they are partially occluded or stop moving at certain points in videos; (iii) We conduct thorough ablation studies to analyse the critical components in data simulation, and validate the necessity of Transformer layers for aggregating temporal information and for developing object permanence. When evaluating on the MoCA camouflage dataset, the model trained only on synthetic data demonstrates state-of-the-art segmentation performance, even outperforming strong supervised approaches. In addition, we also evaluate on the popular benchmarks DAVIS2016 and SegTrackv2, and show competitive performance despite only processing optical flow."
  - id: 57
    order: 338
    poster_session: 4
    session_id: 11
    title: "PartGAN: Unsupervised Part Decomposition for Image Generation and Segmentation"
    authors:
      - author: "Yuheng Li (UW Madison)"
      - author: "Krishna Kumar Singh (Adobe Research)"
      - author: "Yang Xue (University of California, Davis)"
      - author: "Yong Jae Lee (University of Wisconsin-Madison)"
    all_authors: "Yuheng Li, Krishna Kumar Singh, Yang Xue and Yong Jae Lee"
    code: ""
    keywords:
      - word: "part decomposition"
      - word: "image generation"
      - word: "weakly supervised"
    paper: "papers/0057.pdf"
    supp: "supp/0057_supp.zip"
    abstract: "We propose PartGAN, a novel generative model that disentangles and generates background, object shape, object texture, and decomposes objects into parts without any mask or part annotations. To achieve object-level disentanglement, we build upon prior work and maximize the mutual information between the generated factors and sampled latent prior codes. To achieve part-level decomposition, we learn a part generator, which decomposes an object into parts that are spatially localized and disjoint, and consistent across instances. Extensive experiments on multiple datasets demonstrate that PartGAN discovers consistent object parts, which enable part-based controllable image generation."
  - id: 59
    order: 14
    poster_session: 1
    session_id: 2
    title: "Gradient Frequency Modulation for Visually Explaining Video Understanding Models"
    authors:
      - author: "Xin Miao Lin (University of Massachusetts Amherst)"
      - author: "Wentao Bao (Rochester Institute of Technology)"
      - author: "Matthew Wright (Rochester Institute of Technology)"
      - author: "Yu Kong (Rochester Institute of Technology)"
    all_authors: "Xin Miao Lin, Wentao Bao, Matthew Wright and Yu Kong"
    code: ""
    keywords:
      - word: "model explanation"
      - word: "model explainability"
      - word: "explainable AI"
      - word: "video action recognition"
      - word: "Discrete Fourier Transform"
      - word: "video perturbation"
      - word: "interpretable machine learning"
      - word: "video model explanation"
      - word: "frequency modulation"
      - word: "spatiotemporal consistency"
      - word: ""
    paper: "papers/0059.pdf"
    supp: "supp/0059_supp.zip"
    abstract: "In many applications, it is essential to understand why a machine learning model makes the decisions it does, but this is inhibited by the black-box nature of state-of-the-art neural networks. Because of this, increasing attention has been paid to explainability in deep learning, including in the area of video understanding. Due to the temporal dimension of video data, the main challenge of explaining a video action recognition model is to produce spatiotemporally consistent visual explanations, which has been ignored in the existing literature. In this paper, we propose Frequency-based Extremal Perturbation (F-EP) to explain a video understanding model's decisions. Because the explanations given by perturbation methods are noisy and non-smooth both spatially and temporally, we propose to modulate the frequencies of gradient maps from the neural network model with a Discrete Cosine Transform (DCT). We show in a range of experiments that F-EP provides more spatiotemporally consistent explanations that more faithfully represent the model's decisions compared to the existing state-of-the-art methods."
  - id: 63
    order: 121
    poster_session: 2
    session_id: 5
    title: "UBR²S: Uncertainty-Based Resampling and Reweighting Strategy for Unsupervised Domain Adaptation"
    authors:
      - author: "Tobias Ringwald (Karlsruhe Institute of Technology)"
      - author: "Rainer Stiefelhagen (Karlsruhe Institute of Technology)"
    all_authors: "Tobias Ringwald and Rainer Stiefelhagen"
    code: "https://gitlab.com/tringwald/UBR2S"
    keywords:
      - word: "domain adaptation"
      - word: "transfer learning"
      - word: "unsupervised domain adaptation"
      - word: "synthetic to real"
      - word: "uncertainty"
      - word: "Monte Carlo dropout"
    paper: "papers/0063.pdf"
    supp: "supp/0063_supp.zip"
    abstract: "Unsupervised domain adaptation (UDA) deals with the adaptation process of a model to an unlabeled target domain while annotated data is only available for a given source domain. This poses a challenging task, as the domain shift between source and target instances deteriorates a model's performance when not addressed. In this paper, we propose UBR²S – the Uncertainty-Based Resampling and Reweighting Strategy – to tackle this problem. UBR²S employs a Monte Carlo dropout-based uncertainty estimate to obtain per-class probability distributions, which are then used for dynamic resampling of pseudo-labels and reweighting based on their sample likelihood and the accompanying decision error. Our proposed method achieves state-of-the-art results on multiple UDA datasets with single and multi-source adaptation tasks and can be applied to any off-the-shelf network architecture. Code for our method is available at https://gitlab.com/tringwald/UBR2S."
  - id: 68
    order: 122
    poster_session: 2
    session_id: 5
    title: "Repaint: Improving the Generalization of Down-Stream Visual Tasks by Generating Multiple Instances of Training Examples"
    authors:
      - author: "Amin Banitalebi-Dehkordi (Huawei Technologies Canada Co., Ltd.)"
      - author: "Yong Zhang (Huawei Technologies Canada Co., Ltd.)"
    all_authors: "Amin Banitalebi-Dehkordi and Yong Zhang"
    code: ""
    keywords:
      - word: "Texture Bias"
      - word: "Repaint"
      - word: "Image Generation"
      - word: "Semantic Synthesis"
      - word: "Down-Stream Task"
      - word: "VAE-GAN"
      - word: ""
    paper: "papers/0068.pdf"
    supp: "supp/0068_supp.zip"
    abstract: "Convolutional Neural Networks (CNNs) for visual tasks are believed to learn both the low-level textures and high-level object attributes, throughout the network depth. This paper further investigates the 'texture bias' in CNNs. To this end, we regenerate multiple instances of training examples from each original image, through a process we call 'repainting'. The repainted examples preserve the shape and structure of the regions and objects within the scenes, but diversify their texture and color. Our method can regenerate a same image at different daylight, season, or weather conditions, can have colorization or de-colorization effects, or even bring back some texture information from blacked-out areas. The in-place repaint allows us to further use these repainted examples for improving the generalization of CNNs. Through an extensive set of experiments, we demonstrate the usefulness of the repainted examples in training, for the tasks of image classification (ImageNet) and object detection (COCO), over several state-of-the-art network architectures at different capacities, and across different data availability regimes. "
  - id: 69
    order: 15
    poster_session: 1
    session_id: 2
    title: "Semi-Supervised Raw-to-Raw Mapping"
    authors:
      - author: "Mahmoud Afifi (Apple)"
      - author: "Abdullah Abuolaim (York University)"
    all_authors: "Mahmoud Afifi and Abdullah Abuolaim"
    code: ""
    keywords:
      - word: "raw-to-raw mapping"
      - word: "spectral sensitivity"
      - word: "camera sensor"
      - word: "camera ISP"
      - word: "color mapping"
    paper: "papers/0069.pdf"
    supp: ""
    abstract: "The raw-RGB colors of a camera sensor vary due to the spectral sensitivity differences across different sensor makes and models. This paper focuses on the task of mapping between different sensor raw-RGB color spaces. Prior work addressed this problem using a pairwise calibration to achieve accurate color mapping. Although being accurate, this approach is less practical as it requires: (1) capturing pair of images by both camera devices with a color calibration object placed in each new scene; (2) accurate image alignment or manual annotation of the color calibration object. This paper aims to tackle color mapping in the raw space through a more practical setup. Specifically, we present a semi-supervised raw-to-raw mapping method trained on a small set of paired images alongside an unpaired set of images captured by each camera device. Through extensive experiments, we show that our method achieves better results compared to other domain adaptation alternatives in addition to the single-calibration solution. We have generated a new dataset of raw images from two different smartphone cameras as part of this effort. Our dataset includes unpaired and paired sets for our semi-supervised training and evaluation. "
  - id: 79
    order: 339
    poster_session: 4
    session_id: 11
    title: "Each Attribute Matters: Contrastive Attention for Sentence-based Image Editing"
    authors:
      - author: "Liuqing Zhao (Suzhou University of Science and Technology	)"
      - author: "Fan Lyu (College of Intelligence and Computing, Tianjin University)"
      - author: "Fuyuan Hu (Suzhou University of Science and Technology)"
      - author: "Kaizhu Huang (Xi'an Jiaotong-Liverpool Univ.)"
      - author: "fenglei xu (Suzhou University of Science and Technology)"
      - author: "Linyan 	Li (Suzhou Institute of Trade & Commerce)"
    all_authors: "Liuqing Zhao, Fan Lyu, Fuyuan Hu, Kaizhu Huang, Fenglei Xu and Linyan 	Li"
    code: "https://github.com/Zlq2021/CA-GAN"
    keywords:
      - word: "Image manipulation"
      - word: "Generation adversarial network"
    paper: "papers/0079.pdf"
    supp: "supp/0079_supp.zip"
    abstract: "Sentence-based Image Editing (SIE) aims to deploy natural language to edit an image. Offering potentials to reduce expensive manual editing, SIE has attracted much interest recently.
However, existing methods can hardly produce accurate editing and even lead to failures in attribute editing when the query sentence is with multiple editable attributes.
To cope with this problem, by focusing on enhancing the difference between attributes, this paper proposes a novel model called Contrastive Attention Generative Adversarial Network (CA-GAN),  which is inspired from contrastive training.
Specifically, we first design a novel contrastive attention module to enlarge the editing difference between random combinations of attributes which are formed during training. We then construct an attribute discriminator to ensure effective editing on each attribute.  
A series of experiments show that our method can generate very encouraging results in sentence-based image editing with multiple attributes on CUB and COCO dataset."
  - id: 84
    order: 123
    poster_session: 2
    session_id: 5
    title: "Leveraging Class Hierarchies with Metric-Guided Prototype Learning"
    authors:
      - author: "Vivien Sainte Fare Garnot (IGN)"
      - author: "loic landrieu (IGN)"
    all_authors: "Vivien Sainte Fare Garnot and Loic Landrieu"
    code: "https://github.com/VSainteuf/metric-guided-prototypes-pytorch"
    keywords:
      - word: "hierarchical classification"
      - word: "class hierarchy"
      - word: "prototype learning"
      - word: "prototypical networks"
      - word: "metric-guiding"
      - word: ""
    paper: "papers/0084.pdf"
    supp: "supp/0084_supp.zip"
    abstract: "In many classification tasks, the set of target classes can be organized into a hierarchy. This structure induces a semantic distance between classes, and can be summarized  under the form of a cost matrix,  defining a finite metric on the class set. 
In this paper, we propose to model the hierarchical class structure by integrating this metric in the supervision of a prototypical network. Our method relies on jointly learning a feature-extracting network and a set of class prototypes whose relative arrangement in the embedding space follows the hierarchical metric.
We show that this approach allows for a consistent improvement of the rate of error weighted by the cost matrix when compared to traditional methods and other prototype-based strategies. Furthermore, when the induced metric contains insight on the data structure, our method improves the overall precision as well. Experiments on four different public datasets—from agricultural time series classification to depth image semantic segmentation—validate our approach."
  - id: 85
    order: 16
    poster_session: 1
    session_id: 2
    title: "Crafting Object Detection in Very Low Light"
    authors:
      - author: "Yang Hong (Beijing Institute of Technology)"
      - author: "Kaixuan Wei (Princeton University)"
      - author: "Linwei Chen (Beijing Institute of Technology)"
      - author: "Ying Fu (Beijing Institute of Technology)"
    all_authors: "Yang Hong, Kaixuan Wei, Linwei Chen and Ying Fu"
    code: "https://github.com/ying-fu/LODDataset"
    keywords:
      - word: "low-light"
      - word: "object detection"
      - word: "RAW images"
      - word: "paired dataset"
      - word: "unprocessing"
      - word: "noise model"
      - word: "images synthesis"
      - word: ""
    paper: "papers/0085.pdf"
    supp: "supp/0085_supp.zip"
    abstract: "	Over the last decade, object detection, as a leading application in computer vision, has been intensively studied, heavily engineered and widely applicable to	everyday life. However, existing object detection algorithms could easily break down under very dim environments, due to significantly low signal-to-noise ratio (SNR). Prepending a low-light image enhancement step before detection, as a common practice, increases the computation cost substantially, yet	still  doesn't yield satisfactory results. In this paper, we systematically investigate object detection in very low light, and identify several design principles that are essential to the low-light detection system. Based upon these criteria, we design a practical low-light detection system which utilizes a realistic low-light synthetic pipeline as well as an auxiliary low-light recovery module. The former can transform any labelled images from existing object detection datasets into their low-light counterparts to facilitate end-to-end training, while the latter can boost the low-light detection performance without adding additional computation cost at inference. Furthermore, we capture a real-world low-light object detection dataset, containing more than two thousand paired low/normal-light images with instance-level annotations to support this line of work. Extensive experiments collectively show the promising results of our designed detection system in very low light, paving the way for real-world object detection in the dark."
  - id: 91
    order: 340
    poster_session: 4
    session_id: 11
    title: "Boundary Guided Context Aggregation for Semantic Segmentation"
    authors:
      - author: "Haoxiang Ma (Beihang University)"
      - author: "Hongyu Yang (Beihang University)"
      - author: "Di Huang (Beihang University, China)"
    all_authors: "Haoxiang Ma, Hongyu Yang and Di Huang"
    code: "https://github.com/mahaoxiang822/Boundary-Guided-Context-Aggregation"
    keywords:
      - word: "semantic segmentation"
      - word: "context aggregation"
      - word: "boundary detection"
      - word: "multi-task learning"
    paper: "papers/0091.pdf"
    supp: "supp/0091_supp.zip"
    abstract: "The recent studies on semantic segmentation are starting to notice the significance of the boundary information, where most approaches see boundaries as the supplement of semantic details. However, simply combing boundaries and the mainstream features cannot ensure a holistic improvement of semantics modeling. In contrast to the previous studies, we exploit boundary as a significant guidance for context aggregation to promote the overall semantic understanding of an image. To this end, we propose a Boundary guided Context Aggregation Network (BCANet), where a Multi-Scale Boundary extractor (MSB) borrowing the backbone features at multiple scales is specifically designed for accurate boundary detection. Based on which, a Boundary guided Context Aggregation module (BCA) improved from Non-local network is further proposed to capture long-range dependencies between the pixels in the boundary regions and the ones inside the objects. By aggregating the context information along the boundaries, the inner pixels of the same category achieve mutual gains and therefore the intra-class consistency is enhanced. We conduct extensive experiments on the Cityscapes and ADE20K databases, and comparable results are achieved with the state-of-the-art methods, clearly demonstrating the effectiveness of the proposed one."
  - id: 92
    order: 341
    poster_session: 4
    session_id: 11
    title: "Learning multiplane images from single views with self-supervision"
    authors:
      - author: "Gustavo Sutter P. Carvalho (Samsung)"
      - author: "Diogo C Luvizon (Max Planck Institute for Informatics)"
      - author: "Antonio Joia (SAMSUNG)"
      - author: "Andre GC Pacheco (Samsung)"
      - author: "Otávio Penatti (SAMSUNG )"
    all_authors: "Gustavo Sutter P. Carvalho, Diogo C Luvizon, Antonio Joia, Andre GC Pacheco and Otávio Penatti"
    code: ""
    keywords:
      - word: "novel-view synthesis"
      - word: "multiplane images"
      - word: "cyclic supervision"
      - word: "depth estimation"
    paper: "papers/0092.pdf"
    supp: "supp/0092_supp.zip"
    abstract: "Generating single novel views from an already captured image is a hard task
in computer vision and graphics, in particular when the single input image
has dynamic parts such as persons or moving objects.
In this paper, we tackle this problem by proposing a new framework, called CycleMPI, that is capable of learning a multiplane image representation from single
images through a cyclic training strategy for self-supervision. Our framework does not require stereo data for training, therefore it can be trained with massive visual data from the Internet, resulting in a better generalization capability even
for very challenging cases. Although our method does not require stereo data for
supervision, it reaches results on stereo datasets comparable to the state
of the art in a zero-shot scenario. We evaluated our method on RealEstate10K and Mannequim Challenge datasets for view synthesis, and presented qualitative results on Places II dataset."
  - id: 94
    order: 124
    poster_session: 2
    session_id: 5
    title: "Measuring the Biases and Effectiveness of Content-Style Disentanglement"
    authors:
      - author: "Xiao Liu (University of Edinburgh)"
      - author: "Spyridon Thermos (University of Edinburgh)"
      - author: "Gabriele Valvano (IMT School for Advanced Studies Lucca)"
      - author: "Agisilaos Chartsias (Ultromics Ltd)"
      - author: "Alison Q O'Neil (Canon Medical Research Europe)"
      - author: "Sotirios Tsaftaris (The University of Edinburgh)"
    all_authors: "Xiao Liu, Spyridon Thermos, Gabriele Valvano, Agisilaos Chartsias, Alison Q O'Neil and Sotirios Tsaftaris"
    code: "https://github.com/vios-s/CSDisentanglement_Metrics_Library"
    keywords:
      - word: "Disentangled Representations Learning"
      - word: "Content and Style Disentanglement"
      - word: "Metrics"
      - word: "Biases"
      - word: "Semantic Segmentation"
      - word: "Image to Image Translation"
      - word: "Pose Estimation"
      - word: ""
    paper: "papers/0094.pdf"
    supp: "supp/0094_supp.zip"
    abstract: "A recent spate of state-of-the-art semi- and un-supervised solutions disentangle and encode image \"content\" into a spatial tensor and image appearance or \"style\" into a vector, to achieve good performance in spatially equivariant tasks (e.g. image-to-image translation). To achieve this, they employ different model design, learning objective, and data biases. While considerable effort has been made to measure disentanglement in vector representations, and assess its impact on task performance, such analysis for (spatial) content - style disentanglement is lacking. In this paper, we conduct an empirical study to investigate the role of different biases in content-style disentanglement settings and unveil the relationship between the degree of content-style disentanglement and task performance. In particular, we consider the setting where we: (i) identify key design choices and learning constraints for three popular content-style disentanglement models; (ii) relax or remove such constraints in an ablation fashion; and (iii) use two metrics to measure the degree of disentanglement and assess its effect on each task performance. Our experiments reveal that there is a \"sweet spot\" between disentanglement, task performance and - surprisingly - content interpretability, suggesting that blindly forcing for higher disentanglement can hurt model performance and content factors semanticness. Our findings, as well as the used task-independent metrics, can be used to guide the design and selection of new models for tasks where content-style representations are useful."
  - id: 96
    order: 232
    poster_session: 3
    session_id: 8
    title: "Hand-Object Contact Prediction via Motion-Based Pseudo-Labeling and Guided Progressive Label Correction"
    authors:
      - author: "Takuma Yagi (The University of Tokyo)"
      - author: "Md Tasnimul Hasan (The University of Tokyo)"
      - author: "Yoichi Sato (University of Tokyo)"
    all_authors: "Takuma Yagi, Md Tasnimul Hasan and Yoichi Sato"
    code: "https://github.com/takumayagi/hand_object_contact_prediction/"
    keywords:
      - word: "hand-object interaction"
      - word: "contact prediction"
      - word: "learning from noisy labels"
      - word: "label correction"
      - word: "pseudo-labeling"
      - word: "first-person vision"
      - word: "egocentric vision"
    paper: "papers/0096.pdf"
    supp: "supp/0096_supp.zip"
    abstract: "Every hand-object interaction begins with contact. Despite predicting the contact state between hands and objects is useful in understanding hand-object interactions, prior methods on hand-object analysis have assumed that the interacting hands and objects are known, and were not studied in detail. In this study, we introduce a video-based method for predicting contact between a hand and an object. Specifically, given a video and a pair of hand and object tracks, we predict a binary contact state (contact or no-contact) for each frame. However, annotating a large number of hand-object tracks and contact labels is costly. To overcome the difficulty, we propose a semi-supervised framework consisted of (i) automatic collection of training data with motion-based pseudo-labels and (ii) guided progressive label correction (gPLC), which corrects noisy pseudo-labels with a small amount of trusted data. We validated our framework's effectiveness on a newly built benchmark dataset for hand-object contact prediction and showed superior performance against existing baseline methods. Code and data are available at https://github.com/takumayagi/hand_object_contact_prediction."
  - id: 97
    order: 223
    oral_session: 6
    poster_session: 3
    session_id: 7
    title: "BI-GCN: Boundary-Aware Input-Dependent Graph Convolution Network for Biomedical Image Segmentation"
    authors:
      - author: "Yanda Meng (University of Liverpool)"
      - author: "hongrun zhang (University of Liverpool)"
      - author: "Dongxu Gao (University of Portsmouth)"
      - author: "Yitian Zhao (Cixi Institute of Biomedical Engineering, Ningbo Institute of Industrial Technology, Chinese Academy of Sciences)"
      - author: "xiaoyun yang (Remark Holdings)"
      - author: "Xuesheng Qian (CSA Intellicloud Ltd)"
      - author: "Xiaowei Huang (Liverpool University)"
      - author: "Yalin Zheng (University of Liverpool)"
    all_authors: "Yanda Meng, Hongrun Zhang, Dongxu Gao, Yitian Zhao, Xiaoyun Yang, Xuesheng Qian, Xiaowei Huang and Yalin Zheng"
    code: "https://github.com/smallmax00/BI-GConv"
    keywords:
      - word: "Medical Image Segmentation"
      - word: "Graph Convolution Network"
      - word: ""
    paper: "papers/0097.pdf"
    supp: "supp/0097_supp.zip"
    abstract: "Segmentation is an essential operation of image processing. The convolution operation suffers from a limited receptive field, while global modelling is fundamental to segmentation tasks. In this paper, we apply graph convolution into the segmentation task and propose an improved Laplacian. Different from existing methods, our Laplacian is data-dependent, and we introduce two attention diagonal matrices to learn a better vertex relationship. Additionally, it takes advantage of both region and boundary information when performing graph-based information propagation.  Specifically, we model and reason about the boundary-aware region-wise correlations of different classes through learning graph representations, which is capable of manipulating long-range semantic reasoning across various regions with the spatial enhancement along the object's boundary. Our model is well-equipped to obtain global semantic region information while also accommodating local spatial boundary characteristics simultaneously. Experiments on two types of challenging datasets demonstrate that our method outperforms the state-of-the-art approaches on the segmentation of polyps in colonoscopy images and of the optic disc and optic cup in colour fundus images."
  - id: 103
    order: 342
    poster_session: 4
    session_id: 11
    title: "Local-Global Associative Frame Assemble in Video Re-ID"
    authors:
      - author: "Qilei Li (Queen Mary, University of London)"
      - author: "Jiabo Huang (Queen Mary University of London)"
      - author: "Shaogang Gong (Queen Mary University of London)"
    all_authors: "Qilei Li, Jiabo Huang and Shaogang Gong"
    code: "http://liqilei.github.io/projects/li2021loga"
    keywords:
      - word: "video Re-ID"
      - word: "local aligned quality"
      - word: "global appearance correlations"
      - word: "associative frame assemble"
    paper: "papers/0103.pdf"
    supp: ""
    abstract: "Noisy and unrepresentative frames in automatically generated object bounding boxes from video sequences cause significant challenges in learning discriminative representations in video re-identification (Re-ID). Most existing methods tackle this problem by assessing the importance of video frames according to either their local part alignments or global appearance correlations separately. However, given the diverse and unknown sources of noise which usually co-exist in captured video data, existing methods have not been effective satisfactorily. In this work, we explore jointly both local alignments and global correlations with further consideration of their mutual promotion/reinforcement so to better assemble complementary discriminative Re-ID information within all the relevant frames in video tracklets. Specifically, we concurrently optimise a local aligned quality (LAQ) module that distinguishes the quality of each frame based on local alignments, and a global correlated quality (GCQ) module that estimates global appearance correlations. With the help of a local-assembled global appearance prototype, we associate LAQ and GCQ to exploit their mutual complement. Extensive experiments demonstrate the superiority of the proposed model against state-of-the-art methods on four video Re-ID benchmarks, including MARS, Duke-Video, Duke-SI, and iLIDS-VID. "
  - id: 108
    order: 17
    poster_session: 1
    session_id: 2
    title: "Feature Space Saturation during Training"
    authors:
      - author: "Mats L Richter (Universität Osnabrück)"
      - author: "Justin C Shenk (Peltarion)"
      - author: "Wolf Byttner (Rapid Health)"
      - author: "Anna Wiedenroth (Universität Osnabrück)"
      - author: "Mikael Huss (Peltarion)"
    all_authors: "Mats L Richter, Justin C Shenk, Wolf Byttner, Anna Wiedenroth and Mikael Huss"
    code: "https://github.com/delve-team/delve (experiment control and covariance approximation)

https://github.com/delve-team/phd-lab (general experimental setup)
"
    keywords:
      - word: "deep learning"
      - word: "convolutional neural networks"
      - word: "PCA"
      - word: "XAI"
      - word: "explainable AI"
      - word: "neural architecture"
      - word: "classification"
    paper: "papers/0108.pdf"
    supp: "supp/0108_supp.zip"
    abstract: "We propose layer saturation - a simple, online-computable method for analyzing the information processing in neural networks.  First, we show that a layer’s output can be restricted to an eigenspace of its covariance matrix without performance loss. We propose a computationally lightweight method that approximates the covariance matrix during training. From the dimension of its relevant eigenspace we derive layer saturation- the ratio between the eigenspace dimension and layer width.  We show evidence that saturation indicates which layers contribute to network performance. We demonstrate how to alter layer saturation in a neural network by changing network depth,  filter sizes and input resolution. Finally we show that pathological patterns of saturation are indicative of parameter inefficiencies caused by a mismatch between input resolution and neural architecture."
  - id: 113
    order: 125
    poster_session: 2
    session_id: 5
    title: "Multi-Teacher Single-Student Visual Transformer with Multi-Level Attention for Face Spoofing Detection"
    authors:
      - author: "Yao-Hui Huang (University of National Taiwan Ocean)"
      - author: "Jun-Wei Hsieh (College of Artificial Intelligence and Green Energy )"
      - author: "Ming-Ching Chang (University at Albany - SUNY)"
      - author: "Lipeng Ke (University at Buffalo)"
      - author: "Siwei Lyu (University at Buffalo)"
      - author: "Arpita Samanta Santra (National Taiwan Ocean University)"
    all_authors: "Yao-Hui Huang, Jun-Wei Hsieh, Ming-Ching Chang, Lipeng Ke, Siwei Lyu and Arpita Samanta Santra"
    code: ""
    keywords:
      - word: "Image Processcing"
      - word: "liveness detection"
      - word: "Face Anti-Spoofing"
      - word: "presentation attacks_x0000_"
    paper: "papers/0113.pdf"
    supp: "supp/0113_supp.zip"
    abstract: "Face biometrics have attracted significant attention in many security-based applications. The presentation attack (PA) or face spoofing is a cybercriminal attempt to gain illegitimate access to a victim's device using photos, videos, or 3D artificial masks of the victim's face. Various deep learning approaches can tackle particular PA attacks when tested on standard datasets. However, these methods fail to generalize to complex environments or unseen datasets. We propose a new Multi-Teacher Single-Student (MTSS) visual Transformer with a multi-level attention design to improve the generalizability of face spoofing detection. Then, a novel Multi-Level Attention Module with a DropBlock (MAMD) is designed to strengthen discriminative features while dropping irrelevant spatial features to avoid overfitting. Finally, these rich convolutional feature sets are combined and fed into the MTSS network for face spoofing training. With this MAMD module, our method survives well under small training datasets with poorly lighted conditions. Experimental results demonstrate the superiority of our method when compared with several anti-spoofing methods on four datasets (CASIA-MFSD, Replay-Attack, MSU-MFSD, and OULU-NPU). Furthermore, our model can run on Jetson TX2 up to 80 FPS for real-world applications."
  - id: 117
    order: 343
    poster_session: 4
    session_id: 11
    title: "Image-Text Alignment using Adaptive Cross-attention with Transformer Encoder for Scene Graphs"
    authors:
      - author: "Juyong Song (Samsung Research)"
      - author: "Sunghyun Choi (Samsung Research)"
    all_authors: "Juyong Song and Sunghyun Choi"
    code: "https://github.com/SamsungLabs/RELAX"
    keywords:
      - word: "cross-attention"
      - word: "multi-modal"
      - word: "retrieval"
      - word: "scene-graphs"
      - word: "graph neural networks"
      - word: "contrastive loss"
    paper: "papers/0117.pdf"
    supp: "supp/0117_supp.zip"
    abstract: "Neural image and text encoders have been proposed to align the abstract image and symbolic text representation. Global-local and local-local information integration between two modalities are essential for an effective alignment. In this paper, we present RELation-aware Adaptive Cross-attention (RELAX) that achieves state-of-the-art performance in cross-modal retrieval tasks by incorporating several novel improvements. First, cross-attention methods integrate global-local information via weighted global feature of a modality (taken as value) for a local feature of the other modality (taken as query). We can make more accurate alignments if we could also consider the global weights of the query modality. To this end, we introduce adaptive embedding to consider the weights. Second, to enhance the usage of scene-graphs that can capture the high-level relation of local features, we introduce transformer encoders for textual scene graphs to align with visual scene graphs. Lastly, we use NT-XEnt loss that takes the weighted sum of the samples based on their importance. We show that our approach is effective in extensive experiments that outperform other state-of-the-art models."
  - id: 119
    order: 126
    poster_session: 2
    session_id: 5
    title: "A Strong Baseline for Semi-Supervised Incremental Few-Shot Learning"
    authors:
      - author: "Linglan Zhao (Shanghai Jiao Tong University)"
      - author: "Dashan Guo (	Hikvision Research Institute)"
      - author: "Yunlu Xu (Hikvision Research Institute)"
      - author: "Liang Qiao (Zhejiang University & Hikvision Research Institute)"
      - author: "Zhanzhan Cheng (Zhejiang University & Hikvision Research Institute)"
      - author: "Shiliang Pu (Hikvision Research Institute)"
      - author: "Yi Niu (Hikvision Research Institute)"
      - author: "Xiangzhong Fang (Shanghai Jiao Tong University)"
    all_authors: "Linglan Zhao, Dashan Guo, Yunlu Xu, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu and Xiangzhong Fang"
    code: ""
    keywords:
      - word: "few-shot learning"
      - word: "meta-learning"
      - word: "incremental learning"
      - word: "semi-supervised learning"
    paper: "papers/0119.pdf"
    supp: "supp/0119_supp.zip"
    abstract: "Few-shot learning (FSL) aims to learn models that generalize to novel classes with limited training samples. Recent works advance FSL towards a scenario where unlabeled examples are also available and propose semi-supervised FSL methods. Another line of methods also cares about the performance of base classes in addition to the novel ones and thus establishes the incremental FSL scenario. In this paper, we generalize the above two under a more realistic yet complex setting, named by Semi-Supervised Incremental Few-Shot Learning (S2I-FSL). To tackle the task, we propose a novel paradigm containing two parts: (1) a well-designed meta-training algorithm for mitigating ambiguity between base and novel classes caused by unreliable pseudo labels and (2) a model adaptation mechanism to learn discriminative features for novel classes while preserving base knowledge using few labeled and all the unlabeled data. Extensive experiments on standard FSL, semi-supervised FSL, incremental FSL, and the firstly built S2I-FSL benchmarks demonstrate the effectiveness of our proposed method."
  - id: 121
    order: 18
    poster_session: 1
    session_id: 2
    title: "Adaptive End-to-End Budgeted Network Learning via Inverse Scale Space"
    authors:
      - author: "zuyuan zhong (Fudan University)"
      - author: "Chen Liu (Hong Kong University of Science and Technology)"
      - author: "Yanwei Fu (Fudan University)"
    all_authors: "Zuyuan Zhong, Chen Liu and Yanwei Fu"
    code: ""
    keywords:
      - word: "deep learning"
      - word: "network architecture"
      - word: "growing network"
      - word: "budgeted network learning"
      - word: "pruning"
      - word: ""
    paper: "papers/0121.pdf"
    supp: "supp/0121_supp.zip"
    abstract: "This paper studies the task of budgeted network learning~cite{veniat2018learning} that aims at discovering good convolutional network structures under parameters/FLOPs constraints.  Particularly, we present a novel Adaptive End-to-End Network Learning (AdeNeL) approach that enables learning the structures and parameters of networks simultaneously within the budgeted cost in terms of computation and memory consumption. We keep the depth of networks fixed to ensure a fair comparison with the backbones of competitors. Our AdeNeL learns to optimize both the parameters and the number of filters in each layer. To achieve this goal, our AdeNeL utilizes an iterative sparse regularization path -- Discretized Differential Inclusion of Inverse Scale Space (DI-ISS) to measure the capacity of the networks in the training process.  Notably, the DI-ISS imports a group of augmented variables to explore the inverse scale space and this group of variables can be used to measure the redundancy of the network. According to the redundancy of the current network, our AdeNeL can choose appropriate operations for current network such as adding more filters. By this strategy, we can control the balance between the computational cost and the model performance in a dynamic way. Extensive experiments on several datasets including MNIST, CIFAR10/100,  ImageNet with popular VGG and ResNet backbones validate the efficacy of our proposed method. In specific, on VGG16 backbone, our method on CIFAR10 achieves 92.71% test accuracy with only 0.58M (3.8%) parameters and 43M (13.7%) FLOPs, comparing to  92.90% test accuracy and 14.99 M parameters, 313M FLOPs of the full-sized VGG16 model."
  - id: 126
    order: 333
    oral_session: 8
    poster_session: 4
    session_id: 10
    title: "Learning to Sparsify Differences of Synaptic Signal for Efficient Event Processing"
    authors:
      - author: "Yusuke Sekikawa (DENSO IT Laboratory)"
      - author: "Keisuke Uto (Denso IT Laboratory)"
    all_authors: "Yusuke Sekikawa and Keisuke Uto"
    code: "https://github.com/DensoITLab/spr_sigmadelta"
    keywords:
      - word: "event-based processing"
      - word: "sigma-delta neuron"
      - word: "temporally sparse processing"
      - word: "learning to sparsify"
    paper: "papers/0126.pdf"
    supp: "supp/0126_supp.zip"
    abstract: "Neural network processors on edge devices need to process spatiotemporal data with low latency, which requires a large amount of multiply-accumulate operation (MAC).
In this paper, we propose a difference-driven neural network framework for efficient video or event stream processing.
Our framework achieves lower MAC by learning to sparsify the ``temporal differences of synaptic signals'' (TDSS) of proposed masked convolutional neural networks.
By reducing the TDSS, MAC reduction is achieved in a unified manner by increasing the quantization step size, disconnecting synapses, and learning weights that respond sparsely to inputs.
A novel quantizer is another key to realize unified optimization; the quantizer has a gradient called ``macro-grad'' that guides the step size to reduce the MAC by reducing the TDSS loss. 
Experiments conducted using a wide range of tasks and data (frames/events) show that the proposed framework can reduce MAC by a factor of 32 to 240 compared to dense convolution while maintaining comparable accuracy, which is several times better than the current state-of-the-art methods."
  - id: 128
    order: 233
    poster_session: 3
    session_id: 8
    title: "LSTA-Net: Long short-term Spatio-Temporal Aggregation Network for Skeleton-based Action Recognition"
    authors:
      - author: "Tailin Chen (Newcastle Univesity)"
      - author: "Shidong Wang (Newcastle University)"
      - author: "Desen Zhou (Baidu, Inc.)"
      - author: "Yu Guan (Newcastle University)"
    all_authors: "Tailin Chen, Shidong Wang, Desen Zhou and Yu Guan"
    code: "https://github.com/tailin1009/LSTA-Net"
    keywords:
      - word: "Action Recogntion"
      - word: "Skeleton-based"
      - word: "Graph Convolution Network"
      - word: "Long short-term Aggregation"
      - word: "LSTA-Net"
    paper: "papers/0128.pdf"
    supp: ""
    abstract: "Modelling various spatio-temporal dependencies is the key to recognising human actions in skeleton sequences.  Most existing methods excessively relied on the design of traversal rules or graph topologies to draw the dependencies of the dynamic joints, which is inadequate to reflect the relationships of the distant yet important joints. Furthermore, due to the locally adopted operations, the important long-range temporal information is therefore not well explored in existing works. To address this issue, in this work we propose LSTA-Net: a novel Long short-term Spatio-Temporal Aggregation Network, which can effectively capture the long/short-range dependencies in a spatio-temporal manner. We devise our model into a pure factorised architecture that can alternately perform spatial feature aggregation and temporal feature aggregation.  To improve the feature aggregation effect, a channel-wise attention mechanism is also designed and employed. Extensive experiments were conducted on three public benchmark datasets, and the results suggest that our approach can capture both long-and-short range dependencies in the space and time domain, yielding higher results than other state-of-the-art methods."
  - id: 129
    order: 344
    poster_session: 4
    session_id: 11
    title: "Joint-Aware Regression: Rethinking Regression-Based Method for 3D Hand Pose Estimation"
    authors:
      - author: "Xiaozheng Zheng (Beijing University of Posts and Telecommunications)"
      - author: "Pengfei Ren (Beijing University of Posts and Telecommunications)"
      - author: "Haifeng Sun (Beijing university of posts and telecommunications)"
      - author: "Jingyu Wang (Beijing University of Posts and Telecommunications)"
      - author: "Qi Qi (Beijing University of Posts and Telecommunications)"
      - author: "Jianxin Liao (beijing university of posts and telecommunications)"
    all_authors: "Xiaozheng Zheng, Pengfei Ren, Haifeng Sun, Jingyu Wang, Qi Qi and Jianxin Liao"
    code: ""
    keywords:
      - word: "hand pose estimation"
      - word: ""
    paper: "papers/0129.pdf"
    supp: "supp/0129_supp.zip"
    abstract: "3D hand pose estimation approaches can be divided into two categories, including regression-based methods and detection-based methods. Detection-based methods utilize fully convolutional networks to obtain hand-crafted coordinate representations like heatmaps and then use a coordinate decoding function like soft-argmax to decode coordinates. In contrast, regression-based methods employ low-dimension features from convolutional networks as unconstrained coordinate representations and then use fully-connected layers to decode coordinates. This way allows the network to learn the coordinate representations and the corresponding coordinate decoding function automatically. However, it causes either weak coordinate representational power or decoding's optimization difficulty. These drawbacks cause regression-based methods far less accurate than detection-based methods. However, detection-based methods require many computations for deconvolution, and their hand-crafted coordinate representations may not be optimal. This paper proposes a novel framework for regression-based methods that can preserve the strength of representations and avoid severe optimization difficulty while remaining flexible, lightweight, and efficient. More specifically, we use joint-specific feature maps as coordinate representations and the joint-shared coordinate decoding module. Moreover, we apply a multi-head mechanism to exploit different coordinate representations and design a learnable re-parameterization method to do multi-stage refinement better. Our approach outperforms state-of-the-art methods on four public benchmarks, including FreiHAND, HO-3D, RHD, and STB."
  - id: 130
    order: 345
    poster_session: 4
    session_id: 11
    title: "AniFormer: Data-driven 3D Animation with Transformer"
    authors:
      - author: "Haoyu Chen (University of Oulu)"
      - author: "Hao Tang (ETH Zurich)"
      - author: "Nicu Sebe (University of Trento)"
      - author: "Guoying Zhao (University of Oulu)"
    all_authors: "Haoyu Chen, Hao Tang, Nicu Sebe and Guoying Zhao"
    code: "https://github.com/mikecheninoulu/AniFormer"
    keywords:
      - word: "3D motion"
      - word: "3D generation"
      - word: "3D style transfer"
      - word: "Transformer"
      - word: "3D animation"
    paper: "papers/0130.pdf"
    supp: "supp/0130_supp.pdf"
    abstract: "We present a novel task, i.e, animating a target 3D object through the motion of a raw driving sequence. In previous works, extra auxiliary correlations between source and target meshes or intermedia factors are inevitable to capture the motions in the driving sequences. Instead, we introduce AniFormer, a novel Transformer-based architecture, that generates animated 3D sequences by directly taking the raw driving sequences and arbitrary same-type target meshes as inputs. Specifically, we customize the Transformer architecture for 3D animation that generates mesh sequences by integrating styles from target meshes and motions from the driving meshes. Besides, instead of the conventional single regression head in the vanilla Transformer, AniFormer generates multiple frames as outputs to preserve the sequential consistency of the generated meshes. To achieve this, we carefully design a pair of regression constraints, i.e., motion and appearance constraints, that can provide strong regularization on the generated mesh sequences. Our AniFormer achieves high-fidelity, realistic, temporally coherent animated results and outperforms compared start-of-the-art methods on benchmarks of diverse categories."
  - id: 133
    order: 224
    oral_session: 6
    poster_session: 3
    session_id: 7
    title: "CTRN: Class-Temporal Relational Network for Action Detection"
    authors:
      - author: "Rui Dai (INRIA)"
      - author: "Srijan Das (Stony Brook University)"
      - author: "Francois Bremond (Inria Sophia Antipolis, France)"
    all_authors: "Rui Dai, Srijan Das and Francois Bremond"
    code: ""
    keywords:
      - word: "action detection"
      - word: "graph reasoning"
      - word: "graph convolutional network"
      - word: "temporal modelling"
      - word: "multi-label classification"
      - word: ""
    paper: "papers/0133.pdf"
    supp: "supp/0133_supp.zip"
    abstract: "Action detection is an essential and challenging task, especially for densely labelled datasets of untrimmed videos. There are many real-world challenges in those datasets, such as composite action, co-occurring action, and high temporal variation of instance duration. For handling these challenges, we propose to explore both the class and temporal relations of detected actions. In this work, we introduce an end-to-end network: Class-Temporal Relational Network (CTRN). 
It contains three key components: (1) The Representation Transform Module filters the class-specific features from the mixed representations to build graph-structured data. (2) The Class-Temporal Module models the class and temporal relations in a sequential manner. (3) G-classifier leverages the privileged knowledge of the snippet-wise co-occurring action pairs to further improve the co-occurring action detection. We evaluate CTRN on three challenging densely labelled datasets and achieve state-of-the-art performance, reflecting the effectiveness and robustness of our method. "
  - id: 134
    order: 234
    poster_session: 3
    session_id: 8
    title: "Exemplar-Based Early Event Prediction in Video"
    authors:
      - author: "ZEKUN ZHANG (Stony Brook University)"
      - author: "FARRUKH M KORAISHY (Stony Brook University)"
      - author: "Minh  Hoai (Stony Brook University)"
    all_authors: "ZEKUN ZHANG, FARRUKH M KORAISHY and Minh  Hoai"
    code: "https://github.com/cvlab-stonybrook/EnEx"
    keywords:
      - word: "early prediction"
      - word: "video prediction"
      - word: "exemplar"
      - word: ""
    paper: "papers/0134.pdf"
    supp: "supp/0134_supp.zip"
    abstract: "Observing a video stream and being able to predict target events of interest before they occur is an important but challenging task due to the stochastic nature of visual events. This task requires a classifier that can separate the precursory signals that lead to the events and the ones that do not. However, a naïve approach for training this classifier would require seeing many examples of the target events before a model with high precision can be obtained.  In this paper, we propose a method for early prediction of visual events based on an ensemble of exemplar predictors. Each exemplar predictor is associated with an instance of the target event, being trained to separate the target event from  negative samples. The exemplar predictors can be calibrated and integrated to create a stronger predictor. Experiments on several datasets show that the proposed exemplar-based framework outperforms other methods, yielding higher precision given fewer training samples. Our code and datasets can be found at https://github.com/cvlab-stonybrook/EnEx."
  - id: 138
    order: 127
    poster_session: 2
    session_id: 5
    title: "Single-Modal Entropy based Active Learning for Visual Question Answering"
    authors:
      - author: "Dong-Jin Kim (KAIST)"
      - author: "Jae Won Cho (KAIST)"
      - author: "Jinsoo Choi (KAIST)"
      - author: "Yunjae Jung (KAIST)"
      - author: "In So Kweon (KAIST)"
    all_authors: "Dong-Jin Kim, Jae Won Cho, Jinsoo Choi, Yunjae Jung and In So Kweon"
    code: ""
    keywords:
      - word: "Visual Question Answering"
      - word: "Vision and Language"
      - word: "Active Learning"
    paper: "papers/0138.pdf"
    supp: "supp/0138_supp.zip"
    abstract: "Constructing a large-scale labeled dataset in the real world, especially for high-level tasks (e.g, Visual Question Answering), can be expensive and time-consuming. In addition, with the ever-growing amounts of data and architecture complexity, Active Learning has become an important aspect of computer vision research. In this work, we address Active Learning in the multi-modal setting of Visual Question Answering (VQA). In light of the multi-modal inputs, image and question, we propose a novel method for effective sample acquisition through the use of ad hoc single-modal branches for each input to leverage its information. Our mutual information based sample acquisition strategy Single-Modal Entropic Measure (SMEM) in addition to our self-distillation technique enables the sample acquisitor to exploit all present modalities and find the most informative samples. Our novel idea is simple to implement, cost-efficient, and readily adaptable to other multi-modal tasks. We confirm our findings on various VQA datasets through state-of-the-art performance by comparing to existing Active Learning baselines."
  - id: 141
    order: 218
    oral_session: 5
    poster_session: 3
    session_id: 6
    title: "Cascaded Cross MLP-Mixer GANs for Cross-View Image Translation"
    authors:
      - author: "Bin Ren (University of Trento)"
      - author: "Hao Tang (ETH Zurich)"
      - author: "Nicu Sebe (University of Trento)"
    all_authors: "Bin Ren, Hao Tang and Nicu Sebe"
    code: "https://github.com/Amazingren/CrossMLP"
    keywords:
      - word: "cross view"
      - word: "MLP"
      - word: "image translation"
      - word: "image generation"
      - word: ""
    paper: "papers/0141.pdf"
    supp: "supp/0141_supp.zip"
    abstract: "It is hard to generate an image at target view well for previous cross-view image translation methods that directly adopt a simple encoder-decoder or U-Net structure, especially for drastically different views and severe deformation cases. To ease this problem, we propose a novel two-stage framework with a new Cascaded Cross MLP-Mixer (CrossMLP) sub-network in the first stage and one refined pixel-level loss in the second stage. In the first stage, the CrossMLP sub-network learns the latent transformation cues between image code and semantic map code via our novel cross MLP-Mixer blocks. Then the coarse results are generated progressively under the guidance of those cues. Moreover, in the second stage, we design a refined pixel-level loss that eases the noisy semantic label problem in the cross-view translation task in a much simple fashion for better network optimization. Extensive experimental results on Dayton~cite{vo2016localizing} and CVUSA~cite{workman2015wide} datasets show that our method can generate significantly better results than state-of-the-art methods. The source code, data, and trained models are available later."
  - id: 142
    order: 128
    poster_session: 2
    session_id: 5
    title: "Alleviating Noisy-label Effects in Image Classification via Probability Transition Matrix"
    authors:
      - author: "Ziqi Zhang (Tsinghua University)"
      - author: "Yuexiang Li (Jarvis Lab, Tencent)"
      - author: "Hongxin Wei (Nanyang Technological University)"
      - author: "Kai Ma (Tencent)"
      - author: "Tao Xu (Tsinghua University)"
      - author: "Yefeng Zheng (Tencent)"
    all_authors: "Ziqi Zhang, Yuexiang Li, Hongxin Wei, Kai Ma, Tao Xu and Yefeng Zheng"
    code: ""
    keywords:
      - word: "noisy labels"
      - word: "image classification"
      - word: "instance selection"
      - word: "robust learning"
      - word: "inter-class correlation"
      - word: "soft label"
      - word: "medical image"
    paper: "papers/0142.pdf"
    supp: ""
    abstract: "Deep-learning-based image classification frameworks often suffer from the noisy label problem caused by the inter-observer variation. Recent studies employed learning-to-learn paradigms (e.g., Co-teaching and JoCoR) to filter the samples with noisy labels from the training set. However, most of them use a simple cross-entropy loss as the criterion for noisy label identification. The hard samples, which are beneficial for classifier learning, are often mistakenly treated as noises in such a setting since both the hard samples and ones with noisy labels lead to a relatively larger loss value than the easy cases. In this paper, we propose a plugin module, namely noise ignoring block (NIB), consisting of a probability transition matrix and an inter-class correlation (IC) loss, to separate the hard samples from the mislabeled ones, and further boost the accuracy of image classification network trained with noisy labels. Concretely, our IC loss is calculated as Kullback-Leibler divergence between the network prediction and the accumulative soft label generated by the probability transition matrix. Such that, with the lower value of IC loss, the hard cases can be easily distinguished from mislabeled cases. Extensive experiments are conducted on natural and medical image datasets (CIFAR-10 and ISIC 2019). The experimental results show that our NIB module consistently improves the performances of the state-of-the-art robust training methods."
  - id: 145
    order: 19
    poster_session: 1
    session_id: 2
    title: "Median Pixel Difference Convolutional Network for Robust Face Recognition"
    authors:
      - author: "Jiehua Zhang (University of Oulu)"
      - author: "Zhuo Su (University of Oulu)"
      - author: "Li Liu (National University of Defense Technology)"
    all_authors: "Jiehua Zhang, Zhuo Su and Li Liu"
    code: ""
    keywords:
      - word: "face recognition"
      - word: "noise robustness"
      - word: "efficient CNN"
    paper: "papers/0145.pdf"
    supp: "supp/0145_supp.zip"
    abstract: "Face recognition is one of the most active tasks in computer vision and has been widely used in the real world. With great advances made in convolutional neural networks (CNN), lots of face recognition algorithms have achieved high accuracy on various face datasets. However, existing face recognition algorithms based on CNNs are vulnerable to noise. Noise corrupted image patterns could lead to false activations, significantly decreasing face recognition accuracy in noisy situations. To equip CNNs with built-in robustness to noise of different levels, we proposed a Median Pixel Difference Convolutional Network (MeDiNet) by replacing some traditional convolutional layers with the proposed novel Median Pixel Difference Convolutional Layer (MeDiConv) layer. The proposed MeDiNet integrates the idea of traditional multiscale median filtering with deep CNNs. The MeDiNet is tested on the four face datasets (LFW, CA-LFW, CP-LFW, and YTF) with versatile settings on blur kernels, noise intensities, scales, and JPEG quality factors. Extensive experiments show that our MeDiNet can effectively remove noisy pixels in the feature map and suppress the negative impact of noise, leading to achieving limited accuracy loss under these practical noises compared with the standard CNN under clean conditions. "
  - id: 148
    order: 235
    poster_session: 3
    session_id: 8
    title: "DRT: Detection Refinement for Multiple Object Tracking"
    authors:
      - author: "bisheng wang (Nanjing university of science and technology)"
      - author: "Christian Fruhwirth-Reisinger (Graz University of Technology)"
      - author: "Horst Possegger (Graz University of Technology)"
      - author: "Horst Bischof (Graz University of Technology)"
      - author: "Guo Cao (Nanjing university of science and technology)"
    all_authors: "Bisheng Wang, Christian Fruhwirth-Reisinger, Horst Possegger, Horst Bischof and Guo Cao"
    code: ""
    keywords:
      - word: "Multiple Object Tracking"
      - word: "Tracking by Detection"
      - word: "Detection Refinement"
      - word: ""
    paper: "papers/0148.pdf"
    supp: "supp/0148_supp.zip"
    abstract: "Deep learning methods have led to a remarkable progress in multiple object tracking (MOT). However, when tracking in crowded scenes, existing methods still suffer from both inaccurate and missing detections. This paper proposes Detection Refinement for Tracking (DRT) to address these two issues for people tracking. First, we construct an encoder-decoder backbone network with a novel semi-supervised heatmap training procedure, which leverages human heatmaps to obtain a more precise localization of the targets. Second, we integrate a \"one patch, multiple predictions\" mechanism into DRT which refines the detection results and recovers occluded pedestrians at the same time. Additionally, we leverage a data-driven LSTM-based motion model which can recover lost targets at negligible computational cost. Compared with strong baseline methods, our DRT achieves significant improvements on publicly available MOT datasets. In addition, DRT generalizes well, i.e. it can be applied to any detector to improve their performance."
  - id: 149
    order: 20
    poster_session: 1
    session_id: 2
    title: "Recurrence-in-Recurrence Networks for Video Deblurring"
    authors:
      - author: "JoonKyu Park (Seoul National Univ.)"
      - author: "Seungjun Nah (Seoul National University)"
      - author: "Kyoung Mu Lee (Seoul National University)"
    all_authors: "JoonKyu Park, Seungjun Nah and Kyoung Mu Lee"
    code: "https://github.com/jkpark0825/RIRN"
    keywords:
      - word: "video deblurring"
      - word: "recurrence-in-recurrence"
      - word: "inner-recurrence"
      - word: "recurrent neural networks"
      - word: "attention"
      - word: ""
    paper: "papers/0149.pdf"
    supp: "supp/0149_supp.zip"
    abstract: "State-of-the-art video deblurring methods often adopt recurrent neural networks to model the temporal dependency between the frames. While the hidden states play key role in delivering information to the next frame, abrupt motion blur tend to weaken the relevance in the neighbor frames. In this paper, we propose recurrence-in-recurrence network architecture to cope with the limitations of short-ranged memory. We employ additional recurrent units inside the RNN cell. First, we employ inner-recurrence module~(IRM) to manage the long-ranged dependency in a sequence. IRM learns to keep track of the cell memory and provides complementary information to find the deblurred frames. Second, we adopt an attention-based temporal blending strategy to extract the necessary part of the information in the local neighborhood. The adpative temporal blending~(ATB) can either attenuate or amplify the features by the spatial attention. Our extensive experimental results and analysis validate the effectiveness of IRM and ATB on various RNN architectures."
  - id: 158
    order: 129
    poster_session: 2
    session_id: 5
    title: "Semi-Supervised Learning with Taxonomic Labels"
    authors:
      - author: "Jong-Chyi Su (Facebook)"
      - author: "Subhransu Maji (University of Massachusetts, Amherst)"
    all_authors: "Jong-Chyi Su and Subhransu Maji"
    code: "https://github.com/cvl-umass/ssl-evaluation"
    keywords:
      - word: "semi-supervised learning"
      - word: "taxonomy"
      - word: "hierarchical prediction"
      - word: "fine-grained classification"
      - word: "self-supervised learning"
      - word: "coarse labels"
    paper: "papers/0158.pdf"
    supp: ""
    abstract: "We propose techniques to incorporate coarse taxonomic labels to train image classifiers in fine-grained domains. Such labels can often be obtained with a smaller effort for fine-grained domains such as the natural world where categories are organized according to a biological taxonomy. On the Semi-iNat dataset consisting of 810 species across three Kingdoms, incorporating Phylum labels improves the Species level classification accuracy by 6% in a transfer learning setting using ImageNet pre-trained models. Incorporating the hierarchical label structure with a state-of-the-art semi-supervised learning algorithm called FixMatch improves the performance further by 1.3%. The relative gains are larger when detailed labels such as Class or Order are provided, or when models are trained from scratch. However, we find that most methods are not robust to the presence of out-of-domain data from novel classes. We propose a technique to select relevant data from a large collection of unlabeled images guided by the hierarchy which improves the robustness. Overall, our experiments show that semi-supervised learning with coarse taxonomic labels are practical for training classifiers in fine-grained domains. "
  - id: 180
    order: 130
    poster_session: 2
    session_id: 5
    title: "Feature and Label Embedding Spaces Matter in Addressing Image Classifier Bias"
    authors:
      - author: "William Thong (University of Amsterdam)"
      - author: "Cees Snoek (University of Amsterdam)"
    all_authors: "William Thong and Cees Snoek"
    code: "https://github.com/twuilliam/bias-classifiers"
    keywords:
      - word: "bias mitigation"
      - word: "model debiasing"
      - word: "fairness"
      - word: ""
    paper: "papers/0180.pdf"
    supp: "supp/0180_supp.zip"
    abstract: "This paper strives to address image classifier bias, with a focus on both feature and label embedding spaces. Previous works have shown that spurious correlations from protected attributes, such as age, gender, or skin tone, can cause adverse decisions. To balance potential harms, there is a growing need to identify and mitigate image classifier bias. First, we identify in the feature space a bias direction. We compute class prototypes of each protected attribute value for every class, and reveal an existing subspace that captures the maximum variance of the bias. Second, we mitigate biases by mapping image inputs to label embedding spaces. Each value of the protected attribute has its projection head where classes are embedded through a latent vector representation rather than a common one-hot encoding. Once trained, we further reduce in the feature space the bias effect by removing its direction. Evaluation on biased image datasets, for multi-class, multi-label and binary classifications, shows the effectiveness of tackling both feature and label embedding spaces in improving the fairness of the classifier predictions, while preserving classification performance."
  - id: 181
    order: 131
    poster_session: 2
    session_id: 5
    title: "Defensive Tensorization"
    authors:
      - author: "Adrian Bulat (Samsung AI Center, Cambridge)"
      - author: "Jean Kossaifi (NVIDIA)"
      - author: "Sourav Bhattacharya (Samsung AI)"
      - author: "Yannis Panagakis (Imperial College London)"
      - author: "Timothy Hospedales (Edinburgh University)"
      - author: "Georgios Tzimiropoulos (Queen Mary University of London)"
      - author: "Nicholas Lane (Samsung AI Center Cambridge & University of Oxford)"
      - author: "Maja Pantic (Imperial College London/ Facebook)"
    all_authors: "Adrian Bulat, Jean Kossaifi, Sourav Bhattacharya, Yannis Panagakis, Timothy Hospedales, Georgios Tzimiropoulos, Nicholas Lane and Maja Pantic"
    code: ""
    keywords:
      - word: "tensors"
      - word: "tensor networks"
      - word: "tensor decomposition"
      - word: "randomization"
      - word: "adversarial defence"
      - word: "binary networks"
      - word: "network quantization"
      - word: "tensorization"
      - word: ""
    paper: "papers/0181.pdf"
    supp: "supp/0181_supp.zip"
    abstract: "We propose defensive tensorization, an adversarial defence technique that leverages a latent high-order factorization of the network. The layers of a network are first expressed as factorized tensor layers. Tensor dropout is then applied in the latent subspace, therefore resulting in dense reconstructed weights, without the sparsity or perturbations typically induced by the randomization. Our approach can be readily integrated with any arbitrary neural architecture and combined with techniques like adversarial training. We empirically demonstrate the effectiveness of our approach on standard image classification benchmarks. We validate the versatility of our approach across domains and low-precision architectures by considering an audio classification task and binary networks. In all cases, we demonstrate improved performance compared to prior works."
  - id: 182
    order: 132
    poster_session: 2
    session_id: 5
    title: "Two-directional Image Retargeting with Region-Aware Texture Enhancement"
    authors:
      - author: "Dubok Park (Samsung Research, Samsung Electronics)"
    all_authors: "Dubok Park"
    code: ""
    keywords:
      - word: "image retargeting"
      - word: "saliency"
      - word: "importance map"
      - word: "image enhancement"
      - word: ""
    paper: "papers/0182.pdf"
    supp: "supp/0182_supp.zip"
    abstract: "In this paper, a novel framework for image retargeting based on two-directional non-uniform scaling with region-aware texture enhancement is proposed. First, the importance map is estimated via saliency and depth information from an input image. Then, two-directional scaling functions are derived from importance map by 1D projection onto horizontal and vertical axis according to estimated scaling portion which controls the resizing rate of each direction. Finally, the target image is generated by two-directional non-uniform scaling with region-aware guidance map. Experimental results validate the proposed framework can achieve content-preserving and visually pleasing results compared to the conventional methods."
  - id: 183
    order: 236
    poster_session: 3
    session_id: 8
    title: "ASFormer: Transformer for Action Segmentation"
    authors:
      - author: "Fangqiu Yi (Peking University)"
      - author: "Hongyu Wen (Peking Universiity)"
      - author: "Tingting Jiang (Peking University)"
    all_authors: "Fangqiu Yi, Hongyu Wen and Tingting Jiang"
    code: "https://github.com/ChinaYi/ASFormer"
    keywords:
      - word: "action segmentation"
      - word: "transformer"
      - word: "action detection"
      - word: ""
    paper: "papers/0183.pdf"
    supp: "supp/0183_supp.zip"
    abstract: "Algorithms for the action segmentation task typically use temporal models to predict what action is occurring at each frame for a minute-long daily activity. Recent studies have shown the potential of Transformer in modeling the relations among elements in sequential data. However, there are several major concerns when directly applying the Transformer to the action segmentation task, such as the lack of inductive biases with small training sets, the deficit in processing long input sequence, and the limitation of the decoder architecture to utilize temporal relations among multiple action segments to refine the initial predictions. To address these concerns, we design an efficient Transformer-based model for the action segmentation task, named ASFormer, with three distinctive characteristics: (i) We explicitly bring in the local connectivity inductive priors because of the high locality of features. It constrains the hypothesis space within a reliable scope, and is beneficial for the action segmentation task to learn a proper target function with small training sets. (ii) We apply a pre-defined hierarchical representation pattern that efficiently handles long input sequences. (iii) We carefully design the decoder to refine the initial predictions from the encoder.  Extensive experiments on three public datasets demonstrate the effectiveness of our methods. Code is available at url{https://github.com/ChinaYi/ASFormer}."
  - id: 190
    order: 237
    poster_session: 3
    session_id: 8
    title: "A Closer Look at Few-Shot Video Classification: A New Baseline and Benchmark"
    authors:
      - author: "Zhenxi Zhu (Nanjing University)"
      - author: "Limin Wang (Nanjing University)"
      - author: "Sheng Guo (Ant Group)"
      - author: "Gangshan Wu (Nanjing University)"
    all_authors: "Zhenxi Zhu, Limin Wang, Sheng Guo and Gangshan Wu"
    code: "https://github.com/MCG-NJU/FSL-Video"
    keywords:
      - word: "few-shot learning"
      - word: "classifier-based baseline"
      - word: "new benchmark"
      - word: "action recognition"
      - word: ""
    paper: "papers/0190.pdf"
    supp: ""
    abstract: "The existing few-shot video classification methods often employ a meta-learning paradigm by designing customized temporal alignment module for similarity calculation. While significant progress has been made, these methods fail to focus on learning effective representations, and heavily rely on the ImageNet pre-training, which might be unreasonable for the few-shot recognition setting due to semantics overlap. In this paper, we aim to present an in-depth study on few-shot video classification by making three contributions. First, we perform a consistent comparative study on the existing metric-based methods to figure out their limitations in representation learning. Accordingly, we propose a simple classifier-based baseline without any temporal alignment that surprisingly outperforms the state-of-the-art meta-learning based methods. Second, we discover that there is a high correlation between the novel action class and the ImageNet object class, which is problematic in the few-shot recognition setting. Our results show that the performance of training from scratch drops significantly, which implies that the existing benchmarks cannot provide enough base data. Finally, we present a new benchmark with more base data to facilitate future few-shot video classification without pre-training. The code will be made available at https://github.com/MCG-NJU/FSL-Video."
  - id: 193
    order: 21
    poster_session: 1
    session_id: 2
    title: "Learning Synergistic Attention for Light Field Salient Object Detection"
    authors:
      - author: "Yi Zhang (INSA Rennes)"
      - author: "Geng Chen (Northwestern Polytechnical University)"
      - author: "Qian Chen (University of Science and Technology of China)"
      - author: "Yujia Sun (Inner Mongolia University)"
      - author: "Yong Xia (Northwestern Polytechnical University, Research & Development Institute of Northwestern Polytechnical University in Shenzhen)"
      - author: "Olivier DEFORGES (INSA Rennes)"
      - author: "Wassim Hamidouche (INSA Rennes)"
      - author: "Lu Zhang (INSA Rennes)"
    all_authors: "Yi Zhang, Geng Chen, Qian Chen, Yujia Sun, Yong Xia, Olivier DEFORGES, Wassim Hamidouche and Lu Zhang"
    code: "https://github.com/PanoAsh/SA-Net"
    keywords:
      - word: "light field"
      - word: "focal stack"
      - word: "salient object detection"
      - word: "attention"
    paper: "papers/0193.pdf"
    supp: "supp/0193_supp.zip"
    abstract: "In this work, we propose Synergistic Attention Network (SA-Net) to address the light field salient object detection by establishing a synergistic effect between multi-modal features with advanced attention mechanisms. Our SA-Net exploits the rich information of focal stacks via 3D convolutional neural networks, decodes the high-level features of multi-modal light field data with two cascaded synergistic attention modules, and predicts the saliency map using an effective feature fusion module in a progressive manner. Extensive experiments on three widely-used benchmark datasets show that our SA-Net outperforms 28 state-of-the-art models, sufficiently demonstrating its effectiveness and superiority. Our code is available at https://github.com/PanoAsh/SA-Net."
  - id: 195
    order: 6
    oral_session: 2
    poster_session: 1
    session_id: 1
    title: "Towards Monocular Shape from Refraction"
    authors:
      - author: "Antonin Sulc (University of Konstanz)"
      - author: "Imari Sato (National Institute of Informatics)"
      - author: "Bastian Goldluecke (University of Konstanz)"
      - author: "Tali Treibitz (University of Haifa)"
    all_authors: "Antonin Sulc, Imari Sato, Bastian Goldluecke and Tali Treibitz"
    code: "https://github.com/sulcantonin/SfR-BMVC21"
    keywords:
      - word: "shape from x"
      - word: "refraction"
      - word: "optimisation"
      - word: ""
    paper: "papers/0195.pdf"
    supp: "supp/0195_supp.zip"
    abstract: "Refraction is a common physical phenomenon and has long been researched in computer vision. Objects imaged through a refractive object appear distorted in the image as a function of the shape of the interface between the media. This hinders many computer vision applications, but can be utilized for obtaining the geometry of the refractive interface. Previous approaches for refractive surface recovery largely relied on various priors or additional information like multiple images of the analyzed surface. In contrast, we claim that a simple energy function based on Snell's law enables the reconstruction of an arbitrary refractive surface geometry using just a single image and known background texture and geometry. In the case of a single point, Snell's law has two degrees of freedom, therefore to estimate a surface depth, we need additional information. We show that solving for an entire surface at once introduces implicit parameter-free spatial regularization and yields convincing results when an intelligent initial guess is provided. We demonstrate our approach through simulations and real-world experiments, where the reconstruction shows encouraging results in the single-frame monocular setting."
  - id: 197
    order: 22
    poster_session: 1
    session_id: 2
    title: "Does a GAN leave distinct model-specific fingerprints?"
    authors:
      - author: "Yuzhen Ding (Arizona State University)"
      - author: "Nupur  Thakur  (Arizona State University)"
      - author: "baoxin Li (Arizona State University)"
    all_authors: "Yuzhen Ding, Nupur  Thakur and baoxin Li"
    code: ""
    keywords:
      - word: "GAN"
      - word: "fingerprint"
      - word: "hierarchical Bayesian modeling"
      - word: ""
    paper: "papers/0197.pdf"
    supp: "supp/0197_supp.zip"
    abstract: " Generative Adversarial Networks (GANs) have been breaking their own records in terms of the quality of the synthesized images, which could be so high as to make it impossible to distinguish generated images from real ones by human eyes. This has raised threats to security and privacy-sensitive applications, and thus it is important to be able to tell if an image is generated by GANs, and better yet, by which GAN. The task is in a sense similar to digital image forensics for establishing image authenticity, but the literature has inconclusive reports as to whether GANs leave unique fingerprints in the generated images. In this paper, we attempt to develop a comprehensive understanding towards answering this question. We propose a model to extract fingerprints that can be viewed largely as GAN-specific. We further identify a few key components that contribute to defining the fingerprint of the generated images. Using experiments based on state-of-the-art GAN models and different datasets, we evaluate the performance of our model and verify the major conclusions of our analysis."
  - id: 209
    order: 133
    poster_session: 2
    session_id: 5
    title: "DU-DARTS: Decreasing the Uncertainty of Differentiable Architecture Search"
    authors:
      - author: "Shun Lu (Institute of Computing Technology, Chinese Academy of Sciences)"
      - author: "Yu Hu (Institute of Computing Technology, Chinese Academy of Sciences)"
      - author: "Longxing Yang (Institute of Computing Technology, Chinese Academy of Sciences)"
      - author: "Zihao Sun (Institute of Computing Technology, Chinese Academy of Sciences)"
      - author: "Jilin Mei (Institute of Computing Technology, Chinese Academy of Sciences)"
      - author: "Yiming Zeng (Tecent ADlab)"
      - author: "Xiaowei Li (Institute Of Computing Technology Chinese Academy Of Sciences)"
    all_authors: "Shun Lu, Yu Hu, Longxing Yang, Zihao Sun, Jilin Mei, Yiming Zeng and Xiaowei Li"
    code: "https://github.com/ShunLu91/DU-DARTS"
    keywords:
      - word: "neural architecture search"
      - word: "differentiable NAS"
      - word: "decrease uncertainty"
    paper: "papers/0209.pdf"
    supp: "supp/0209_supp.zip"
    abstract: "Differentiable Neural Architecture Search (DARTS) recently attracts a lot of research attentions because of its high efficiency. However, the competition of candidate operations in DARTS introduces high uncertainty for selecting the truly important operation, thus leading to serious performance collapse. In this work, we decrease the uncertainty of differentiable architecture search (DU-DARTS) by enforcing the distribution of architecture parameters to approach the one-hot categorical distribution and by replacing the zero operation with a gate switch. Without any extra search cost, our method achieves state-of-the-art performance with 2.32%, 16.74% and 24.1% test error on CIFAR-10, CIFAR-100 and ImageNet datasets, respectively. Moreover, DU-DARTS can robustly find an excellent architecture on NAS-Bench-1Shot1, which further demonstrates the effectiveness of our method."
  - id: 212
    order: 23
    poster_session: 1
    session_id: 2
    title: "Make Baseline Model Stronger: Embedded Knowledge Distillation in Weight-Sharing Based Ensemble Network"
    authors:
      - author: "Shuchang LYU (Beihang University)"
      - author: "Qi Zhao (Beihang University)"
      - author: "Yujing Ma (BeiHang University)"
      - author: "lijiang chen (Beihang University)"
    all_authors: "Shuchang LYU, Qi Zhao, Yujing Ma and Lijiang Chen"
    code: ""
    keywords:
      - word: "knowledge distillation"
      - word: "ensemble learning"
      - word: "high-efficiency network"
    paper: "papers/0212.pdf"
    supp: "supp/0212_supp.zip"
    abstract: "Recently, many notable convolutional neural networks have powerful performance with compact and efficient structure. To further pursue performance improvement, previous methods either introduce more computation or design complex modules. In this paper, we propose an elegant weight-sharing based ensemble network embedded knowledge distillation (EKD-FWSNet) to enhance the generalization ability of baseline models with no increase of computation and complex modules. Specifically, we first design an auxiliary branch alongside with baseline model, then set branch points and shortcut connections between two branches to construct different forward paths. In this way, we form a weight-sharing ensemble network with multiple output predictions. Furthermore, we integrate the information from diverse posterior probabilities and intermediate feature maps, which are then transferred to baseline model through knowledge distillation strategy. Extensive image classification experiments on CIFAR-10/100 and tiny-ImageNet datasets demonstrate that our proposed EKD-FWSNet can help numerous baseline models improve the accuracy by large margin (sometimes more than 4%). We also conduct extended experiments on remote sensing datasets (AID, NWPU-RESISC45, UC-Merced) and achieve state-of-the-art results."
  - id: 218
    order: 134
    poster_session: 2
    session_id: 5
    title: "Equivariance-bridged SO(2)-Invariant Representation Learning using Graph Convolutional Network"
    authors:
      - author: "Sungwon Hwang (KAIST)"
      - author: "Hyungtae Lim (KAIST)"
      - author: "Hyun Myung (KAIST)"
    all_authors: "Sungwon Hwang, Hyungtae Lim and Hyun Myung"
    code: "https://github.com/deepshwang/swn_gcn"
    keywords:
      - word: "rotation-equivariance"
      - word: "representation learning"
      - word: "graph convolutional network"
    paper: "papers/0218.pdf"
    supp: "supp/0218_supp.zip"
    abstract: "Training a Convolutional Neural Network (CNN) to be robust against rotation has mostly been done with data augmentation. In this paper, another progressive vision of research direction is highlighted to encourage less dependence on data augmentation by achieving structural rotational invariance of a network. The deep equivariance-bridged SO(2) invariant network is proposed to echo such vision. First, Self-Weighted Nearest Neighbors Graph Convolutional Network (SWN-GCN) is proposed to implement Graph Convolutional Network (GCN) on the graph representation of an image to acquire rotationally equivariant representation, as GCN is more suitable for constructing deeper network than spectral graph convolution-based approaches. Then, invariant representation is eventually obtained with Global Average Pooling (GAP), a permutation-invariant operation suitable for aggregating high-dimensional representations, over the equivariant set of vertices retrieved from SWN-GCN. Our method achieves the state-of-the-art image classification performance on rotated MNIST and CIFAR-10 images, where the models are trained with a non-augmented dataset only. Quantitative validations over invariance of the representations also demonstrate strong invariance of deep representations of SWN-GCN over rotations."
  - id: 219
    order: 24
    poster_session: 1
    session_id: 2
    title: "FAR: A General Framework for Attributional Robustness"
    authors:
      - author: "Adam Ivankay (IBM Research Zurich)"
      - author: "Ivan Girardi (IBM)"
      - author: "Chiara Marchiori (IBM Research)"
      - author: "Pascal Frossard (EPFL)"
    all_authors: "Adam Ivankay, Ivan Girardi, Chiara Marchiori and Pascal Frossard"
    code: ""
    keywords:
      - word: "robustness"
      - word: "attribution robustness"
      - word: "adversarial attacks"
      - word: "explainability"
      - word: "attribution maps"
    paper: "papers/0219.pdf"
    supp: "supp/0219_supp.zip"
    abstract: "Attribution maps are popular tools for explaining neural networks' predictions. By assigning an importance value to each input dimension that represents its impact towards the outcome, they give an intuitive explanation of the decision process. However, recent work has discovered vulnerability of these maps to imperceptible adversarial changes, which can prove critical in safety-relevant domains, such as healthcare. Therefore, we define a novel generic framework for attributional robustness (FAR) as general problem formulation for training models with robust attributions. This framework consist of a generic regularization term and training objective that minimize the maximal dissimilarity of attribution maps in a local neighbourhood of the input. We show that FAR is a generalized, less constrained formulation of currently existing training methods. We then propose two new concretizations of this framework, AAT and AdvAAT, that directly optimize for both robust attributions and predictions. Experiments performed on widely used vision datasets show that our methods perform better or comparably to current ones in terms of attributional robustness while being more generally applicable. We finally show that our methods mitigate undesired dependencies between attributional robustness and some training and estimation parameters, which seem to critically affect other competitor methods."
  - id: 221
    order: 238
    poster_session: 3
    session_id: 8
    title: "MinMaxCAM: Improving object coverage for CAM-based Weakly Supervised Object Localization"
    authors:
      - author: "Kaili Wang (KU Leuven"
      - author: "University of Antwerp, imec-IDLab)"
      - author: "Jose Oramas (University of Antwerp, imec-IDLab)"
      - author: "Tinne Tuytelaars (KU Leuven)"
    all_authors: "Kaili Wang (KU Leuven, University of Antwerp, imec-IDLab), Jose Oramas and Tinne Tuytelaars"
    code: "https://github.com/shadowwkl/MinMaxCAM"
    keywords:
      - word: "weakly supervised learning"
      - word: "object localization"
      - word: "class activation map"
      - word: "WSOL"
    paper: "papers/0221.pdf"
    supp: "supp/0221_supp.zip"
    abstract: "One of the most common problems of weakly supervised object localization is that of inaccurate object coverage.  In the context of state-of-the-art methods based on ClassActivation Mapping, this is caused either by localization maps which focus, exclusively, on the most discriminative region of the objects of interest, or by activations occurring in background regions.   To address these two problems,  we propose two representation regularization mechanisms: Full Region Regularizationwhich tries to maximize the coverage of the localization map inside the object region, and Common Region Regularization which minimizes the activations occurring in background regions.  We evaluate the two regularizations on the ImageNet, CUB-200-2011 and OpenImages-segmentation datasets, and show that the proposed regularizations tackle both problems, outperforming the state-of-the-art by a significant margin."
  - id: 223
    order: 346
    poster_session: 4
    session_id: 11
    title: "HandTailor: Towards High-Precision Monocular 3D Hand Recovery"
    authors:
      - author: "Jun Lv (Shanghai Jiao Tong University)"
      - author: "Wenqiang Xu (Shanghai Jiao Tong University)"
      - author: "Lixin Yang (Shanghai Jiao Tong University)"
      - author: "Sucheng Qian (Shanghai Jiao Tong University)"
      - author: "Chongzhao Mao (Flexiv Ltd)"
      - author: "Cewu Lu (Shanghai Jiao Tong University)"
    all_authors: "Jun Lv, Wenqiang Xu, Lixin Yang, Sucheng Qian, Chongzhao Mao and Cewu Lu"
    code: "https://sites.google.com/view/handtailor"
    keywords:
      - word: "hand pose estimation"
      - word: "hand shape reconstruction"
      - word: ""
    paper: "papers/0223.pdf"
    supp: "supp/0223_supp.zip"
    abstract: "3D hand pose estimation and shape recovery are challenging tasks in computer vision. We introduce a novel framework HandTailor, which combines a learning-based hand module and an optimization-based tailor module to achieve high-precision hand mesh recovery from a monocular RGB image. The proposed hand module adapts both perspective projection and weak perspective projection in a single network towards accuracy-oriented and in-the-wild scenarios. The proposed tailor module then utilizes the coarsely reconstructed mesh model provided by the hand module as initialization to obtain better results. The tailor module is time-efficient, costs only ~8ms per frame on a modern CPU. We demonstrate that HandTailor can get state-of-the-art performance on several public benchmarks, with impressive qualitative results. Code and video are available on our project webpage https://sites.google.com/view/handtailor."
  - id: 229
    order: 117
    oral_session: 4
    poster_session: 2
    session_id: 4
    title: "The Curious Layperson: Fine-Grained Image Recognition without Expert Labels"
    authors:
      - author: "Subhabrata Choudhury (University of Oxford)"
      - author: "Iro Laina (University of Oxford)"
      - author: "Christian Rupprecht (University of Oxford)"
      - author: "Andrea Vedaldi (Oxford University)"
    all_authors: "Subhabrata Choudhury, Iro Laina, Christian Rupprecht and Andrea Vedaldi"
    code: "https://github.com/subhc/clever"
    keywords:
      - word: "fine-grained recognition"
      - word: "weakly-supervised recognition"
      - word: "fine-grained retrieval"
      - word: "unsupervised recognition"
      - word: "image-to-text retrieval"
      - word: "text-to-image retrieval"
      - word: "image classification"
    paper: "papers/0229.pdf"
    supp: "supp/0229_supp.zip"
    abstract: "Most of us are not experts in specific fields, such as ornithology. Nonetheless, we do have general image and language understanding capabilities that we use to match what we see to expert resources. This allows us to expand our knowledge and perform novel tasks without ad-hoc external supervision. On the contrary, machines have a much harder time consulting expert-curated knowledge bases unless trained specifically with that knowledge in mind. Thus, in this paper we consider a new problem: fine-grained image recognition without expert annotations, which we address by leveraging the vast knowledge available in web encyclopedias. First, we learn a model to describe the visual appearance of objects using non-expert image descriptions. We then train a fine- grained textual similarity model that matches image descriptions with documents on a sentence-level basis. We evaluate the method on two datasets and compare with several strong baselines and the state of the art in cross-modal retrieval. Code is available at: https://github.com/subhc/clever."
  - id: 234
    order: 135
    poster_session: 2
    session_id: 5
    title: "Contextual Convolution Blocks"
    authors:
      - author: "David Marwood (Google, Inc)"
      - author: "Shumeet Baluja (Google, Inc.)"
    all_authors: "David Marwood and Shumeet Baluja"
    code: ""
    keywords:
      - word: "spatially selective features"
      - word: "convolutional layer"
      - word: "cc-block"
      - word: "self-attention"
      - word: "se-block"
      - word: "squeeze and excitation"
      - word: "excitation map"
    paper: "papers/0234.pdf"
    supp: ""
    abstract: "A fundamental processing layer of modern deep neural networks is the 2D convolution.  It applies a filter uniformly across the input, effectively creating feature detectors that are translation invariant.  In contrast, fully-connected layers are spatially selective, allowing unique detectors across the input.  However, full connectivity comes at the expense of an enormous number of free parameters to be trained, the associated difficulty in learning without over-fitting, and the loss of spatial coherence. We introduce Contextual Convolution Blocks, a novel method to create spatially selective feature detectors that are locally translation invariant. This increases the expressive power of the network beyond standard convolutional layers and allows learning unique filters for distinct regions of the input. The filters no longer need to be discriminative in regions not likely to contain the target features. This is a generalization of the Squeeze-and-Excitation architecture that introduces minimal extra parameters. We provide experimental results on three datasets and a thorough exploration into how the increased expressiveness is instantiated."
  - id: 236
    order: 347
    poster_session: 4
    session_id: 11
    title: "Temporal-Spatial Graph Attention Networks for DCE-MRI Breast Tumor Segmentation"
    authors:
      - author: "Tianxu Lv (Jiangnan University)"
      - author: "Xiang Pan (Jiangnan University)"
    all_authors: "Tianxu Lv and Xiang Pan"
    code: ""
    keywords:
      - word: "Breast Tumor Segmentation"
      - word: "DCE-MRI"
      - word: "Graph Neural Networks"
      - word: "Temporal-Spatial Attention"
    paper: "papers/0236.pdf"
    supp: ""
    abstract: "Recent researches on medical image segmentation resort to the combination of natural image segmentation models and medical domain knowledge. However, prior methods only focus on single image segmentation or 3D convolutional operation based volume segmentation, and overlook the spatial correlations of inter-slice and temporal correlations of the inter-sequence in DCE-MRI images. In this paper, we propose a novel end-to-end temporal-spatial graph attention network (TSGAN),  which precisely segments tumor of 4D (volume space, time) DCE-MRI images by conjointly exploiting the spatial contextual dependency of inter-slice and temporal contextual dependency of inter-sequence. Specially, we design a graph temporal attention module to integrate the temporal-spatial representations hidden in 4D data into deep segmentation. The spatial dependency is learnt by graph attention operation, which attends over its neighbourhoods' features for each vertex. Meanwhile, the spatial representations learnt by the graph attention layer are combined with the temporal representations by a temporal attention operator. Then the temporal dependency is exploited by spreading on the graph. We also design a  tumour structural similarity (TSS) loss used to exploit the tumour structural dependency and enhance inter-voxel similarity within the same tissue for segmentation. We demonstrate that the proposed model outperforms recent state-of-the-art methods through comprehensive experiments."
  - id: 242
    order: 239
    poster_session: 3
    session_id: 8
    title: "PlaneRecNet: Multi-Task Learning with Cross-Task consistency for Piece-Wise Plane Detection and Reconstruction from a Single RGB Image"
    authors:
      - author: "Yaxu Xie (DFKI)"
      - author: "Fangwen Shu (DFKI)"
      - author: "Jason Rambach (DFKI)"
      - author: "A. Pagani (DFKI)"
      - author: "Didier Stricker (DFKI)"
    all_authors: "Yaxu Xie, Fangwen Shu, Jason Rambach, A. Pagani and Didier Stricker"
    code: "https://github.com/EryiXie/PlaneRecNet"
    keywords:
      - word: "planar reconstruction"
      - word: "multi-task learning"
      - word: "cross-task consistency"
      - word: "instance segmentation"
      - word: "monocular depth estimation"
    paper: "papers/0242.pdf"
    supp: "supp/0242_supp.zip"
    abstract: "Piece-wise 3D planar reconstruction provides holistic scene understanding of man-made environments, especially for indoor scenarios. Most recent approaches focused on improving the segmentation and reconstruction results by introducing advanced network architectures but overlooked the dual characteristics of piece-wise planes as objects and geometric models. Different from other existing approaches, we start from enforcing cross-task consistency for our multi-task convolutional neural network, PlaneRecNet, which integrates a single-stage instance segmentation network for piece-wise planar segmentation and a depth decoder to reconstruct the scene from a single RGB image.  To achieve this, we introduce several novel loss functions (geometric constraint) that jointly improve the accuracy of piece-wise planar segmentation and depth estimation. Meanwhile, a novel Plane Prior Attention module is used to guide depth estimation with the awareness of plane instances. Exhaustive experiments are conducted in this work to validate the effectiveness and efficiency of our method."
  - id: 243
    order: 240
    poster_session: 3
    session_id: 8
    title: "Stacked Temporal Attention: Improving First-person Action Recognition by Emphasizing Discriminative Clips"
    authors:
      - author: "Lijin Yang (The University of Tokyo)"
      - author: "Yifei Huang (The University of Tokyo)"
      - author: "Yusuke Sugano (The University of Tokyo)"
      - author: "Yoichi Sato (University of Tokyo)"
    all_authors: "Lijin Yang, Yifei Huang, Yusuke Sugano and Yoichi Sato"
    code: ""
    keywords:
      - word: "Egocentric action recognition"
      - word: "Action recognition"
      - word: "Temporal attention"
    paper: "papers/0243.pdf"
    supp: "supp/0243_supp.zip"
    abstract: "First-person action recognition is a challenging task in video understanding. Because of strong ego-motion and a limited field of view, many backgrounds or noisy frames in a first-person video can distract an action recognition model during its learning process. To encode more discriminative features, the model needs to have the ability to focus on the most relevant part of the video for action recognition. Previous works explored to address this problem by applying temporal attention but failed to consider the global context of the full video, which is critical for determining the relatively significant parts.
In this work, we propose a simple yet effective Stacked Temporal Attention Module (STAM) to compute temporal attention based on the global knowledge across clips for emphasizing the most discriminative features. We achieve this by stacking multiple self-attention layers. Instead of naive stacking, which is experimentally proven to be ineffective, we carefully design the input to each self-attention layer so that both local and global context of the video is considered during generating the temporal attention weights. Experiments demonstrate that our proposed STAM can be built on top of most existing backbones and boost the performance in various datasets."
  - id: 244
    order: 136
    poster_session: 2
    session_id: 5
    title: "Transformer-based Monocular Depth Estimation with Attention Supervision"
    authors:
      - author: "Wenjie Chang (University of Science and Technology of China)"
      - author: "Yueyi Zhang (University of Science and Technology of China)"
      - author: "Zhiwei Xiong (University of Science and Technology of China)"
    all_authors: "Wenjie Chang, Yueyi Zhang and Zhiwei Xiong"
    code: "https://github.com/WJ-Chang-42/ASTransformer"
    keywords:
      - word: "transformer"
      - word: "depth estimation"
      - word: ""
    paper: "papers/0244.pdf"
    supp: "supp/0244_supp.zip"
    abstract: "Transformer, which excels in capturing long-range dependencies, has shown great performance in a variety of computer vision tasks. In this paper, we propose a hybrid network with a Transformer-based encoder and a CNN-based decoder for monocular depth estimation. The encoder follows the architecture of classical Vision Transformer. To better exploit the potential of the Transformer encoder, we introduce the Attention Supervision to the Transformer layer, which enhances the representative ability. The down-sampling operations before the Transformer encoder lead to degradation of the details in the predicted depth map. Thus, we devise an Attention-based Up-sample Block and deploy it to compensate the texture features. Experiments on both indoor and outdoor datasets demonstrate that the proposed method achieves the state-of-the-art performance on both quantitative and qualitative evaluations. The source code and trained models can be downloaded at https://github.com/WJ-Chang-42/ASTransformer."
  - id: 251
    order: 137
    poster_session: 2
    session_id: 5
    title: "A Probabilistic Hard Attention Model For Sequentially Observed Scenes"
    authors:
      - author: "Samrudhdhi B Rangrej (McGill University)"
      - author: "James J. Clark (McGill University)"
    all_authors: "Samrudhdhi B Rangrej and James J. Clark"
    code: "https://github.com/samrudhdhirangrej/Probabilistic-Hard-Attention"
    keywords:
      - word: "hard attention"
      - word: "variational autoencoder"
      - word: "normalizing flows"
      - word: "classification"
      - word: "probabilistic model"
    paper: "papers/0251.pdf"
    supp: "supp/0251_supp.zip"
    abstract: "A visual hard attention model actively selects and observes a sequence of subregions in an image to make a prediction. The majority of hard attention models determine the attention-worthy regions by first analyzing a complete image. However, it may be the case that the entire image is not available initially but instead sensed gradually through a series of partial observations. In this paper, we design an efficient hard attention model for classifying such sequentially observed scenes. The presented model never observes an image completely. To select informative regions under partial observability, the model uses Bayesian Optimal Experiment Design. First, it synthesizes the features of the unobserved regions based on the already observed regions. Then, it uses the predicted features to estimate the expected information gain (EIG) attained, should various regions be attended. Finally, the model attends to the actual content on the location where the EIG mentioned above is maximum. The model uses a) a recurrent feature aggregator to maintain a recurrent state, b)  a linear classifier to predict the class label, c) a Partial variational autoencoder to predict the features of unobserved regions. We use normalizing flows in Partial VAE to handle multi-modality in the feature-synthesis problem. We train our model using a differentiable objective and test it on five datasets. Our model gains 2-10% higher accuracy than the baseline models when both have seen only a couple of glimpses."
  - id: 254
    order: 348
    poster_session: 4
    session_id: 11
    title: "MorphGAN: One-Shot Face Synthesis GAN for Detecting Recognition Bias"
    authors:
      - author: "Nataniel Ruiz (Boston University)"
      - author: "Barry Theobald (Apple)"
      - author: "Anurag Ranjan (Apple)"
      - author: "Ahmed Hussen Abdelaziz (Apple)"
      - author: "Nicholas   Apostoloff (Apple Inc.)"
    all_authors: "Nataniel Ruiz, Barry Theobald, Anurag Ranjan, Ahmed Hussen Abdelaziz and Nicholas   Apostoloff"
    code: ""
    keywords:
      - word: "face synthesis"
      - word: "face GAN"
      - word: "face recognition bias"
      - word: "face recognition robustness"
      - word: "face manipulation"
      - word: "face reenactment"
      - word: "fairness"
      - word: "bias"
      - word: "robustness"
      - word: "face generation"
    paper: "papers/0254.pdf"
    supp: "supp/0254_supp.zip"
    abstract: "To detect bias in face recognition networks, it can be useful to probe a network under test using samples in which attributes vary in some controlled way.  However, capturing a sufficiently large dataset with specific control over the attributes of interest is difficult. In this work, we describe a simulator that applies specific head pose and facial expression adjustments to images of previously unseen people.  The simulator first fits a 3D morphable model to a provided image, applies the desired head pose and facial expression controls, then renders the model into an image.  Next, a conditional Generative Adversarial Network (GAN) conditioned on the original image and the rendered morphable model is used to produce the image of the original person with the new facial expression and head pose. We call this conditional GAN -- MorphGAN.

Images generated using MorphGAN conserve the identity of the person in the original image, and the provided control over head pose and facial expression allows test sets to be created to identify robustness issues of a facial recognition network with respect to pose and expression.  Images generated by MorphGAN can also be used to augment training data. We show that augmenting small datasets of faces with new poses and expressions improves the recognition performance by up to 9% depending on the augmentation and data scarcity."
  - id: 255
    order: 25
    poster_session: 1
    session_id: 2
    title: "Adversarial Robustness Comparison of Vision Transformer and MLP-Mixer to CNNs"
    authors:
      - author: "Philipp Benz (KAIST)"
      - author: "Soomin Ham (KAIST)"
      - author: "Chaoning Zhang (KAIST)"
      - author: "Adil Karjauv (KAIST)"
      - author: "In So Kweon (KAIST)"
    all_authors: "Philipp Benz, Soomin Ham, Chaoning Zhang, Adil Karjauv and In So Kweon"
    code: "https://github.com/phibenz/robustness_comparison_vit_mlp-mixer_cnn"
    keywords:
      - word: "Adversarial Examples"
      - word: "Adversarial Perturbation"
      - word: "Adversarial Robustness"
      - word: "Robustness"
      - word: "Adversarial Transferability"
      - word: "Vision Transformers"
      - word: "ViT"
      - word: "MLP-Mixer"
      - word: "CNN"
    paper: "papers/0255.pdf"
    supp: ""
    abstract: "Convolutional Neural Networks (CNNs) have become the de facto gold standard in computer vision applications in the past years. Recently, however, new model architectures have been proposed challenging the status quo. The Vision Transformer (ViT) relies solely on attention modules, while the MLP-Mixer architecture substitutes the self-attention modules with Multi-Layer Perceptrons (MLPs). Despite their great success, CNNs have been widely known to be vulnerable to adversarial attacks, causing serious concerns for security-sensitive applications. Thus, it is critical for the community to know whether the newly proposed ViT and MLP-Mixer are also vulnerable to adversarial attacks. To this end, we empirically evaluate their adversarial robustness under several adversarial attack setups and benchmark them against the widely used CNNs. Overall, we find that the two architectures, especially ViT, are more robust than their CNN models. Using a toy example, we also provide empirical evidence that the lower adversarial robustness of CNNs can be partially attributed to their shift-invariant property. Our frequency analysis suggests that the most robust ViT architectures tend to rely more on low-frequency features compared with CNNs. Additionally, we have an intriguing finding that MLP-Mixer is extremely vulnerable to universal adversarial perturbations."
  - id: 259
    order: 26
    poster_session: 1
    session_id: 2
    title: "PDF-Distil: including Prediction Disagreements in Feature-based Distillation for object detection"
    authors:
      - author: "Heng ZHANG (Univ Rennes 1)"
      - author: "Elisa Fromont (Université Rennes 1, IRISA/INRIA rba)"
      - author: "Sébastien Lefèvre (Université de Bretagne Sud / IRISA)"
      - author: "Bruno AVIGNON (Atermes)"
    all_authors: "Heng ZHANG, Elisa Fromont, Sébastien Lefèvre and Bruno AVIGNON"
    code: "https://github.com/ZHANGHeng19931123/MutualGuide"
    keywords:
      - word: "knowledge distillation: object detection"
      - word: ""
    paper: "papers/0259.pdf"
    supp: ""
    abstract: "Knowledge distillation aims at compressing deep models by transferring the learned knowledge from precise but cumbersome teacher models to compact student models. Due to the extreme imbalance between the foreground and the background of images, when traditional knowledge distillation methods are directly applied to the object detection task, there is a large performance gap between the teacher model and the student model. We tackle this imbalance problem from a sampling perspective, and we propose to include the teacher-student prediction disagreements into a feature-based knowledge distillation framework. This is done with PDF-Distil by dynamically generating a weighting mask applied to the knowledge distillation loss, based on the disagreements between the predictions of both models. Extensive experiments on PASCAL VOC and MS COCO datasets demonstrate that, compared to state-of-the-art methods, PDF-Distil is able to better reduce the performance gap between the teacher and student models."
  - id: 263
    order: 27
    poster_session: 1
    session_id: 2
    title: "PhIT-Net: Photo-consistent Image Transform for Robust Illumination Invariant Matching"
    authors:
      - author: "Damian Kaliroff (Technion)"
      - author: "Guy Gilboa (Technion)"
    all_authors: "Damian Kaliroff and Guy Gilboa"
    code: "https://github.com/dkaliroff/phitnet"
    keywords:
      - word: "invariant representations"
      - word: "illumination invariance"
      - word: "image transform"
      - word: "photo-consistency"
      - word: "change detection"
      - word: "patch matching"
      - word: "triplet loss"
      - word: "self-supervised learning"
      - word: ""
    paper: "papers/0263.pdf"
    supp: "supp/0263_supp.zip"
    abstract: "We propose a new and completely data-driven approach for generating a photo- consistent image transform. We show that simple classical algorithms which operate in the transform domain become extremely resilient to illumination changes. This considerably improves matching accuracy, outperforming the use of state-of-the-art invariant representations as well as new matching methods based on deep features. 
The transform is obtained by training a neural network with a specialized triplet loss, designed to emphasize actual scene changes while attenuating illumination changes. The transform yields an illumination invariant representation, structured as an image map, which is highly flexible and can be easily used for various tasks.  "
  - id: 264
    order: 349
    poster_session: 4
    session_id: 11
    title: "MVT: Multi-view Vision Transformer for 3D Object Recognition"
    authors:
      - author: "Shuo Chen (University of Amsterdam)"
      - author: "Tan Yu (Baidu Research)"
      - author: "Ping Li (Baidu Research)"
    all_authors: "Shuo Chen, Tan Yu and Ping Li"
    code: ""
    keywords:
      - word: "3D object recognition"
      - word: "Transformer-based methods"
      - word: ""
    paper: "papers/0264.pdf"
    supp: ""
    abstract: "Inspired by the great success achieved by CNN in image recognition, view-based methods applied CNNs to model the projected views for 3D object understanding and achieved excellent performance. Nevertheless, multi-view CNN models cannot model the communications between patches from different views, limiting its effectiveness in 3D object recognition. Inspired by the recent success gained by vision Transformer in image recognition, we propose a Multi-view Vision Transformer (MVT) for 3D object recognition. Since each patch feature in a Transformer block has a global reception field, it naturally achieves communications between patches from different views. Meanwhile, it takes much less inductive bias compared with its CNN counterparts. Considering both effectiveness and efficiency, we develop a global-local structure for our MVT. Our experiments on two public benchmarks, ModelNet40 and ModelNet10, demonstrate the competitive performance of our MVT."
  - id: 265
    order: 241
    poster_session: 3
    session_id: 8
    title: "Diagnosing Errors in Video Relation Detectors"
    authors:
      - author: "Shuo Chen (University of Amsterdam)"
      - author: "Pascal Mettes (University of Amsterdam)"
      - author: "Cees Snoek (University of Amsterdam)"
    all_authors: "Shuo Chen, Pascal Mettes and Cees Snoek"
    code: "https://github.com/shanshuo/DiagnoseVRD"
    keywords:
      - word: "video relation detection"
      - word: "error diagnosis"
    paper: "papers/0265.pdf"
    supp: "supp/0265_supp.zip"
    abstract: "Video relation detection forms a new and challenging problem in computer vision, where subjects and objects need to be spatio-temporally localized and a predicate label needs to be assigned if and only if there is an interaction between the two. Despite recent progress in video relation detection, overall performance is still marginal and it remains unclear what the key factors are towards solving the problem. Following examples set in the object detection and action localization literature, we perform a deep dive into the error diagnosis of current video relation detection approaches. We introduce a diagnostic tool for analyzing the sources of detection errors. Our tool evaluates and compares current approaches beyond a single scalar metric, the mean Average Precision, by defining different error types specific to video relation detection, used for false positive analyses. Moreover, we examine different factors of influence on the performance in false negative analyses, including relation length, number of subject/object/predicate instances, and subject/object size. Finally, we present the effect on the video related performances with an oracle fix for each error type. On the existing video relation benchmarks, we show where current approaches excel and fall short, allowing us to pinpoint the most important future directions in the field."
  - id: 271
    order: 350
    poster_session: 4
    session_id: 11
    title: "Learning Texture Generators for 3D Shape Collections from Internet Photo Sets"
    authors:
      - author: "Rui Yu (USTC)"
      - author: "Yue Dong (Microsoft Research Asia)"
      - author: "Pieter Peers (College of William & Mary)"
      - author: "Xin Tong (Microsoft)"
    all_authors: "Rui Yu, Yue Dong, Pieter Peers and Xin Tong"
    code: ""
    keywords:
      - word: "Texture generation"
      - word: "GAN"
    paper: "papers/0271.pdf"
    supp: "supp/0271_supp.zip"
    abstract: "We present a method for decorating existing 3D shape collections by learning a texture generator from internet photo collections.  We condition the StyleGAN texture generation by injecting multiview silhouettes of a 3D shape with
SPADE-IN.  To bridge the inherent domain gap between the multiview silhouettes from the shape collection and the distribution of silhouettes in the photo collection, we employ a mixture of silhouettes from both collections for training. Furthermore, we do not assume each exemplar in the photo collection is viewed from more than one vantage point, and leverage multiview discriminators to promote semantic view-consistency over the generated textures.  We validate and demonstrate the efficacy of our design on three real-world 3D shape collections."
  - id: 282
    order: 138
    poster_session: 2
    session_id: 5
    title: "Mitigating Bias in Visual Transformers via Targeted Alignment"
    authors:
      - author: "Sruthi Sudhakar (Georgia Tech)"
      - author: "Viraj Prabhu (Georgia Tech)"
      - author: "Arvindkumar Krishnakumar (Georgia Tech)"
      - author: "Judy Hoffman (Georgia Tech)"
    all_authors: "Sruthi Sudhakar, Viraj Prabhu, Arvindkumar Krishnakumar and Judy Hoffman"
    code: ""
    keywords:
      - word: "Fairness"
      - word: "Algorithmic Bias Mitigation"
      - word: "Visual Transformers"
    paper: "papers/0282.pdf"
    supp: "supp/0282_supp.pdf"
    abstract: "As transformer architectures become increasingly prevalent in computer vision, it is critical to understand their fairness implications. We perform the first study of the fairness of transformers applied to computer vision and benchmark several bias mitigation approaches from prior work. We visualize the feature space of the transformer self-attention modules and discover that a significant portion of the bias is encoded in the query matrix. With this knowledge, we propose TADeT, a targeted alignment strategy for debiasing transformers that aims to discover and remove bias primarily from query matrix features. We measure performance using Balanced Accuracy and Standard Accuracy, and fairness using Equalized Odds and Balanced Accuracy Difference. TADeT consistently leads to improved fairness over prior work on multiple attribute prediction tasks on the CelebA dataset, without compromising performance."
  - id: 284
    order: 351
    poster_session: 4
    session_id: 11
    title: "Leveraging Geometry for Shape Estimation from a Single RGB Image"
    authors:
      - author: "Florian Maximilian Langer (Department of Engineering, University of Cambridge)"
      - author: "Ignas Budvytis (Department of Engineering, University of Cambridge)"
      - author: "Roberto Cipolla (University of Cambridge)"
    all_authors: "Florian Maximilian Langer, Ignas Budvytis and Roberto Cipolla"
    code: ""
    keywords:
      - word: "3D shape"
      - word: "CAD model"
      - word: "deformation"
      - word: "image-based retrieval"
      - word: "key points"
      - word: "geometry"
      - word: ""
    paper: "papers/0284.pdf"
    supp: "supp/0284_supp.zip"
    abstract: "Predicting 3D shapes and poses of static objects from a single RGB image is an important research area in modern computer vision. Its applications range from augmented reality to robotics and digital content creation. Typically this task is performed through direct object shape and pose predictions which is inaccurate. A promising research direction ensures meaningful shape predictions by retrieving CAD models from large scale databases and aligning them to the objects observed in the image. However, existing work does not take the object geometry into account, leading to inaccurate object pose predictions, especially for unseen objects. In this work we demonstrate how cross-domain keypoint matches from an RGB image to a rendered CAD model allow for more precise object pose predictions compared to ones obtained through direct predictions. We further show that keypoint matches can not only be used to estimate the pose of an object, but also to modify the shape of the object itself. This is important as the accuracy that can be achieved with object retrieval alone is inherently limited to the available CAD models. Allowing shape adaptation bridges the gap between the retrieved CAD model and the observed shape. We demonstrate our approach on the challenging Pix3D dataset. The proposed geometric shape prediction improves the AP mesh over the state-of-the-art from 33.2 to 37.8 on seen objects and from 8.2 to 17.1 on unseen objects. Furthermore, we demonstrate more accurate shape predictions without closely matching CAD models when following the proposed shape adaptation."
  - id: 286
    order: 352
    poster_session: 4
    session_id: 11
    title: "Higher-Order Implicit Fairing Networks for 3D Human Pose Estimation"
    authors:
      - author: "Jianning Quan (Concordia university)"
      - author: "Abdessamad Ben Hamza (Concordia University)"
    all_authors: "Jianning Quan and Abdessamad Ben Hamza"
    code: ""
    keywords:
      - word: "Human pose estimation"
      - word: "higher-order graph convolution"
      - word: "implicit fairing"
      - word: "Jacobi method"
    paper: "papers/0286.pdf"
    supp: ""
    abstract: "Estimating a 3D human pose has proven to be a challenging task, primarily because of the complexity of the human body joints, occlusions, and variability in lighting conditions. In this paper, we introduce a higher-order graph convolutional framework with initial residual connections for 2D-to-3D pose estimation. Using multi-hop neighborhoods for node feature aggregation, our model is able to capture the long-range dependencies between body joints. Moreover, our approach leverages residual connections, which are integrated by design in our network architecture, ensuring that the learned feature representations retain important information from the initial features of the input layer as the network depth increases. Experiments and ablations studies conducted on a standard benchmark demonstrate the effectiveness of our model, achieving superior performance over strong baseline methods for 3D human pose estimation."
  - id: 287
    order: 28
    poster_session: 1
    session_id: 2
    title: "Absolute Scale from Varifocal Monocular Camera through SfM and Defocus Combined"
    authors:
      - author: "Nao Mishima (Toshiba Research and Development Center)"
      - author: "Akihito Seki (Toshiba)"
      - author: "Shinsaku Hiura (University of Hyogo)"
    all_authors: "Nao Mishima, Akihito Seki and Shinsaku Hiura"
    code: ""
    keywords:
      - word: "monocular camera"
      - word: "varifocal lens"
      - word: "absolute scale"
      - word: "depth"
      - word: "structure from motion"
      - word: "depth from defocus"
      - word: "convex regression"
    paper: "papers/0287.pdf"
    supp: ""
    abstract: "The absolute scale estimation of monocular structure from motion (SfM) is still under-explored even though it is essential for robotic tasks or real-world interaction. Typically, the use of physical scale cues requires a calibration process while context scale cues introduce geometric assumptions. In this paper, we propose a novel method to obtain absolute scales of the scene and camera motion by combining monocular SfM and uncalibrated depth from defocus (DfD) which is free for zooming and focusing on each shot independently. Specifically, we exploit that the scene structure and field of view (FoV) of each camera estimated by SfM are tightly coupled to the focal length and focused distance of DfD, and the radius of the effective aperture of the lens constrains the absolute scale of the entire estimation. The effectiveness of the proposed method is verified by using a commercially available camera with a varifocal lens through various experiments."
  - id: 291
    order: 353
    poster_session: 4
    session_id: 11
    title: "Talking Head Generation with Audio and Speech Related Facial Action Units"
    authors:
      - author: "Sen Chen (College of Intelligence and Computing, Tianjin University)"
      - author: "Zhilei Liu (Tianjin University)"
      - author: "Jiaxing Liu (Tianjin University)"
      - author: "zhengxiang yan (Tianjin University)"
      - author: "Longbiao Wang (Tianjin University)"
    all_authors: "Sen Chen, Zhilei Liu, Jiaxing Liu, Zhengxiang Yan and Longbiao Wang"
    code: ""
    keywords:
      - word: "Talking Face Generation"
      - word: "Facial Action Unit"
      - word: "Generative Adversarial Network"
      - word: "Video Synthesis"
      - word: "Face Manipulation"
    paper: "papers/0291.pdf"
    supp: "supp/0291_supp.zip"
    abstract: "The task of talking head generation is to synthesize a lip synchronized talking head video by inputting an arbitrary face image and audio clips. Most existing methods ignore the local driving information of the mouth muscles. In this paper, we propose a novel recurrent generative network that uses both audio and speech-related facial action units (AUs) as the driving information. AU information related to the mouth can guide the movement of the mouth more accurately. Since speech is highly correlated with speech-related AUs, we propose an Audio-to-AU module in our system to predict the speech-related AU information from speech.   In addition, we use AU classifier to ensure that the generated images contain correct AU information. Frame discriminator is also constructed for adversarial training to improve the realism of the generated face. We verify the effectiveness of our model on the GRID dataset and TCD-TIMIT dataset. We also conduct an ablation study to verify the contribution of each component in our model. Quantitative and qualitative experiments demonstrate that our method outperforms existing methods in both image quality and lip-sync accuracy."
  - id: 296
    order: 139
    poster_session: 2
    session_id: 5
    title: "Rethinking Token-Mixing MLP for MLP-based Vision Backbone"
    authors:
      - author: "Tan Yu (Baidu Research)"
      - author: "XU LI (Baidu Research)"
      - author: "Yunfeng Cai (Baidu Research)"
      - author: "Mingming Sun (Baidu Research)"
      - author: "Ping Li (Baidu Research)"
    all_authors: "Tan Yu, XU LI, Yunfeng Cai, Mingming Sun and Ping Li"
    code: ""
    keywords:
      - word: "vision backbone"
      - word: "MLP"
      - word: "image recognition"
    paper: "papers/0296.pdf"
    supp: ""
    abstract: "In the past decade, we have witnessed the rapid progress in machine vision backbone.  By introducing the inductive bias  from the image processing, convolution neural network (CNN) has achieved excellent performance in numerous  computer vision tasks and has been established as de facto  backbone. In recent years,  inspired by the great success achieved by Transformer in NLP tasks, vision Transformer   models emerge. Using much less inductive bias, they have achieved promising performance in computer vision tasks compared with their CNN counterparts.  More recently, researchers investigate in using the pure-MLP architecture to build the vision backbone to further reduce the inductive bias,  achieving a good performance.  The pure-MLP backbone  is built upon  channel-mixing MLPs to fuse the channels and token-mixing MLPs for communications between patches.  In this paper, we re-think the design of the token-mixing MLP.  We discover that token-mixing MLPs in existing MLP-based backbones are spatial-specific, and thus it is sensitive to spatial translation. Meanwhile, the channel-agnostic  property of  the existing token-mixing MLPs limits their capability in mixing tokens.  To overcome these limitations, we propose an improved structure termed as Circulant Channel-Specific  (CCS) token-mixing MLP, which is spatial-invariant and channel-specific.  It takes fewer parameters but achieves higher classification accuracy on ImageNet1K benchmark. "
  - id: 298
    order: 242
    poster_session: 3
    session_id: 8
    title: "360° Optical Flow using Tangent Images"
    authors:
      - author: "Mingze Yuan (University of Bath)"
      - author: "Christian Richardt (University of Bath)"
    all_authors: "Mingze Yuan and Christian Richardt"
    code: "https://github.com/yuanmingze/360OpticalFlow-TangentImages"
    keywords:
      - word: "optical flow"
      - word: "360° image processing"
      - word: ""
    paper: "papers/0298.pdf"
    supp: "supp/0298_supp.zip"
    abstract: "Omnidirectional 360° images have found many promising and exciting applications in computer vision, robotics and other fields, thanks to their increasing affordability, portability and their 360° field of view. The most common format for storing, processing and visualising 360° images is equirectangular projection (ERP). However, the distortion introduced by the nonlinear mapping from 360° image to ERP image is still a barrier that holds back ERP images from being used as easily as conventional perspective images. This is especially relevant when estimating 360° optical flow, as the distortions need to be mitigated appropriately. In this paper, we propose a 360° optical flow method based on tangent images. Our method leverages gnomonic projection to locally convert ERP images to perspective images, and uniformly samples the ERP image by projection to a cubemap and regular icosahedron vertices, to incrementally refine the estimated 360° flow fields even in the presence of large rotations. Our experiments demonstrate the benefits of our proposed method both quantitatively and qualitatively."
  - id: 299
    order: 354
    poster_session: 4
    session_id: 11
    title: "Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction"
    authors:
      - author: "Ce Liu (ETH Zurich)"
      - author: "Shuhang Gu (ETH Zurich, Switzerland)"
      - author: "Luc Van Gool (ETH Zurich)"
      - author: "Radu Timofte (ETH Zurich)"
    all_authors: "Ce Liu, Shuhang Gu, Luc Van Gool and Radu Timofte"
    code: "https://github.com/cnexah/DeepLineEncoding"
    keywords:
      - word: "monocular depth perception"
      - word: "hough transform"
      - word: "3D object detection"
      - word: "depth prediction"
      - word: ""
    paper: "papers/0299.pdf"
    supp: "supp/0299_supp.zip"
    abstract: "In autonomous driving scenarios, straight lines and vanishing points are important cues for single-image depth perception. In this paper, we propose the deep line encoding to make better use of the line information in scenes. More specifically, we transform potential lines into parameter space through the deep Hough transform. The aggregation of features along a line encodes the semantics of the entire line, whereas the voting location indicates the algebraic parameters. For efficiency, we further propose the novel line pooling to select and encode the most important lines in scenes. With deep line encoding, we advance the state-of-the-art on KITTI single-image 3D object detection and depth prediction benchmarks. The code is available at https://github.com/cnexah/DeepLineEncoding."
  - id: 300
    order: 243
    poster_session: 3
    session_id: 8
    title: "Depth-only Object Tracking"
    authors:
      - author: "Song Yan (Tampere University)"
      - author: "Jinyu Yang (Southern University of Science and Technology)"
      - author: "Ales Leonardis (University of Birmingham)"
      - author: "Joni-Kristian Kamarainen (Tampere University)"
    all_authors: "Song Yan, Jinyu Yang, Ales Leonardis and Joni-Kristian Kamarainen"
    code: "https://github.com/xiaozai/DeT"
    keywords:
      - word: "RGBD tracking"
      - word: "depth tracking"
      - word: "synthetic tracking datasets"
      - word: ""
    paper: "papers/0300.pdf"
    supp: "supp/0300_supp.zip"
    abstract: "Depth (D) indicates occlusion and is less sensitive to illumination changes, which make depth attractive modality for Visual Object Tracking (VOT). Depth is used in RGBD object tracking where the best trackers are deep RGB trackers with additional heuristic using depth maps. There are two potential reasons for the heuristics: 1) the lack of large RGBD tracking datasets to train deep RGBD trackers and 2) the long-term evaluation protocol of VOT RGBD that benefits from heuristics such as depth-based occlusion detection. In this work, we study how far D-only tracking can go if trained with large amounts of depth data. To compensate the lack of depth data, we generate depth maps for tracking. We train a \"Depth-DiMP\" from the scratch with the generated data and finetune it with the available small RGBD tracking datasets. The depth-only DiMP achieves good accuracy in depth-only tracking and combined with the original RGB DiMP the end-to-end trained RGBD-DiMP outperforms the recent VOT 2020 RGBD winners."
  - id: 307
    order: 244
    poster_session: 3
    session_id: 8
    title: "Spatial-Temporal Residual Aggregation for High Resolution Video Inpainting"
    authors:
      - author: "Vishnu Sanjay Ramiya Srinivasan (Huawei)"
      - author: "Rui Ma (Huawei Canada)"
      - author: "Qiang Tang (Huawei Canada)"
      - author: "Zili Yi (Huawei Canada)"
      - author: "Zhan Xu (Huawei Canada)"
    all_authors: "Vishnu Sanjay Ramiya Srinivasan, Rui Ma, Qiang Tang, Zili Yi and Zhan Xu"
    code: "https://github.com/Ascend-Research/STRA_Net"
    keywords:
      - word: "high resolution video inpainting"
      - word: "spatial-temporal aggregation"
      - word: "residual aggregation"
      - word: "spatial-temporal attention"
      - word: "image alignment"
    paper: "papers/0307.pdf"
    supp: "supp/0307_supp.zip"
    abstract: "Recent learning-based inpainting algorithms have achieved compelling results for completing missing regions after removing undesired objects in videos. To maintain the temporal consistency among the frames, 3D spatial and temporal operations are often heavily used in the deep networks. However, these methods usually suffer from memory constraints and can only handle low resolution videos. We propose STRA-Net, a novel spatial-temporal residual aggregation framework for high resolution video inpainting. The key idea is to first learn and apply a spatial and temporal inpainting network on the downsampled low resolution videos. Then, we refine the low resolution results by aggregating the learned spatial and temporal image residuals (details) to the upsampled inpainted frames. Both the quantitative and qualitative evaluations show that we can produce more temporal-coherent and visually appealing results than the state-of-the-art methods on inpainting high resolution videos."
  - id: 308
    order: 245
    poster_session: 3
    session_id: 8
    title: "Knowing What, Where and When to Look: Video Action modelling with Attention"
    authors:
      - author: "Juan-Manuel Perez-Rua (Facebook AI)"
      - author: "Brais Martinez (Samsung AI Center)"
      - author: "Xiatian Zhu (Samsung)"
      - author: "Antoine S Toisoul (Samsung)"
      - author: "Victor A Escorcia (Samsung AI Center)"
      - author: "Tao Xiang (University of Surrey)"
    all_authors: "Juan-Manuel Perez-Rua, Brais Martinez, Xiatian Zhu, Antoine S Toisoul, Victor A Escorcia and Tao Xiang"
    code: ""
    keywords:
      - word: "Action recognition"
      - word: "Fine-grained action"
      - word: "video attention"
      - word: "Spatial attention"
      - word: "Channel attention"
      - word: "Temporal attention"
      - word: "Spatio-temporal attention"
      - word: "Feature refinement"
    paper: "papers/0308.pdf"
    supp: "supp/0308_supp.pdf"
    abstract: "Attentive video modelling is essential for action recognition in unconstrained videos 
due to their rich yet redundant information over space and time.  
However, introducing attention in a deep neural network for action recognition is challenging for two reasons. 
First, an effective attention module needs to learn {em what} (objects and their local motion patterns),
{em where} (spatially), and {em when} (temporally) to focus on. 
Second, a video attention module must be efficient because existing action recognition models already suffer from high computational cost. 
To address both challenges, a novel What-Where-When (W3) video attention module is proposed. 
Departing from existing alternatives, our W3 module models all three facets of video attention jointly. 
Crucially, it is extremely efficient by
factorising the high-dimensional video feature data into low-dimensional meaningful spaces 
(1D channel vector for `what' and 2D spatial tensors for `where'), 
followed by a lightweight temporal attention reasoning. 
Extensive experiments show that our attention model brings significant improvements to existing action recognition models, 
achieving a new state-of-the-art performance on a number of benchmarks."
  - id: 316
    order: 29
    poster_session: 1
    session_id: 2
    title: "Deep Motion Blind Video Stabilization"
    authors:
      - author: "Muhammad Kashif Ali (Hanyang University)"
      - author: "Sangjoon Yu (Hanyang University)"
      - author: "Tae Hyun Kim (Hanyang Univeristy)"
    all_authors: "Muhammad Kashif Ali, Sangjoon Yu and Tae Hyun Kim"
    code: "https://github.com/MKashifAli/Motion_Blind_Video_Stabilization"
    keywords:
      - word: "Video Stabilization"
      - word: "Video enhancement"
      - word: "Temporally Consistent Video Generation"
      - word: ""
    paper: "papers/0316.pdf"
    supp: "supp/0316_supp.zip"
    abstract: "Despite the advances in the field of generative models in computer vision, video stabilization still lacks a pure regressive deep-learning-based formulation. Deep video stabilization is generally formulated with the help of explicit motion estimation modules due to the lack of a dataset containing pairs of videos with similar perspective but different motion. Therefore, the deep learning approaches for this task have difficulties in the pixel-level synthesis of latent stabilized frames, and resort to motion estimation modules for indirect transformations of the unstable frames to stabilized frames, leading to the loss of visual content near the frame boundaries. In this work, we aim to declutter this over-complicated formulation of video stabilization with the help of a novel dataset that contains pairs of training videos with similar perspective and different motion and verify its effectiveness by successfully learning motion-blind full-frame video stabilization through employing strictly conventional generative techniques and further improve the stability through a curriculum-learning inspired adversarial training strategy. Through extensive experimentation, we show the quantitative and qualitative advantages of the proposed approach to the state-of-the-art video stabilization approaches. Moreover, our method achieves ~3x speed-up over the currently fastest video stabilization methods."
  - id: 319
    order: 30
    poster_session: 1
    session_id: 2
    title: "PS-Transformer: Learning Sparse Photometric Stereo Network using Self-Attention Mechanism"
    authors:
      - author: "Satoshi Ikehata (National Institute of Informatics)"
    all_authors: "Satoshi Ikehata"
    code: ""
    keywords:
      - word: "photometric stereo"
      - word: "transformer"
    paper: "papers/0319.pdf"
    supp: "supp/0319_supp.zip"
    abstract: "Existing deep calibrated photometric stereo networks basically aggregate observations under different lights based on the pre-defined operations such as linear projection and max pooling. While they are effective with the dense capture, simple first-order operations often fail to capture the high-order interactions among observations under small number of different lights. To tackle this issue, this paper presents a deep sparse calibrated photometric stereo network named PS-Transformer which leverages the learnable self-attention mechanism to properly capture the complex inter-image interactions. PSTransformer builds upon the dual-branch design to explore both pixel-wise and image-wise features and individual feature is trained with the intermediate surface normal supervision to maximize geometric feasibility. A new synthetic dataset named CyclesPS+ is also presented with the comprehensive analysis to successfully train the photometric stereo networks. Extensive results on the publicly available benchmark datasets demonstrate that the surface normal prediction accuracy of the proposed method significantly outperforms other state-of-the-art algorithms with the same number of input images and is even comparable to that of dense algorithms which input 10 times larger number of images."
  - id: 325
    order: 246
    poster_session: 3
    session_id: 8
    title: "FacialGAN: Style Transfer and Attribute Manipulation on Synthetic Faces"
    authors:
      - author: "Ricard Durall Lopez (ITWM Fraunhofer)"
      - author: "Jireh Jam (Manchester Metropolitan University)"
      - author: "Dominik Strassel (Fraunhofer ITWM)"
      - author: "Moi Hoon Yap (Manchester Metropolitan University)"
      - author: "Janis Keuper (Fraunhofer)"
    all_authors: "Ricard Durall Lopez, Jireh Jam, Dominik Strassel, Moi Hoon Yap and Janis Keuper"
    code: "https://github.com/cc-hpc-itwm/FacialGAN"
    keywords:
      - word: "GAN"
      - word: "attribute manipulation"
      - word: "style transfer"
      - word: "face editing"
    paper: "papers/0325.pdf"
    supp: "supp/0325_supp.zip"
    abstract: "Facial image manipulation is a generation task where the output face is shifted towards an intended target direction in terms of facial attribute and styles.  Recent works have achieved great success in various editing techniques such as style transfer and attribute translation. However, current approaches are either focusing on pure style transfer, or on the translation of predefined sets of attributes with restricted interactivity. To address this issue, we propose FacialGAN, a novel framework enabling simultaneous rich style transfers and interactive facial attributes manipulation. While preserving the identity of a source image, we transfer the diverse styles of a target image to the source image. We then incorporate the geometry information of a segmentation mask to provide a fine-grained manipulation of facial attributes.  Finally, a multi-objective learning strategy is introduced to optimize the loss of each specific tasks. Experiments on the CelebA-HQ dataset, with CelebAMask-HQ as semantic mask labels, show our model’s capacity in producing visually compelling results in style transfer, attribute manipulation, diversity and face verification. For reproducibility, we provide an interactive open-source tool to perform facial manipulations, and the Pytorch implementation of the model."
  - id: 327
    order: 247
    poster_session: 3
    session_id: 8
    title: "Render In-between: Motion Guided Video Synthesis for Action Interpolation"
    authors:
      - author: "Hsuan-I Ho (ETH Zürich)"
      - author: "Xu Chen (ETH Zürich)"
      - author: "Jie Song (ETH Zurich)"
      - author: "Otmar Hilliges (ETH Zurich)"
    all_authors: "Hsuan-I Ho, Xu Chen, Jie Song and Otmar Hilliges"
    code: "https://github.com/azuxmioy/Render-In-Between"
    keywords:
      - word: "video interpolation"
      - word: "action prediction"
      - word: "human motion modeling"
      - word: "human generation"
      - word: "human centric video"
      - word: "neural renderer"
      - word: "transformer"
    paper: "papers/0327.pdf"
    supp: "supp/0327_supp.zip"
    abstract: "Upsampling videos of human activity is an interesting yet challenging task with many potential applications ranging from gaming to entertainment and sports broadcasting. The main difficulty in synthesizing video frames in this setting stems from the highly complex and non-linear nature of human motion and the complex appearance and texture of the body. We propose to address these issues in a motion-guided frame-upsampling framework that is capable of producing realistic human motion and appearance. A novel motion model is trained to inference the non-linear skeletal motion between frames by leveraging a large-scale motion-capture dataset (AMASS). The high-frame-rate pose predictions are then used by a neural rendering pipeline to produce the full-frame output, taking the pose and background consistency into consideration. Our pipeline only requires low-frame-rate videos and unpaired human motion data but does not require high-frame-rate videos for training. Furthermore, we contribute the first evaluation dataset that consists of high-quality and high-frame-rate videos of human activities for this task. Compared with state-of-the-art video interpolation techniques, our method produces in-between frames with better quality and accuracy, which is evident by state-of-the-art results on pixel-level, distributional metrics and comparative user evaluations. Our code and the collected dataset are available at https://git.io/Render-In-Between."
  - id: 328
    order: 355
    poster_session: 4
    session_id: 11
    title: "DomainMix: Learning Generalizable Person Re-Identification Without Human Annotations"
    authors:
      - author: "Wenhao Wang (Beihang University)"
      - author: "Shengcai Liao (Inception Institute of Artificial Intelligence)"
      - author: "Fang Zhao (Inception Institute of Artificial Intelligence)"
      - author: "Cuicui Kang (MBZUAI)"
      - author: "Ling Shao (Inception Institute of Artificial Intelligence)"
    all_authors: "Wenhao Wang, Shengcai Liao, Fang Zhao, Cuicui Kang and Ling Shao"
    code: "https://github.com/WangWenhao0716/DomainMix"
    keywords:
      - word: "DomainMix"
      - word: "Person Re-Identification"
      - word: "Domain Generalization"
    paper: "papers/0328.pdf"
    supp: "supp/0328_supp.zip"
    abstract: "Existing person re-identification models often have low generalizability, which is mostly due to limited availability of large-scale labeled data in training. However, labeling large-scale training data is very expensive and time-consuming, while large-scale synthetic dataset shows promising value in learning generalizable person re-identification models. Therefore, in this paper a novel and practical person re-identification task is proposed,i.e. how to use labeled synthetic dataset and unlabeled real-world dataset to train a universal model. In this way, human annotations are no longer required, and it is scalable to large and diverse real-world datasets. To address the task, we introduce a framework with high generalizability, namely DomainMix. Specifically, the proposed method firstly clusters the unlabeled real-world images and selects the reliable clusters. During training, to address the large domain gap between two domains, a domain-invariant feature learning method is proposed, which introduces a new loss,i.e. domain balance loss, to conduct an adversarial learning between domain-invariant feature learning and domain discrimination, and meanwhile learns a discriminative feature for person re-identification. This way, the domain gap between synthetic and real-world data is much reduced, and the learned feature is generalizable thanks to the large-scale and diverse training data. Experimental results show that the proposed annotation-free method is more or less comparable to the counterpart trained with full human annotations, which is quite promising. In addition, it achieves the current state of the art on several person re-identification datasets under direct cross-dataset evaluation."
  - id: 329
    order: 140
    poster_session: 2
    session_id: 5
    title: "ESAD: End-to-end Semi-supervised Anomaly Detection"
    authors:
      - author: "Chaoqin Huang (Shanghai Jiao Tong University)"
      - author: "Fei Ye (Cooperative Medianet Innovation Center, Shang Hai Jiao Tong University)"
      - author: "Peisen Zhao (Cooperative Medianet Innovation Center, Shang hai Jiao Tong University)"
      - author: "Ya Zhang (Cooperative Medianet Innovation Center, Shang hai Jiao Tong University)"
      - author: "Yan-Feng Wang (Cooperative medianet innovation center of Shanghai Jiao Tong University)"
      - author: "Qi Tian (Huawei Cloud & AI)"
    all_authors: "Chaoqin Huang, Fei Ye, Peisen Zhao, Ya Zhang, Yan-Feng Wang and Qi Tian"
    code: ""
    keywords:
      - word: "anomaly detection"
      - word: "semi-supervised learning"
      - word: ""
    paper: "papers/0329.pdf"
    supp: "supp/0329_supp.zip"
    abstract: "This paper explores semi-supervised anomaly detection, a more practical setting for anomaly detection where a small additional set of labeled samples are provided. We propose a new KL-divergence based objective function and show that two factors: the mutual information between the data and latent representations, and the entropy of latent representations, constitute an integral objective function for anomaly detection. To resolve the contradiction in simultaneously optimizing the two factors, we propose a novel encoder-decoder-encoder structure, with the first encoder focusing on optimizing the mutual information and the second encoder focusing on optimizing the entropy. The two encoders are enforced to share similar encoding with a consistent constraint on their latent representations. Extensive experiments have revealed that the proposed method significantly outperforms several state-of-the-arts on multiple benchmark datasets, including medical diagnosis and several classic anomaly detection benchmarks."
  - id: 330
    order: 356
    poster_session: 4
    session_id: 11
    title: "Separating Content and Style for Unsupervised Image-to-Image Translation"
    authors:
      - author: "Yunfei Liu (Beihang University)"
      - author: "Haofei Wang (Peng Cheng Laboratory)"
      - author: "yang yue (Beihang University)"
      - author: "Feng Lu (Beihang University)"
    all_authors: "Yunfei Liu, Haofei Wang, Yang Yue and Feng Lu"
    code: "https://github.com/DreamtaleCore/SCS-UIT"
    keywords:
      - word: "Image-to-Image Translation"
      - word: "unsupervised learning"
      - word: "CNN Interpretation"
    paper: "papers/0330.zip"
    supp: ""
    abstract: "Unsupervised image-to-image translation aims to learn the mapping between two visual domains with unpaired samples. The existing works usually focus on disentangling the domain-invariant content code and domain-specific style code individually for multi-modal purposes. However, interpreting and manipulating the translated image has not been well explored. In this paper, we propose to separate the content code and style code simultaneously in a unified framework. Based on the correlation between the latent features and the high-level domain-invariant tasks, the proposed framework shows good properties like multi-modal translation, good interpretability, and ease of manipulation. The experimental results also demonstrate that the proposed approach outperforms the existing unsupervised image translation methods in terms of visual quality and diversity."
  - id: 332
    order: 248
    poster_session: 3
    session_id: 8
    title: "Towards Overcoming False Positives in Visual Relationship Detection"
    authors:
      - author: "Daisheng Jin (Beijing SenseTime Technology Development Limited)"
      - author: "Xiao Ma (National University of Singapore)"
      - author: "Chongzhi Zhang (BUAA)"
      - author: "Yizhuo Zhou (TIKTOK PTE. LTD.)"
      - author: "Jiashu Tao (National University of Singapore)"
      - author: "Mingyuan Zhang (Beijing SenseTime Technology Development Limited)"
      - author: "Zhoujun Li (Beihang University)"
    all_authors: "Daisheng Jin, Xiao Ma, Chongzhi Zhang, Yizhuo Zhou, Jiashu Tao, Mingyuan Zhang and Zhoujun Li"
    code: "https://github.com/kingjg/SABRA"
    keywords:
      - word: "visual relationship detection"
      - word: "balanced sampling"
    paper: "papers/0332.pdf"
    supp: "supp/0332_supp.zip"
    abstract: "In this paper, we investigate the cause of the high false positive rate in Visual Relationship Detection (VRD). We observe that during training, the relationship proposal distribution is highly imbalanced: most of the negative relationship proposals are easy to identify, e.g., the inaccurate object detection, which leads to the under-fitting of low-frequency difficult proposals. This paper presents Spatially-Aware Balanced negative pRoposal sAmpling (SABRA), a robust VRD framework as a proof of concept that alleviates the influence of false positives. To effectively optimize the model under imbalanced distribution,
SABRA adopts Balanced Negative Proposal Sampling (BNPS) strategy for mini-batch sampling. BNPS divides proposals into 5 well-defined sub-classes and generates a balanced training distribution. To further resolve the low-frequency challenging false positive proposals with high spatial ambiguity, we adopt a spatial learning module that implicitly imposes the object-centric spatial configuration with a spatial mask decoder, using the global spatial features extracted with Graph Neural Networks. SABRA is conceptually simple and outperforms SOTA methods by a large margin on two human-object interaction (HOI) datasets and one general VRD dataset."
  - id: 334
    order: 357
    poster_session: 4
    session_id: 11
    title: "Extended Differentiable Marching Cubes by Manifold-Preserving Shape Inflation"
    authors:
      - author: "Kiichi Itoh (The University of Tokyo)"
      - author: "Tatsuya Yatagawa (The University of Tokyo)"
      - author: "Yutaka Ohtake (The University of Tokyo)"
      - author: "Suzuki Hiromasa (The University of Tokyo)"
    all_authors: "Kiichi Itoh, Tatsuya Yatagawa, Yutaka Ohtake and Suzuki Hiromasa"
    code: ""
    keywords:
      - word: "surface reconstruction"
      - word: "marching cubes"
      - word: "normalizing flow"
      - word: "deep learning"
      - word: ""
    paper: "papers/0334.pdf"
    supp: "supp/0334_supp.zip"
    abstract: "This paper introduces an extended differentiable marching cubes (DMC) method for end-to-end learning of precise 3D surface geometries using a neural network. The original DMC method extracts the isosurface using a fixed-size voxel grid, similar to the traditional marching cubes. Therefore, the original method involves a trade-off between output resolution and memory consumption. In contrast, there remains room to increase the output resolution without increasing the number of voxels because an output surface often exists over a limited number of voxels. According to this observation, our method deforms an input point cloud to occupy the voxel grid as widely as possible, thereby refining small parts of the target shape. To obtain such deformation, we apply normalizing flow (NF), typically used to transform probability density functions. Its invertibility allows us to reproduce a mesh for the input point cloud by cancelling the deformation of the mesh obtained for the deformed point cloud using DMC. To obtain appropriate deformation, NF is conditioned by a global shape feature and is trained by several loss functions to inflate the input shape while preserving its manifold structure. We tested the proposed method with two shape datasets and showed that our extended DMC achieves higher performance than the original DMC, even used with a simple deformation."
  - id: 340
    order: 249
    poster_session: 3
    session_id: 8
    title: "Few-shot Action Recognition with Prototype-centered Attentive Learning"
    authors:
      - author: "Xiatian Zhu (Samsung)"
      - author: "Antoine S Toisoul (Samsung)"
      - author: "Juan-Manuel Perez-Rua (Facebook AI)"
      - author: "Li Zhang (Fudan University)"
      - author: "Brais Martinez (Samsung AI Center)"
      - author: "Tao Xiang (University of Surrey)"
    all_authors: "Xiatian Zhu, Antoine S Toisoul, Juan-Manuel Perez-Rua, Li Zhang, Brais Martinez and Tao Xiang"
    code: ""
    keywords:
      - word: "Few-shot learning"
      - word: "Video recognition"
      - word: "Action classification"
      - word: "Small training data"
      - word: "Model pre-training"
      - word: "Meta-learning"
      - word: "Transformer"
      - word: "Self-attention learning"
      - word: "Cross-attention learning"
      - word: "Prototype learning"
      - word: "Prototype-centered learning"
      - word: "Hybrid-attention learning"
    paper: "papers/0340.pdf"
    supp: "supp/0340_supp.pdf"
    abstract: "Few-shot action recognition aims to recognize action classes with few training samples. Most existing methods adopt a meta-learning approach with episodic training. In each episode, the few samples in a meta-training task are split into support and query sets. The former is used tobuild a classifier, which is then evaluated on the latter using a query-centered loss for model updating. There are however two major limitations: lack of data efficiency due to the query-centered only loss design and inability to deal with the support set outlying samples and inter-class distribution overlapping problems. In this paper, we overcome both limitations by proposing a new Prototype-centered Attentive Learning (PAL) model composed of two novel components. First, a prototype-centered contrastive learning loss is introduced to complement the conventional query-centered learning objective, in order to make full use of the limited training samples in each episode. Second, PAL further integrates a hybrid attentive learning mechanism that can minimize the negative impacts of outliers and promote class separation. Extensive experiments on four standard few-shot action benchmarks show that our method clearly outperforms previous state-of-the-art methods, with the improvement particularly significant (>10%) on the most challenging fine-grained action recognition benchmark."
  - id: 342
    order: 250
    poster_session: 3
    session_id: 8
    title: "A Comprehensive CT Dataset for Liver Computer Assisted Diagnosis"
    authors:
      - author: "Qingsen Yan (The University of Adelaide)"
      - author: "Bo Wang (Tsinghua University )"
      - author: "Dong Gong (The University of Adelaide)"
      - author: "Dingwen Zhang (NWPU)"
      - author: "yang yang (Northwestern Polytechnical University)"
      - author: "Zheng You (Tsinghua University)"
      - author: "Yanning  Zhang (Northwestern Polytechnical University)"
      - author: "Javen Qinfeng Shi (University of Adelaide)"
    all_authors: "Qingsen Yan, Bo Wang, Dong Gong, Dingwen Zhang, Yang Yang, Zheng You, Yanning  Zhang and Javen Qinfeng Shi"
    code: ""
    keywords:
      - word: "liver-related computer assisted"
      - word: "couinaud segmentation"
      - word: "vessel segmentation"
      - word: "dataset"
    paper: "papers/0342.pdf"
    supp: ""
    abstract: "Automated severity assessment of disease in computed tomography (CT) images plays an essential role for regional assessment of disease that is in great need of liver-related computer assisted diagnosis. An effective dataset is a cornerstone of the deep learning-based method. However, the popular liver-related datasets only focus on specific tasks and fail to guide advanced liver diagnosis tasks. To bridge the gap, we construct the first publicly comprehensive liver dataset, called ComLiver, which consists of multiple liver-related tasks and manually marks the elaborate labels for each task (500 cases). Note that the images in the ComLiver dataset are all thin thickness data that have significant clinical implications. In addition, liver and liver lesions segmentations have attracted substantial interest and achieved approving progress, other liver-related tasks of the liver are still under-studied due to various challenges (eg low contrast, vascular complexity, etc.), despite its significance in assisting preoperative planning. To better exploit the advanced tasks in liver therapy, we introduce vessel instance segmentation, couinaud segmentation tasks based on previous tasks. Finally, we perform a thorough evaluation of the state-of-the-art methods of each task on the proposed ComLiver dataset and obtain a number of interesting findings. Results show its challenging nature, unique attributes and present definite prospects for novel, adaptive, and generalized liver-related segmentation methods. We hope this dataset could advance research towards liver-related computer assisted diagnosis."
  - id: 345
    order: 141
    poster_session: 2
    session_id: 5
    title: "Searching for TrioNet: Combining Convolution with Local and Global Self-Attention"
    authors:
      - author: "Huaijin Pi (Zhejiang University)"
      - author: "Huiyu Wang (Johns Hopkins University)"
      - author: "Yingwei Li (Johns Hopkins University)"
      - author: "Zizhang Li (Zhejiang University)"
      - author: "Alan Yuille (Johns Hopkins University)"
    all_authors: "Huaijin Pi, Huiyu Wang, Yingwei Li, Zizhang Li and Alan Yuille"
    code: "https://github.com/phj128/TrioNet"
    keywords:
      - word: "Self-Attention"
      - word: "Neural Architecture Search"
    paper: "papers/0345.pdf"
    supp: "supp/0345_supp.zip"
    abstract: "Recently, self-attention operators have shown superior performance as a stand-alone building block for vision models. However, existing self-attention models are often hand-designed, modified from CNNs, and obtained by stacking one operator only. A wider range of architecture space which combines different self-attention operators and convolution is rarely explored. In this paper, we explore this novel architecture space with weight-sharing Neural Architecture Search (NAS) algorithms. The result architecture is named TrioNet for combining convolution, local self-attention, and global (axial) self-attention operators. In order to effectively search in this huge architecture space, we propose Hierarchical Sampling for better training of the supernet. In addition, we propose a novel weight-sharing strategy, Multi-head Sharing, specifically for multi-head self-attention operators. Our searched TrioNet that combines self-attention and convolution outperforms all stand-alone models with fewer FLOPs on ImageNet classification where self-attention performs better than convolution. Furthermore, on various small datasets, we observe inferior performance for self-attention models, but our TrioNet is still able to match the best operator, convolution in this case."
  - id: 348
    order: 251
    poster_session: 3
    session_id: 8
    title: "ExSinGAN: Learning an Explainable Generative Model from a Single Image"
    authors:
      - author: "Zicheng Zhang (University of Chinese  Academy of Science)"
      - author: "Congying Han (University of Chinese Academy of Sciences)"
      - author: "Tiande Guo (University of Chinese Academy of Sciences)"
    all_authors: "Zicheng Zhang, Congying Han and Tiande Guo"
    code: "https://github.com/102300440/ExSinGAN"
    keywords:
      - word: "single image generation"
      - word: "single image generative model"
      - word: "generative adversarial network"
      - word: "image synthesis"
      - word: ""
    paper: "papers/0348.pdf"
    supp: "supp/0348_supp.zip"
    abstract: "Generating images from a single sample has attracted extensive attention recently. In this paper, we formulate this problem as sampling from the conditional distribution of a single image, and propose a hierarchical framework that simplifies the learning of the intricate conditional distributions through the successive learning of the distributions about structure, semantics and texture, making the process of learning and generation comprehensible. We design ExSinGAN composed of three cascaded GANs for learning an explainable generative model from a given image, where the cascaded GANs model the distributions about structure, semantics and texture successively. ExSinGAN is learned not only from the internal patches of the given image as the previous works did, but also from the external prior obtained by the GAN inversion technique. Benefiting from the appropriate combination of internal and external information, ExSinGAN has a more powerful capability of generation and competitive generalization ability for the image manipulation tasks compared with prior works. "
  - id: 349
    order: 142
    poster_session: 2
    session_id: 5
    title: "Multi-Glimpse Network: A Robust and Efficient Classification Architecture based on Recurrent Downsampled Attention"
    authors:
      - author: "Sia Huat Tan (Tsinghua University)"
      - author: "Runpei Dong (Xi'an Jiaotong University)"
      - author: "Kaisheng Ma (Tsinghua University )"
    all_authors: "Sia Huat Tan, Runpei Dong and Kaisheng Ma"
    code: "https://github.com/siahuat0727/MGNet"
    keywords:
      - word: "image classification"
      - word: "robustness"
      - word: "computational efficiency"
      - word: "recurrent attention"
      - word: ""
    paper: "papers/0349.pdf"
    supp: "supp/0349_supp.zip"
    abstract: "Most feedforward convolutional neural networks spend roughly the same efforts for each pixel. Yet human visual recognition is an interaction between eye movements and spatial attention, which we will have several glimpses of an object in different regions. Inspired by this observation, we propose an end-to-end trainable Multi-Glimpse Network (MGNet) which aims to tackle the challenges of high computation and the lack of robustness based on recurrent downsampled attention mechanism. Specifically, MGNet sequentially selects task-relevant regions of an image to focus on and then adaptively combines all collected information for the final prediction. MGNet expresses strong resistance against adversarial attacks and common corruptions with less computation.  Also, MGNet is inherently more interpretable as it explicitly informs us where it focuses during each iteration. Our experiments on ImageNet100 demonstrate the potential of recurrent downsampled attention mechanisms to improve a single feedforward manner. For example, MGNet improves 4.76% accuracy on average in common corruptions with only 36.9% computational cost. Moreover, while the baseline incurs an accuracy drop to 7.6%, MGNet manages to maintain 44.2% accuracy in the same PGD attack strength with ResNet-50 backbone. Our code is available at https://github.com/siahuat0727/MGNet."
  - id: 352
    order: 358
    poster_session: 4
    session_id: 11
    title: "Robust Ellipsoid-specific Fitting via Expectation Maximization"
    authors:
      - author: "Mingyang Zhao (University of Chinese Academy and Sciences)"
      - author: "Xiaohong Jia (Chinese Academy of Sciences)"
      - author: "Lei Ma (Peking University)"
      - author: "Xinling Qiu (Chinese Academy of Sciences)"
      - author: "Xin Jiang (Beihang University)"
      - author: "Dong-Ming Yan (NLPR, Institute of Automation, Chinese Academy of Sciences)"
    all_authors: "Mingyang Zhao, Xiaohong Jia, Lei Ma, Xinling Qiu, Xin Jiang and Dong-Ming Yan"
    code: "https://zikai1.github.io/"
    keywords:
      - word: "ellipsoid fitting"
      - word: "geometric primitive fitting"
      - word: "quadrics"
      - word: "outliers"
      - word: "plane fitting"
      - word: "collision detection"
      - word: "magnetometer calibration"
      - word: "shape approximation"
      - word: ""
    paper: "papers/0352.pdf"
    supp: "supp/0352_supp.zip"
    abstract: "Ellipsoid fitting is of general interest in machine vision, such as object detection and shape approximation. Most existing approaches rely on the least-squares fitting of quadrics, minimizing the algebraic or geometric distances, with additional constraints to enforce the quadric as an ellipsoid. However, they are susceptible to outliers and  non-ellipsoid or biased results when the axis ratio exceeds certain thresholds. 
To address these problems, we propose a novel and robust method for ellipsoid fitting in a noisy, outlier-contaminated 3D environment. We explicitly model the ellipsoid by emph{kernel density estimation} (KDE)  of the input data. The ellipsoid fitting is cast as a emph{maximum likelihood estimation} (MLE) problem without extra constraints, where a weighting term is added to depress outliers, and then effectively solved via the emph{Expectation-Maximization (EM)} framework. Furthermore, we introduce the emph{vector $varepsilon$ technique} to accelerate the convergence of the original EM. The proposed method is compared with representative state-of-the-art approaches by extensive experiments, and results show that our method is ellipsoid-specific, parameter free, and more robust against noise, outliers, and the large axis ratio. Our implementation is available at url{https://zikai1.github.io/}."
  - id: 353
    order: 4
    oral_session: 1
    poster_session: 1
    session_id: 0
    title: "Domain Attention Consistency for Multi-Source Domain Adaptation"
    authors:
      - author: "Zhongying Deng (University of Surrey)"
      - author: "Kaiyang Zhou (Nanyang Technological University)"
      - author: "Yongxin Yang (University of Surrey)"
      - author: "Tao Xiang (University of Surrey)"
    all_authors: "Zhongying Deng, Kaiyang Zhou, Yongxin Yang and Tao Xiang"
    code: "https://github.com/Zhongying-Deng/DAC-Net"
    keywords:
      - word: "Transferable Attribute Learning"
      - word: "Domain Attention Consistency"
      - word: "Multi-Source Domain Adaptation"
      - word: ""
    paper: "papers/0353.pdf"
    supp: "supp/0353_supp.zip"
    abstract: "Most existing multi-source domain adaptation (MSDA) methods minimize the distance between multiple source-target domain pairs via feature distribution alignment, an approach borrowed from the single source setting. However, with diverse source domains, aligning pairwise feature distributions is challenging and could even be counter-productive for MSDA. In this paper, we introduce a novel approach: transferable attribute learning. The motivation is simple: although different domains can have drastically different visual appearances, they contain the same set of classes characterized by the same set of attributes; an MSDA model thus should focus on learning the most transferable attributes for the target domain. Adopting this approach, we propose a domain attention consistency network, dubbed DAC-Net. The key design is a feature channel attention module, which aims to identify transferable features (attributes). Importantly, the attention module is supervised by a consistency loss, which is imposed on the distributions of channel attention weights between source and target domains. Moreover, to facilitate discriminative feature learning on the target data, we combine pseudo-labeling with a feature compactness loss to minimize the distance between the target features and the classifier's weight vectors. Extensive experiments on three MSDA benchmarks show that our DAC-Net achieves new state of the art performance on all of them."
  - id: 357
    order: 252
    poster_session: 3
    session_id: 8
    title: "Few-Shot Temporal Action Localization with Query Adaptive Transformer"
    authors:
      - author: "Sauradip Nag (University of Surrey)"
      - author: "Xiatian Zhu (University of Surrey)"
      - author: "Tao Xiang (University of Surrey)"
    all_authors: "Sauradip Nag, Xiatian Zhu and Tao Xiang"
    code: "https://github.com/sauradip/fewshotQAT"
    keywords:
      - word: "temporal action localization"
      - word: "few shot learning"
      - word: "transformer"
      - word: "class imbalance"
      - word: "meta learning"
      - word: "action detection"
    paper: "papers/0357.pdf"
    supp: "supp/0357_supp.pdf"
    abstract: "Existing temporal action localization (TAL) works rely on  a  large  number  of  training  videos  with  exhaustive segment-level annotation, preventing them from scaling to new classes.   As a solution to this problem,  few-shot TAL(FS-TAL) aims to adapt a model to a new class represented by as few as a single video. Exiting FS-TAL methods assume trimmed training videos for new classes. However, this set-ting is not only unnatural – actions are typically captured in untrimmed videos, but also ignores background video segments containing vital contextual cues for foreground action segmentation. In this work, we first propose a new FS-TAL setting by proposing to use untrimmed training videos. Further,  a  novel  FS-TAL  model  is  proposed  which  maximizes the knowledge transfer from training classes whilst enabling the model to be dynamically adapted to both thenew class and each video of that class simultaneously. This is  achieved  by  introducing  a  query  adaptive  Transformer in the model. Extensive experiments on two action localization benchmarks demonstrate that our proposed method can outperform all the state-of-the-art alternatives significantly in both single-domain and cross-domain scenarios."
  - id: 362
    order: 143
    poster_session: 2
    session_id: 5
    title: "UDIS: Unsupervised Discovery of Bias in Deep Visual Recognition Models"
    authors:
      - author: "Arvindkumar Krishnakumar (Georgia Tech)"
      - author: "Viraj Prabhu (Georgia Tech)"
      - author: "Sruthi Sudhakar (Georgia Tech)"
      - author: "Judy Hoffman (Georgia Tech)"
    all_authors: "Arvindkumar Krishnakumar, Viraj Prabhu, Sruthi Sudhakar and Judy Hoffman"
    code: "https://github.com/akrishna77/bias-discovery"
    keywords:
      - word: "bias discovery"
      - word: "fairness"
      - word: "explainability"
      - word: "ethics"
      - word: "visual explanations"
      - word: "failure modes"
      - word: ""
    paper: "papers/0362.pdf"
    supp: "supp/0362_supp.pdf"
    abstract: "Deep learning models have been shown to learn spurious correlations from data that sometimes lead to systematic failures for certain subpopulations. Prior work has typically diagnosed this by crowdsourcing annotations for various protected attributes and measuring performance, which is both expensive to acquire and difficult to scale. In this work, we propose UDIS, an unsupervised algorithm for surfacing and analyzing such failure modes. UDIS identifies subpopulations via hierarchical clustering of dataset embeddings and surfaces systematic failure modes by visualizing low performing clusters along with their gradient-weighted class-activation maps. We show the effectiveness of UDIS in identifying failure modes in models trained for image classification on the CelebA and MSCOCO datasets."
  - id: 363
    order: 253
    poster_session: 3
    session_id: 8
    title: "FlowVOS: Weakly-Supervised Visual Warping for Detail-Preserving and Temporally Consistent Single-Shot Video Object Segmentation"
    authors:
      - author: "Julia Gong (Stanford University)"
      - author: "F. Christopher Holsinger (Stanford University)"
      - author: "Serena Yeung (Stanford University)"
    all_authors: "Julia Gong, F. Christopher Holsinger and Serena Yeung"
    code: "https://github.com/juliagong/flowvos"
    keywords:
      - word: "video object segmentation"
      - word: "single shot video object segmentation"
      - word: "segmentation"
      - word: "object tracking"
      - word: "optical flow"
      - word: "motion tracking"
      - word: "visual warping"
      - word: "weak supervision"
      - word: "video analysis"
      - word: "object segmentation"
    paper: "papers/0363.pdf"
    supp: "supp/0363_supp.zip"
    abstract: "We consider the task of semi-supervised video object segmentation (VOS). Our approach mitigates shortcomings in previous VOS work by addressing detail preservation and temporal consistency using visual warping. In contrast to prior work that uses full optical flow, we introduce a new foreground-targeted visual warping approach that learns flow fields from VOS data. We train a flow module to capture detailed motion between frames using two weakly-supervised losses. Our object-focused approach of warping previous foreground object masks to their positions in the target frame enables detailed mask refinement with fast runtimes without using extra flow supervision. It can also be integrated directly into state-of-the-art segmentation networks. On the DAVIS17 and YouTubeVOS benchmarks, we outperform state-of-the-art offline methods that do not use extra data, as well as many online methods that use extra data. Qualitatively, we also show our approach produces segmentations with high detail and temporal consistency."
  - id: 365
    order: 31
    poster_session: 1
    session_id: 2
    title: "MUSE: Feature Self-Distillation with Mutual Information and Self-Information"
    authors:
      - author: "Yu Gong (Microsoft)"
      - author: "Ye Yu (Microsoft)"
      - author: "Gaurav Mittal (Microsoft)"
      - author: "Greg Mori (Simon Fraser University / Borealis AI)"
      - author: "Mei Chen (Microsoft)"
    all_authors: "Yu Gong, Ye Yu, Gaurav Mittal, Greg Mori and Mei Chen"
    code: ""
    keywords:
      - word: "knowledge distillation"
      - word: "self-distillation"
      - word: "mutual information"
      - word: ""
    paper: "papers/0365.pdf"
    supp: "supp/0365_supp.zip"
    abstract: "We present a novel information-theoretic approach to introduce dependency among features of a deep convolutional neural network (CNN). The core idea of our proposed method, called MUSE, is to combine MUtual information and SElf-information to jointly improve the expressivity of all features extracted from different layers in a CNN. We present two variants of the realization of MUSE---Additive Information and Multiplicative Information. Importantly, we argue and empirically demonstrate that MUSE, compared to other feature discrepancy functions, is a more functional proxy to introduce dependency and effectively improve the expressivity of all features in the knowledge distillation framework. MUSE achieves superior performance over a variety of popular architectures and feature discrepancy functions for self-distillation and online distillation, and performs competitively with state-of-the-art methods for offline distillation. MUSE is also demonstrably versatile that enables it to be easily extended to CNN-based models on tasks other than image classification such as object detection."
  - id: 374
    order: 32
    poster_session: 1
    session_id: 2
    title: "Updated Paired Regions for Shadow Detection from Single Image"
    authors:
      - author: "Xiao Wang (Institute of Information Engineering, Chinese Academy of Sciences)"
      - author: "Siyuan Yao (Institute of Information Engineering,Chinese Academy of Sciences)"
      - author: "Pengwen  Dai (Institute of Information Engineering, Chinese Academy of Sciences)"
      - author: "Rui Wang (State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences"
      - author: "School of Cyber Security, University of Chinese Academy of Sciences)"
      - author: "Xiaochun Cao (Chinese Academy of Sciences)"
    all_authors: "Xiao Wang, Siyuan Yao, Pengwen  Dai, Rui Wang (State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, School of Cyber Security, University of Chinese Academy of Sciences) and Xiaochun Cao"
    code: ""
    keywords:
      - word: "Shadow detection"
      - word: "paired regions"
      - word: "penumbra region"
      - word: "physical confidence coefficients"
      - word: ""
    paper: "papers/0374.pdf"
    supp: "supp/0374_supp.zip"
    abstract: "In recent years, deep neural network based shadow detection approaches achieve high performance on benchmarks, but they require large amounts of labeled data for training to learn the statistical attributes of the shadowed and unshadowed regions. The physical relationship between non-grid regions is ignored due to the character of deep convolution network. In this paper, we seek to analyze the physical principle of the shadowed region and present a paired region based algorithm for shadow detection without extra model training. To be specific, we first segment the image via a region growing based approach to maintain the character of shadowed region. Then the penumbra is detected and used as an important cue. After that we adopt three physics based confidence coefficients when comparing the color of two regions. At last, we design a novel objective function to find the best paired strategy and detect the shadowed region. The proposed approach is tested on three public datasets. The comparative results show that the proposed non-learning approach performs favorably against the state-of-the-art approaches."
  - id: 378
    order: 144
    poster_session: 2
    session_id: 5
    title: "Image Composition Assessment with Saliency-augmented Multi-pattern Pooling"
    authors:
      - author: "Bo Zhang (Shanghai Jiao Tong University)"
      - author: "Li Niu (Shanghai Jiao Tong University)"
      - author: "Liqing Zhang (Shanghai Jiao Tong University)"
    all_authors: "Bo Zhang, Li Niu and Liqing Zhang"
    code: "https://github.com/bcmi/Image-Composition-Assessment-Dataset-CADB"
    keywords:
      - word: "photography composition"
      - word: "image composition assessment"
      - word: "composition pattern"
      - word: "multi-pattern pooling"
      - word: "composition attribute"
      - word: "image aesthetic assessment"
      - word: ""
    paper: "papers/0378.pdf"
    supp: "supp/0378_supp.zip"
    abstract: "Image composition assessment is crucial in aesthetic assessment, which aims to assess the overall composition quality of a given image. However, to the best of our knowledge, there is neither dataset nor method specifically proposed for this task. In this paper, we contribute the first composition assessment dataset CADB with composition scores for each image provided by multiple professional raters. Besides, we propose a composition assessment network SAMP-Net with a novel Saliency-Augmented Multi-pattern Pooling (SAMP) module, which analyses visual layout from the perspectives of multiple composition patterns. We also leverage composition-relevant attributes to further boost the performance, and extend Earth Mover's Distance (EMD) loss to weighted EMD loss to eliminate the content bias. The experimental results show that our SAMP-Net can perform more favorably than previous aesthetic assessment approaches."
  - id: 381
    order: 145
    poster_session: 2
    session_id: 5
    title: "On Adversarial Robustness of 3D Point Cloud Classification under Adaptive Attacks"
    authors:
      - author: "Jiachen Sun (University of Michigan)"
      - author: "Karl Koenig (University of Michigan)"
      - author: "yulong cao (University of Michigan, Ann Arbor	)"
      - author: "Qi Alfred Chen (UC Irvine)"
      - author: "Zhuoqing Morley Mao (University of Michigan)"
    all_authors: "Jiachen Sun, Karl Koenig, Yulong Cao, Qi Alfred Chen and Zhuoqing Morley Mao"
    code: ""
    keywords:
      - word: "point cloud classification"
      - word: "adversarial training"
      - word: "adaptive attack"
    paper: "papers/0381.pdf"
    supp: "supp/0381_supp.zip"
    abstract: "3D point clouds are playing pivotal roles in many safety-critical applications like autonomous driving, where adversarially robust 3D deep learning models are desired. In this study, we conduct the first security analysis of state-of-the-art (SOTA) defenses against 3D adversarial attacks and design adaptive evaluations on them. Our 100% adaptive attack success rates demonstrate that SOTA countermeasures are still fragile. We further present an in-depth study showing how adversarial training (AT) performs in point cloud classification and identify that the required symmetric function (pooling operation) is paramount to 3D models' robustness. Through systematic analysis, we unveil that the default-used fixed pooling (e.g., MAX pooling) generally weakens AT's effectiveness. Interestingly, we also discover that sorting-based parametric pooling significantly improves the models' robustness. Based on the above insights, we propose DeepSym, a deep symmetric pooling operation, to architecturally advance the robustness of PointNet to 47.0% under AT without sacrificing nominal accuracy, outperforming the original design and a strong baseline by +28.5% (~ 2.6x) and +6.5%, respectively. "
  - id: 382
    order: 33
    poster_session: 1
    session_id: 2
    title: "Semi-Online Knowledge Distillation"
    authors:
      - author: "Zhiqiang Liu (South China University of Technology)"
      - author: "Yanxia Liu (South China University of Technology)"
      - author: "Chengkai Huang (Harbin Institute of Technology)"
    all_authors: "Zhiqiang Liu, Yanxia Liu and Chengkai Huang"
    code: " https://github.com/swlzq/Semi-Online-KD"
    keywords:
      - word: "Knowledge Distillation"
      - word: "Model Compression"
    paper: "papers/0382.pdf"
    supp: ""
    abstract: "Knowledge distillation is an effective and stable method for model compression via knowledge transfer. Conventional knowledge distillation (KD) is to transfer knowledge from a large and well pre-trained teacher network to a small student network, which is a one-way process. Recently, deep mutual learning (DML) has been proposed to help student networks learn collaboratively and simultaneously. However, to the best of our knowledge, KD and DML have never been jointly explored in a unified framework to solve the knowledge distillation problem. In this paper, we investigate that the teacher model supports more trustworthy supervision signals in KD, while the student captures more similar behaviors from the teacher in DML. Based on these observations, we first propose to combine KD with DML in a unified framework. Furthermore, we propose a Semi-Online Knowledge Distillation (SOKD) method that effectively improves the performance of the student and the teacher. In this method, we introduce the peer-teaching training fashion in DML in order to alleviate the student's imitation difficulty, and we also leverage the supervision signals provided by the well-trained teacher in KD. Besides, we also show our framework can be easily extended to feature-based distillation methods for improved learning accuracy. Extensive experiments on CIFAR-100 and ImageNet datasets demonstrate the proposed method achieves state-of-the-art performance."
  - id: 386
    order: 254
    poster_session: 3
    session_id: 8
    title: "Hierarchical Interaction Network for Video Object Segmentation from Referring Expressions"
    authors:
      - author: "Zhao Yang (University of Oxford)"
      - author: "Yansong Tang (University of Oxford)"
      - author: "Luca Bertinetto (FiveAI Ltd.)"
      - author: "Hengshuang Zhao (University of Oxford)"
      - author: "Philip Torr (University of Oxford)"
    all_authors: "Zhao Yang, Yansong Tang, Luca Bertinetto, Hengshuang Zhao and Philip Torr"
    code: ""
    keywords:
      - word: "segmentation"
      - word: "video object segmentation"
      - word: "referring segmentation"
      - word: "referring video object segmentation"
      - word: "video object segmentation from referring expressions"
      - word: "referring image segmentation"
      - word: "referring image comprehension"
      - word: "optical flow"
      - word: "visual grounding"
    paper: "papers/0386.pdf"
    supp: "supp/0386_supp.zip"
    abstract: "In this paper, we investigate the problem of video object segmentation from referring expressions (VOSRE). Conventional methods typically perform multi-modal fusion based on linguistic features and the visual features extracted from the top layer of the visual encoder, which limits these models' ability to represent multi-modal inputs at different semantic and spatial granularity levels. To address this issue, we present an end-to-end hierarchical interaction network (HINet) for the VOSRE problem. Our model leverages the feature pyramid produced by the visual encoder to generate multiple levels of multi-modal features. This allows more flexible representation of various linguistic concepts (e.g., object attributes and categories) in different levels of the multi-modal features. Moreover, we further extract signals of moving objects from optical flow input, and utilize them as complementary cues for highlighting the referent and suppressing the background with a motion gating mechanism. In contrast to previous methods, this strategy allows our model to make online predictions without requiring the whole video as input. Despite its simplicity, our proposed HINet improves over the previous state of the art on the DAVIS-16, DAVIS-17, and J-HMDB datasets for the VOSRE task, demonstrating its effectiveness and generality."
  - id: 387
    order: 34
    poster_session: 1
    session_id: 2
    title: "SamplingAug: On the Importance of Patch Sampling Augmentation for Single Image Super-Resolution"
    authors:
      - author: "Shizun Wang (Beijing University of Posts and Telecommunications)"
      - author: "Ming Lu (Intel Labs China)"
      - author: "Kaixin Chen (Beijing University of Posts and Telecommunications)"
      - author: "Jiaming Liu (Beijing University of Posts and Telecommunications)"
      - author: "Xiaoqi Li (Columbia university in the city of New york)"
      - author: "Chuang Zhang (Beijing University of Posts and Telecommunications)"
      - author: "Ming Wu (Beijing University of Posts and Telecommunications)"
    all_authors: "Shizun Wang, Ming Lu, Kaixin Chen, Jiaming Liu, Xiaoqi Li, Chuang Zhang and Ming Wu"
    code: "https://github.com/littlepure2333/SamplingAug"
    keywords:
      - word: "Super-Resolution"
      - word: "Patch Sampling"
    paper: "papers/0387.pdf"
    supp: "supp/0387_supp.zip"
    abstract: "With the development of Deep Neural Networks (DNNs), plenty of methods based on DNNs have been proposed for Single Image Super-Resolution (SISR). However, existing methods mostly train the DNNs on uniformly sampled LR-HR patch pairs, which makes them fail to fully exploit informative patches within the image. In this paper, we present a simple yet effective data augmentation method. We first devise a heuristic metric to evaluate the informative importance of each patch pair. In order to reduce the computational cost for all patch pairs, we further propose to optimize the calculation of our metric by integral image, achieving about two orders of magnitude speedup. The training patch pairs are sampled according to their informative importance with our method. Extensive experiments show our sampling augmentation can consistently improve the convergence and boost the performance of various SISR architectures, including EDSR, RCAN, RDN, SRCNN and ESPCN across different scaling factors (x2, x3, x4). Code is available at https://github.com/littlepure2333/SamplingAug"
  - id: 391
    order: 359
    poster_session: 4
    session_id: 11
    title: "Multi-Granularity Hypergraphs and Adversarial Complementary Learning for Person Re-identification"
    authors:
      - author: "Yi Ma (University of Science and Technology of China)"
      - author: "Tian Bai (	University of Science and Technology of China)"
      - author: "Wenyu Zhang (University of Science and Technology of China)"
      - author: "Jian Hu (University of Science and Technology of China)"
    all_authors: "Yi Ma, Tian Bai, Wenyu Zhang and Jian Hu"
    code: ""
    keywords:
      - word: "Person Re-Identification"
      - word: "Hypergraphs Learning"
      - word: "Adversarial Complementary Learning"
    paper: "papers/0391.pdf"
    supp: "supp/0391_supp.zip"
    abstract: "Many existing person re-identification methods have the following two limitations: $emph{(i)}$ They are suffering from missing body parts and occlusion. $emph{(ii)}$ They fail to get diverse visual cues. To handle these problems, we propose a Multi-Granularity Hypergraphs and Adversarial Complementary Learning (MGHACL) method. Specifically, we first uniformly partition the input images into several stripes, which are used to obtain multi-granularity features later. Then we use the proposed MGHACL to learn the high-order  relations between these features, which makes the learned features robust, and the complementary information of these features, which contain different visual cues. Next, we integrate learned high-order spatial relations information and complementary information to improve the representation capability of each regional feature. Moreover, we use a supervision strategy to learn to extract more accurate global features. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods on three mainstream datasets, including Market-1501, DukeMTMC-ReID, and CUHK03-NP."
  - id: 398
    order: 35
    poster_session: 1
    session_id: 2
    title: "Deep Video Inpainting Detection"
    authors:
      - author: "Peng Zhou (University of Maryland)"
      - author: "Ning Yu (University of Maryland and Max Planck Institute for Informatics)"
      - author: "Zuxuan Wu (UMD)"
      - author: "Larry Davis (University of Maryland)"
      - author: "Abhinav Shrivastava (University of Maryland)"
      - author: "Ser-Nam Lim (Facebook AI)"
    all_authors: "Peng Zhou, Ning Yu, Zuxuan Wu, Larry Davis, Abhinav Shrivastava and Ser-Nam Lim"
    code: "https://github.com/pengzhou1108/VIDNet"
    keywords:
      - word: "Video Inpainting Detection"
      - word: "Manipulation Detection"
      - word: "DeepFake Detection"
    paper: "papers/0398.pdf"
    supp: "supp/0398_supp.zip"
    abstract: "Video inpainting has become an increasingly matured forensics technique and has caused visual misinformation on social media. Yet the countermeasures to detect inpainted regions in videos have received little attention, leaving such threats out of control. To pioneer a mitigation solution, we introduce VIDNet, the first study of learning-based video inpainting detection, which contains a two-stream encoder-decoder architecture with attention module. To reveal artifacts encoded in compression, VIDNet additionally takes in Error Level Analysis frames to augment RGB frames, producing multimodal features at different levels with an encoder. Exploring spatial and temporal relationships, these features are further decoded by a Convolutional LSTM to predict masks of inpainted regions. In addition, when detecting whether a pixel is inpainted or not, we present a quad-directional local attention module that borrows information from its surrounding pixels from four directions. Extensive experiments validate the significant advantages of VIDNet over alternative inpainting detection baselines, as well as its generalization on unseen videos. We have released our code in url{https://github.com/pengzhou1108/VIDNet}."
  - id: 399
    order: 146
    poster_session: 2
    session_id: 5
    title: "Back to the Future: Cycle Encoding Prediction for Self-supervised Video Representation Learning"
    authors:
      - author: "Xinyu Yang (University of Bristol)"
      - author: "Majid Mirmehdi (University of Bristol)"
      - author: "Tilo Burghardt (University of Bristol)"
    all_authors: "Xinyu Yang, Majid Mirmehdi and Tilo Burghardt"
    code: "https://github.com/youshyee/CEP"
    keywords:
      - word: "unsupervised learning"
      - word: "self-supervised learning"
      - word: "video self-supervised learning"
      - word: "contrastive learning"
      - word: "representation learning"
      - word: "cycle consistency"
      - word: "temporal prediction"
      - word: "action recognition"
      - word: ""
    paper: "papers/0399.pdf"
    supp: ""
    abstract: "In this paper, we show that learning video feature spaces in which temporal cycles are maximally predictable benefits action classification. In particular, we propose a novel learning approach, Cycle Encoding Prediction (CEP), that is able to effectively represent the high-level spatio-temporal structure of unlabelled video content. CEP builds a latent space wherein the concept of closed forward-backwards, as well as backwards-forward, temporal loops is approximately preserved. As a self-supervision signal, CEP leverages the bi-directional temporal coherence of entire video snippets and applies loss functions that encourage both temporal cycle closure and contrastive feature separation.  Architecturally, the underpinning network architecture utilises a single feature encoder for all input videos, adding two predictive modules that learn temporal forward and backward transitions. We apply our framework for pretext training of networks for action recognition tasks and report significantly improved results for the standard datasets UCF101 and HMDB51."
  - id: 404
    order: 147
    poster_session: 2
    session_id: 5
    title: "Unsupervised Domain Adaptation of Black-Box Source Models"
    authors:
      - author: "Haojian Zhang (South China University of Technology)"
      - author: "Yabin Zhang (Hong Kong Polytechnic University)"
      - author: "Kui Jia (South China University of Technology)"
      - author: "Lei Zhang (Hong Kong Polytechnic University,  Hong Kong, China)"
    all_authors: "Haojian Zhang, Yabin Zhang, Kui Jia and Lei Zhang"
    code: "https://github.com/zhjscut/IterLNL"
    keywords:
      - word: "domain adaptation"
      - word: "black box"
      - word: "unsupervised"
      - word: "noisy label"
      - word: "iterative"
    paper: "papers/0404.pdf"
    supp: "supp/0404_supp.zip"
    abstract: "Unsupervised domain adaptation (UDA) aims to learn models for a target domain of unlabeled data by transferring knowledge from a labeled source domain. In the traditional UDA setting, labeled source data are assumed to be available for adaptation. Due to increasing concerns for data privacy, source-free UDA is highly appreciated as a new UDA setting, where only a trained source model is assumed to be available, while labeled source data remain private. However, trained source models may also be unavailable in practice since source models may have commercial values and exposing source models brings risks to the source domain, e.g., problems of model misuse and white-box attacks. In this work, we study a subtly different setting, named Black-Box Unsupervised Domain Adaptation (B$^2$UDA), where only the application programming interface of source model is accessible to the target domain; in other words, the source model itself is kept as a black-box one. To tackle B$^2$UDA, we propose a simple yet effective method, termed Iterative Learning with Noisy Labels (IterLNL). With black-box models as tools of noisy labeling, IterLNL conducts noisy labeling and learning with noisy labels (LNL) iteratively. To adapt the LNL to B$^2$UDA, we estimate the noise rate from model predictions of unlabeled target data and propose category-wise sampling to tackle the unbalanced label noise among categories. Experiments on benchmark datasets show the efficacy of IterLNL. Given neither source data nor source models, IterLNL performs comparably with traditional UDA methods that make full use of labeled source data."
  - id: 407
    order: 36
    poster_session: 1
    session_id: 2
    title: "POEM: 1-bit Point-wise Operations based on Expectation-Maximization for Efficient Point Cloud Processing"
    authors:
      - author: "Sheng Xu (Beihang University)"
      - author: "Junhe Zhao (Beihang University)"
      - author: "Yanjing Li (Beihang University)"
      - author: "Baochang Zhang (Beihang University)"
      - author: "Guodong Guo (Baidu)"
    all_authors: "Sheng Xu, Junhe Zhao, Yanjing Li, Baochang Zhang and Guodong Guo"
    code: ""
    keywords:
      - word: "Point  Cloud"
      - word: "Network  Binarization"
      - word: "Network Compression"
    paper: "papers/0407.pdf"
    supp: "supp/0407_supp.zip"
    abstract: "In this paper, we present a new binarization approach of point-wise operations based on  Expectation-Maximization (POEM) for efficient deep learning on point clouds. In this work, we first implement a powerful baseline binarization method and discover that two main drawbacks cause the immense performance drop layer-wise Gaussian-distributed weights and  non-learnable scale factor. Based on the analysis, we formulate our POEM to solve problems in two steps. First, we propose a novel weight optimizer based on the Expectation-Maximization (EM) algorithm to constrain weights to formulate a robust bi-modal distribution. Moreover, a well-designed reconstruction loss is introduced to calculate learnable scale factors to enhance the representation capacity of 1-bit fully-connected (Bi-FC) layers. Extensive experiments demonstrate that our POEM surpasses existing binarization methods by significant margins."
  - id: 409
    order: 255
    poster_session: 3
    session_id: 8
    title: "Space-Time Memory Network for Sounding Object Localization in Videos"
    authors:
      - author: "Sizhe Li (University of Rochester)"
      - author: "Yapeng Tian (University of Rochester)"
      - author: "Chenliang Xu (University of Rochester)"
    all_authors: "Sizhe Li, Yapeng Tian and Chenliang Xu"
    code: "https://sites.google.com/view/bmvc2021stm"
    keywords:
      - word: "Sounding object Localization"
      - word: "Space-Time Memory Network"
      - word: "Audio-Visual"
    paper: "papers/0409.pdf"
    supp: "supp/0409_supp.zip"
    abstract: "Leveraging temporal synchronization and association within sight and sound is an essential step towards robust localization of sounding objects. To this end, we propose a space-time memory network for sounding object localization in videos. It can simultaneously learn spatio-temporal attention over both uni-modal and cross-modal representations from audio and visual modalities. We show and analyze both quantitatively and qualitatively the effectiveness of incorporating spatio-temporal learning in localizing audio-visual objects. We demonstrate that our approach generalizes over various complex audio-visual scenes and outperforms recent state-of-the-art methods. Code and data can be found at https://sites.google.com/view/bmvc2021stm."
  - id: 413
    order: 148
    poster_session: 2
    session_id: 5
    title: "Adaptive Tensor Networks Decomposition"
    authors:
      - author: "Chang Nie (Nanjing University of Science & Technology)"
      - author: "Huan Wang (Nanjing University of Science & Technology)"
      - author: "Le Tian (Nanjing University Of Science & Technology)"
    all_authors: "Chang Nie, Huan Wang and Le Tian"
    code: ""
    keywords:
      - word: "Tensor networks"
      - word: "Adaptive decomposition"
      - word: "Tensor completion"
      - word: "Neural network compression."
    paper: "papers/0413.pdf"
    supp: "supp/0413_supp.zip"
    abstract: "Tensor Decomposition (TD) is a powerful technique in solving high-dimensional optimization problems and has been widely used in machine learning and data science. Many TD models aim to establish a trade-off between computational complexity and representation ability. However, they have the problem of tensor rank selection and latent factor arrangement, and the neglected internal correlation between different modes. In this paper, we propose a data-adaptive TD model established on a generalized tensor rank and name it adaptive tensor network (ATN) decomposition, which constructs an optimal topological structure for TD according to the intrinsic properties of the data. We exploit the generalized tensor rank to measure the correlation between two modes of the data and establish a multi-linear connection between the corresponding latent factors with an adaptive rank. ATN possesses the merits of permutation invariance, strong robustness, and represents high-order data with fewer parameters. We verified ATN's effectiveness and superiority on three typical tasks: tensor completion, image denoising, and neural network compression. Experimental results on synthetic data and real datasets demonstrate that the overall performance of ATN surpasses the state-of-the-art TD method."
  - id: 414
    order: 149
    poster_session: 2
    session_id: 5
    title: "Grand Unified Domain Adaptation"
    authors:
      - author: "Ziyun Cai (Nanjing University of Posts and Telecommunications)"
      - author: "Tengfei Zhang (Nanjing University of Posts and Telecommunications)"
      - author: "Jing Xiao-Yuan (School of Computer, Wuhan University)"
      - author: "Ling Shao (Inception Institute of Artificial Intelligence)"
    all_authors: "Ziyun Cai, Tengfei Zhang, Jing Xiao-Yuan and Ling Shao"
    code: ""
    keywords:
      - word: "domain adaptation"
      - word: "imbalanced label sets"
      - word: "multi-source domain"
      - word: ""
    paper: "papers/0414.pdf"
    supp: ""
    abstract: "Existing domain adaptation (DA) methods try to handle various DA scenarios subject to imbalanced label sets or multiple source/target domains, e.g., Closed set, Open set, Multi-Source, Partial and Multi-Target DA. Though Universal Domain Adaptation (UniDA) and Versatile Domain Adaptation (VDA) have been proposed to address these scenarios simultaneously, the related proposed methods still suffer from two issues: i) UniDA and VDA can hardly cover all of existing DA scenarios, e.g., UniDA cannot handle Multi-Source and Multi-Target DA scenarios, and VDA does not include Open set DA. ii) The proposed UniDA and VDA methods mainly focus on the versatility, and they ignore the essential DA problem, i.e., domain mismatch. This paper introduces Grand Unified Domain Adaptation (GUDA) scenario, which needs no prior knowledge about the number of source/target domains or the overlap. GUDA can cover more existing DA scenarios. Towards tackling GUDA, we formulate a grand unified adaptation network called Graph Contrastive Adaptation Network (GCAN), which can handle above mentioned DA scenarios and further reduces the domain mismatch without any modification. GCAN includes a graph contrastive adaptation objective at the node level, and a transferability rule to gain the common category identification loss. The results illustrate that GCAN works stably on different GUDA settings and shows comparable performance against recent DA methods on five benchmarks."
  - id: 415
    order: 37
    poster_session: 1
    session_id: 2
    title: "One-Step Pixel-Level Perturbation-Based Saliency Detector"
    authors:
      - author: "Vinnam Kim (NCSOFT)"
      - author: "Hyunsouk Cho (NCSOFT)"
      - author: "Sehee Chung (NCSOFT)"
    all_authors: "Vinnam Kim, Hyunsouk Cho and Sehee Chung"
    code: "https://github.com/vinnamkim/oppsd"
    keywords:
      - word: "explainable ai"
      - word: "saliency map"
      - word: ""
    paper: "papers/0415.pdf"
    supp: "supp/0415_supp.zip"
    abstract: "To explain deep neural networks, many perturbation-based saliency methods are studied in the computer vision domain.
However, previous perturbation-based saliency methods require iterative optimization steps or multiple forward propagation steps.
In this paper, we propose a new perturbation-based saliency that requires only one backward propagation step by approximating the perturbation effect on the output in the local area.
We empirically demonstrate that our method shows fast computations and low memory requirements comparable to other most efficient baselines.
Furthermore, our method simultaneously considers all possible perturbing directions so as not to misestimate the perturbation effect.
Our ablation study shows that considering all possible perturbing directions is crucial to obtain a correct saliency map.
Lastly, our method exhibits competitive performance on the benchmarks in evaluating the pixel-level saliency map.
Code is available at https://github.com/vinnamkim/OPPSD."
  - id: 420
    order: 256
    poster_session: 3
    session_id: 8
    title: "View Birdification in the Crowd: Ground-Plane Localization from Perceived Movements"
    authors:
      - author: "Mai Nishimura (OMRON SINIC X)"
      - author: "Shohei Nobuhara (Kyoto University)"
      - author: "Ko Nishino (Kyoto University)"
    all_authors: "Mai Nishimura, Shohei Nobuhara and Ko Nishino"
    code: ""
    keywords:
      - word: "view birdification"
      - word: "crowd modeling"
      - word: "bird-eye-view"
    paper: "papers/0420.pdf"
    supp: "supp/0420_supp.zip"
    abstract: "We introduce view birdification, the problem of recovering ground-plane movements of people in a crowd from an ego-centric video captured from an observer also moving in the crowd. Recovered ground-plane movements would provide a sound basis for situational understanding and benefit downstream applications in computer vision and robotics. In this paper, we formulate view birdification as a geometric trajectory reconstruction problem and derive a cascaded optimization method from a Bayesian perspective. The method first estimates the observer's movement and then localizes surrounding pedestrians for each frame while taking into account the local interactions between them. We introduce three datasets by leveraging synthetic and real trajectories of people in crowds and evaluate the effectiveness of our method. The results demonstrate the accuracy of our method and set the ground for further studies of view birdification as an important but challenging visual understanding problem."
  - id: 421
    order: 150
    poster_session: 2
    session_id: 5
    title: "Human Attention in Fine-grained Classification"
    authors:
      - author: "Yao Rong (University of Tübingen)"
      - author: "Wenjia Xu (Institute of Electronics, Chinese Academy of Sciences)"
      - author: "Zeynep   Akata (University of Tübingen)"
      - author: "Enkelejda Kasneci (University of Tuebingen)"
    all_authors: "Yao Rong, Wenjia Xu, Zeynep   Akata and Enkelejda Kasneci"
    code: "https://github.com/yaorong0921/CUB-GHA"
    keywords:
      - word: "human attention"
      - word: "human gaze"
      - word: "fine-grained classification"
      - word: ""
    paper: "papers/0421.pdf"
    supp: "supp/0421_supp.zip"
    abstract: "The way humans attend to, process and classify a given image has the potential to vastly benefit the performance of deep learning models. Exploiting where humans are focusing can rectify models when they are deviating from essential features for correct decisions. To validate that human attention contains valuable information for decision-making processes such as fine-grained classification, we compare human attention and model explanations in discovering important features. Towards this goal, we collect human gaze data for the fine-grained classification dataset CUB and build a dataset named CUB-GHA (Gaze-based Human Attention). Furthermore, we propose the Gaze Augmentation Training (GAT) and Knowledge Fusion Network (KFN) to integrate human gaze knowledge into classification models. We implement our proposals in CUB-GHA and the recently released medical dataset CXR-Eye of chest X-ray images, which includes gaze data collected from a radiologist. Our result reveals that integrating human attention knowledge benefits classification effectively, e.g. improving the baseline by 4.38% on CXR. Hence, our work provides not only valuable insights into understanding human attention in fine-grained classification, but also contributes to future research in integrating human gaze with computer vision tasks. CUB-GHA and code are available at https://github.com/yaorong0921/CUB-GHA."
  - id: 423
    order: 360
    poster_session: 4
    session_id: 11
    title: "Intersection Prediction from Single 360° Image via Deep Detection of Possible Direction of Travel"
    authors:
      - author: "Naoki Sugimoto (University of Tokyo)"
      - author: "Satoshi Ikehata (National Institute of Informatics)"
      - author: "Kiyoharu Aizawa (The University of Tokyo)"
    all_authors: "Naoki Sugimoto, Satoshi Ikehata and Kiyoharu Aizawa"
    code: ""
    keywords:
      - word: "360° image"
      - word: "PDoT"
      - word: "iii360 dataset"
      - word: "intersection detection"
      - word: ""
    paper: "papers/0423.pdf"
    supp: "supp/0423_supp.zip"
    abstract: "Movie-Map, which is an interactive map with first-person view to engage the user in a simulated walking experience, is made up of short 360° video segments separated by traffic intersections which are seamlessly connected according to viewer's direction of travel. However, in a wide area of urban scale where many roads intersect, manual intersection segmentation requires significant human effort. Therefore, the automatic identification of intersections from 360° videos is important problem for Movie-Map to scale it up. In this paper, we propose a novel method that identifies the intersection from individual frames in 360° videos. Rather than formulating the intersection identification as the standard binary classification task taking a 360° image as input, we identify an intersection based on the number of the possible directions of travel (PDoT) in perspective images projected in eight directions from a single~omni image detected by the neural network for handling various types of intersections. We construct a large-scale 360° Image Intersection Identification (iii360) dataset for the training and evaluation where 360° videos are collected from various areas such as school campus, downtown, suburb, and china town and demonstrate that our PDoT-based method performs significantly better than a naive binary classification based algorithm. Source codes and a part of the dataset will be shared in the community when the paper is published."
  - id: 424
    order: 38
    poster_session: 1
    session_id: 2
    title: "Rethinking Clustering for Robustness"
    authors:
      - author: "Motasem Alfarra (KAUST)"
      - author: "Juan C Perez (Universidad de los Andes"
      - author: "King Abdullah University of Science and Technology)"
      - author: "Adel Bibi (University of Oxford)"
      - author: "Ali K Thabet (Facebook)"
      - author: "Pablo Arbelaez (Universidad de los Andes)"
      - author: "Bernard Ghanem (KAUST)"
    all_authors: "Motasem Alfarra, Juan C Perez (Universidad de los Andes, King Abdullah University of Science and Technology), Adel Bibi, Ali K Thabet, Pablo Arbelaez and Bernard Ghanem"
    code: "https://github.com/clustr-official-account/Rethinking-Clustering-for-Robustness"
    keywords:
      - word: "adversarial robustness"
      - word: "clustering"
      - word: "metric learning"
    paper: "papers/0424.pdf"
    supp: "supp/0424_supp.zip"
    abstract: "This paper studies how encouraging semantically-aligned features during deep neural network training can increase network robustness. Recent works observed that  Adversarial Training leads to robust models, whose learnt features appear to correlate with human perception. Inspired by this connection from robustness to semantics, we study the complementary connection: from semantics to robustness. To do so, we provide a robustness certificate for distance-based classification models (clustering-based classifiers). Moreover, we show that this certificate is tight, and we leverage it to propose emph{ClusTR} (Clustering Training for Robustness), a clustering-based and adversary-free training framework to learn robust models. Interestingly, textit{ClusTR} outperforms adversarially-trained networks by up to $4%$ under strong PGD attacks."
  - id: 425
    order: 257
    poster_session: 3
    session_id: 8
    title: "You Better Look Twice: a new perspective for designing accurate detectors with reduced computations"
    authors:
      - author: "Alexandra Dana (Samsung)"
      - author: "Maor Shutman (Samsung Israel R&D Center)"
      - author: "Yotam Perlitz (SAMSUNG)"
      - author: "Ran Vitek (Samsung Israel R&D Center)"
      - author: "Tomer Peleg (Samsung Israel R&D Center)"
      - author: "Roy J Jevnisek (Samsung)"
    all_authors: "Alexandra Dana, Maor Shutman, Yotam Perlitz, Ran Vitek, Tomer Peleg and Roy J Jevnisek"
    code: "https://github.com/AlexD123123/BLT-net"
    keywords:
      - word: "Pedestrian detection"
      - word: "Object detection"
      - word: "Two-stage object detection"
      - word: "Reduced computations"
      - word: "Pareto frontier"
      - word: ""
    paper: "papers/0425.pdf"
    supp: "supp/0425_supp.zip"
    abstract: "General object detectors use powerful backbones that uniformly extract features from images for enabling detection of a vast amount of object types. However, utilization of such backbones in object detection applications developed for specific object types can unnecessarily over-process background regions.
In addition, they are agnostic to object scales, thus redundantly process all image regions at the same resolution.
In this work we introduce BLT-net, a new low-computation two-stage object detection architecture designed to process images with a significant amount of background and objects of variate scales. BLT-net reduces computations by separating objects from background using a very lite first-stage. 
BLT-net then efficiently merges obtained proposals to further decrease processed image regions and then dynamically reduces their resolution to minimize computations. Resulting image proposals are then processed in the second-stage by a highly accurate model.
We demonstrate our architecture on the pedestrian detection problem, where objects are of different sizes, images are of high resolution and object detection is required to run in real-time. We show that our design reduces computations by a factor of x4-x7 on the Citypersons and Caltech datasets with respect to leading pedestrian detectors, on account of a small accuracy degradation. This method can be applied on other object detection applications to reduce computations."
  - id: 434
    order: 258
    poster_session: 3
    session_id: 8
    title: "LARNet: Latent Action Representation for Human Action Synthesis"
    authors:
      - author: "Naman Biyani (IIT Kanpur)"
      - author: "Aayush Jung Bahadur Rana (University of Central Florida)"
      - author: "Shruti Vyas (University of Central Florida)"
      - author: "Yogesh Rawat (University of Central Florida)"
    all_authors: "Naman Biyani, Aayush Jung Bahadur Rana, Shruti Vyas and Yogesh Rawat"
    code: "https://github.com/aayushjr/larnet"
    keywords:
      - word: "action synthesis"
      - word: "video synthesis"
      - word: "joint generative model"
      - word: "human action generation"
      - word: "end-to-end learning"
      - word: "conditional video generation"
    paper: "papers/0434.pdf"
    supp: "supp/0434_supp.zip"
    abstract: "We present LARNet, a novel end-to-end approach for generating human action videos. A joint generative modeling of appearance and dynamics to synthesize a video is very challenging and therefore recent works in video synthesis have proposed to decompose these two factors. However, these methods require a driving video to model the video dynamics. In this work, we propose a generative approach instead, which explicitly learns action dynamics in latent space avoiding the need of a driving video during inference. The generated action dynamics is integrated with the appearance using a recurrent hierarchical structure which induces motion at different scales to focus on both coarse as well as fine level action details. In addition, we propose a novel mix-adversarial loss function which aims at improving the temporal coherency of synthesized videos. We evaluate the proposed approach on four real-world human action datasets demonstrating the effectiveness of the proposed approach in generating human actions. The code and models will be made publicly available."
  - id: 440
    order: 151
    poster_session: 2
    session_id: 5
    title: "Tendentious Noise-rectifying Framework for Pathological HCC Grading"
    authors:
      - author: "XiaoTian Yu (Zhejiang University)"
      - author: "Zunlei Feng (Zhejiang University)"
      - author: "Yuexuan Wang (Zhejiang University/The University of Hong Kong)"
      - author: "Thomas Kwok To Li (The University of Hong Kong)"
      - author: "Xiuming Zhang (Zhejiang University)"
      - author: "Mingli Song (Zhejiang University)"
    all_authors: "XiaoTian Yu, Zunlei Feng, Yuexuan Wang, Thomas Kwok To Li, Xiuming Zhang and Mingli Song"
    code: ""
    keywords:
      - word: "noisy label"
      - word: "pathological image"
      - word: "HCC"
    paper: "papers/0440.pdf"
    supp: "supp/0440_supp.zip"
    abstract: "Hepatocellular carcinoma (HCC) is one of the most common cancers with high mortality. In clinic, pathology grade assessment is laborious work with high divergence and misdiagnosis rate. Many computer-assisted grading methods were proposed to increase the objectivity of diagnosis and reduce workload. With massive accurate annotation, deep learning based methods have achieved promising results in tumor grading. However, annotating pathology images is very time-consuming and hard to be precise. The inevitable noises caused by manual annotation have been ignored by existing tumor grading methods, which leads to serious performance degradation. In this paper, we propose a Tendentious Noise-rectifying Framework (TNF) for HCC grading on pathology images with noisy annotations. A fundamental way to reduce the negative impact of those noisy data is finding and rectifying those noises. So, we devise a noise-rectifying loss to rectify those noisy labels with high confidence. The rectifying tendency is dynamically adjusted by the feature polymer that contains structural information of a large local area of pathology image. We collect 415 hepatocellular biopsy slides and crop 20,000 patches as the dataset. Exhaustive experiments on this dataset demonstrate that, with the noise-rectifying loss, TNF achieves state-of-the-art performance and finds out the carcinoma cells in the healthy area, which has significant meaning for the diagnosis of HCC."
  - id: 443
    order: 7
    oral_session: 2
    poster_session: 1
    session_id: 1
    title: "Self-Supervised Point Cloud Completion via Inpainting"
    authors:
      - author: "Himangi Mittal (Carnegie Mellon University)"
      - author: "Brian Okorn (Carnegie Mellon University)"
      - author: "Arpit Jangid (Carnegie Mellon University	)"
      - author: "David Held (CMU)"
    all_authors: "Himangi Mittal, Brian Okorn, Arpit Jangid and David Held"
    code: ""
    keywords:
      - word: "Point Cloud Completion"
      - word: "LIDAR"
      - word: "Autonomous Driving"
      - word: "Point Clouds"
      - word: "Self Supervised Learning"
      - word: "Inpainting"
      - word: "KITTI"
      - word: "Shapenet"
    paper: "papers/0443.pdf"
    supp: "supp/0443_supp.zip"
    abstract: "When navigating in urban environments, many of the objects that need to be tracked and avoided are heavily occluded.  Planning and tracking using these partial scans can be challenging. The aim of this work is to learn to complete these partial point clouds, giving us a full understanding of the object's geometry using only partial observations. Previous methods achieve this with the help of complete, ground-truth annotations of the target objects, which are available only for simulated datasets. However, such ground truth is unavailable for real-world LiDAR data. In this work, we present a self-supervised point cloud completion algorithm, PointPnCNet, which is trained only on partial scans without assuming access to complete, ground-truth annotations. Our method achieves this via inpainting. We remove a portion of the input data and train the network to complete the missing region. As it is difficult to determine which regions were occluded in the initial cloud and which were synthetically removed, our network learns to complete the full cloud, including the missing regions in the initial partial cloud. We show that our method outperforms previous unsupervised and weakly-supervised methods on both the synthetic dataset, ShapeNet, and real-world LiDAR dataset, Semantic KITTI."
  - id: 444
    order: 152
    poster_session: 2
    session_id: 5
    title: "Rich Semantics Improve Few-Shot Learning"
    authors:
      - author: "Mohamed Afham Mohamed Aflal (University of Moratuwa)"
      - author: "Salman Khan (MBZUAI/ANU)"
      - author: "Muhammad Haris Khan (Muhammad Bin Zayed University of Artificial Intelligence)"
      - author: "Muzammal Naseer (Australian National University (ANU))"
      - author: "Fahad Shahbaz Khan (MBZUAI)"
    all_authors: "Mohamed Afham Mohamed Aflal, Salman Khan, Muhammad Haris Khan, Muzammal Naseer and Fahad Shahbaz Khan"
    code: "https://github.com/MohamedAfham/RS_FSL"
    keywords:
      - word: "few shot learning"
      - word: "multimodal learning"
      - word: "transformers in vision"
    paper: "papers/0444.pdf"
    supp: "supp/0444_supp.zip"
    abstract: "Human learning benefits from multi-modal inputs that often appear as rich semantics (e.g., description of an object's attributes while learning about it). This enables us to learn generalizable concepts from very limited visual examples. However, current few-shot learning (FSL) methods use numerical class labels to denote object classes which do not provide rich semantic meanings about the learned concepts. In this work, we show that by using  `class-level' language descriptions, that can be acquired with minimal annotation cost, we can improve the FSL performance. Given a support set and queries, our main idea is to create a bottleneck visual feature (hybrid prototype) which is then used to generate language descriptions of the classes as an auxiliary task during training. We develop a Transformer based forward and backward encoding mechanism to relate visual and semantic tokens that can encode intricate relationships between the two modalities. Forcing the prototypes to retain semantic information about class description acts as a regularizer on the visual features, improving their generalization to novel classes at inference. Furthermore, this strategy imposes a human prior on the learned representations, ensuring that the model is faithfully relating visual and semantic concepts, thereby improving model interpretability. Our experiments on four datasets and ablation studies show the benefit of effectively modeling rich semantics for FSL. Code is available at: https://github.com/MohamedAfham/RS_FSL."
  - id: 451
    order: 361
    poster_session: 4
    session_id: 11
    title: "Elsa: Energy-based Learning for Semi-supervised Anomaly Detection"
    authors:
      - author: "Sungwon Han (KAIST)"
      - author: "HyeonHo Song (KAIST)"
      - author: "Seung Eon Lee (KAIST)"
      - author: "Sungwon Park (KAIST)"
      - author: "Meeyoung Cha (KAIST & IBS)"
    all_authors: "Sungwon Han, HyeonHo Song, Seung Eon Lee, Sungwon Park and Meeyoung Cha"
    code: ""
    keywords:
      - word: "contrastive learning"
      - word: "energy-based learning"
      - word: "semi-supervised learning"
      - word: "anomaly detection"
      - word: ""
    paper: "papers/0451.pdf"
    supp: "supp/0451_supp.zip"
    abstract: "Contrastive learning has brought important advances in improving anomaly detection. Yet these techniques rely on clean training data, which cannot be guaranteed in real-world scenarios. This paper presents a theoretical interpretation of when and how contrastive learning alone fails to detect anomalies under data contamination. To address the shortcomings, we propose Elsa, a novel semi-supervised anomaly detection approach, that unifies the concept of energy-based models with unsupervised contrastive learning. Elsa instills robustness against various practical scenarios by a carefully designed fine-tuning step that uses the energy function to divide the normal data into prototype classes or subclasses that reflect heterogeneity of the data distribution. By using a small set of anomaly labels, Elsa improves anomaly detection performance in both clean and contaminated data scenarios by 0.9 and 6.6 AUROC, respectively."
  - id: 455
    order: 153
    poster_session: 2
    session_id: 5
    title: "Self-Supervised Learning in Multi-Task Graphs through Iterative Consensus Shift"
    authors:
      - author: "Emanuela Haller (Bitdefender and Politehnica University of Bucharest)"
      - author: "Elena Burceanu (Bitdefender, University of Bucharest)"
      - author: "Marius Leordeanu (University Politehnica of Bucharest)"
    all_authors: "Emanuela Haller, Elena Burceanu and Marius Leordeanu"
    code: "https://github.com/bit-ml/cshift"
    keywords:
      - word: "multi-task graph"
      - word: "self-supervised"
      - word: "consensus"
      - word: "multi-task agreement"
      - word: "selection ensemble"
      - word: "domain adaptation"
      - word: "domain generalization"
      - word: "distribution shift"
      - word: "experts"
    paper: "papers/0455.pdf"
    supp: "supp/0455_supp.zip"
    abstract: "The human ability to synchronize the feedback from all their senses inspired recent works in multi-task and multi-modal learning. While these works rely on expensive supervision, our multi-task graph requires only pseudo-labels from expert models. Every graph node represents a task, and each edge learns between tasks transformations. Once initialized, the graph learns self-supervised, based on a novel consensus shift algorithm that intelligently exploits the agreement between graph pathways to generate new pseudo-labels for the next learning cycle. 
We demonstrate significant improvement from one unsupervised learning iteration to the next, outperforming related recent methods in extensive multi-task learning experiments on two challenging datasets. Our code is available at https://github.com/bit-ml/cshift."
  - id: 466
    order: 39
    poster_session: 1
    session_id: 2
    title: "Inter-intra Variant Dual Representations for Self-supervised Video Recognition"
    authors:
      - author: "Lin  ZHANG (Carnegie Mellon University)"
      - author: "Qi She (Bytedance AI Lab)"
      - author: "Zhengyang Shen (Peking University)"
      - author: "Changhu Wang (ByteDance.Inc)"
    all_authors: "Lin  ZHANG, Qi She, Zhengyang Shen and Changhu Wang"
    code: "https://github.com/lzhangbj/DualVar"
    keywords:
      - word: "video action recognition"
      - word: "self-supervised learning"
      - word: "contrastive learning"
      - word: "representation learning"
    paper: "papers/0466.pdf"
    supp: "supp/0466_supp.zip"
    abstract: " Contrastive learning applied to self-supervised representation learning has seen a resurgence in deep models. In this paper, we find that existing contrastive learning based solutions for self-supervised video recognition focus on inter-variance encoding but ignore the intra-variance existing in clips within the same video. We thus propose to learn dual representations for each clip which (romannumeral 1) encode intra-variance through a shuffle-rank pretext task; (romannumeral 2) encode inter-variance through a temporal coherent contrastive loss. Experiment results show that our method plays an essential role in balancing inter and intra variances and brings consistent performance gains on multiple backbones and contrastive learning frameworks. Integrated with SimCLR and pretrained on Kinetics-400, our method achieves $textbf{82.0%}$ and $textbf{51.2%}$ downstream classification accuracy on UCF101 and HMDB51 test sets respectively and $textbf{46.1%}$ video retrieval accuracy on UCF101, outperforming both pretext-task based and contrastive learning based counterparts. Our code is available at href{https://github.com/lzhangbj/DualVar}{https://github.com/lzhangbj/DualVar}"
  - id: 467
    order: 362
    poster_session: 4
    session_id: 11
    title: "Inside Out Visual Place Recognition"
    authors:
      - author: "Sarah Ibrahimi (University of Amsterdam)"
      - author: "Nanne van Noord (University of Amsterdam)"
      - author: "Tim Alpherts (University of Amsterdam)"
      - author: "Marcel  Worring (University of Amsterdam)"
    all_authors: "Sarah Ibrahimi, Nanne van Noord, Tim Alpherts and Marcel  Worring"
    code: "https://github.com/saibr/IOVPR"
    keywords:
      - word: "image retrieval"
      - word: "visual place recognition"
      - word: "localization"
    paper: "papers/0467.pdf"
    supp: "supp/0467_supp.zip"
    abstract: "Visual Place Recognition (VPR) is generally concerned with localizing outdoor images. However, localizing indoor scenes that contain part of an outdoor scene can be of large value for a wide range of applications. In this paper, we introduce Inside Out Visual Place Recognition (IOVPR), a task aiming to localize images based on outdoor scenes visible through windows. For this task we present the new large-scale dataset Amsterdam-XXXL, with images taken in Amsterdam, that consists of 6.4 million panoramic street-view images and 1000 user-generated indoor queries. Additionally, we introduce a new training protocol Inside Out Data Augmentation to adapt Visual Place Recognition methods for localizing indoor images, demonstrating the potential of Inside Out Visual Place Recognition. We empirically show the benefits of our proposed data augmentation scheme on a smaller scale, whilst demonstrating the difficulty of this large-scale dataset for existing methods. With this new task we aim to encourage development of methods for IOVPR. The dataset and code are available for research purposes at https://github.com/saibr/IOVPR. "
  - id: 468
    order: 259
    poster_session: 3
    session_id: 8
    title: "Point3D: tracking actions as moving points with 3D CNNs"
    authors:
      - author: "Shentong Mo (Carnegie Mellon University)"
      - author: "Jingfei Xia (Carnegie Mellon University	)"
      - author: "Xiaoqing Tan (University of Pittsburgh)"
      - author: "Bhiksha Raj (Carnegie Mellon University)"
    all_authors: "Shentong Mo, Jingfei Xia, Xiaoqing Tan and Bhiksha Raj"
    code: ""
    keywords:
      - word: "Spatio-temporal action detection"
    paper: "papers/0468.pdf"
    supp: "supp/0468_supp.zip"
    abstract: "Spatio-temporal action recognition has been a challenging task that involves detecting where and when actions occur. Current state-of-the-art action detectors are mostly anchor-based, requiring sensitive anchor designs and huge computations due to calculating large numbers of anchor boxes. Motivated by nascent anchor-free approaches, we propose Point3D, a flexible and computationally efficient network with high precision for spatio-temporal action recognition. Our Point3D consists of a Point Head for action localization and a3D Head for action classification. Firstly, Point Head is used to track center points and knot key points of humans to localize the bounding box of an action. These location features are then piped into a time-wise attention to learn long-range dependencies across frames. The 3D Head is later deployed for the final action classification. Our Point3D achieves state-of-the-art performance on the JHMDB, UCF101-24, and AVA benchmarks in terms of frame-mAP and video-mAP. Comprehensive ablation studies also demonstrate the effectiveness of each module proposed in our Point3D."
  - id: 471
    order: 363
    poster_session: 4
    session_id: 11
    title: "GaitMask: Mask-based Model for Gait Recognition"
    authors:
      - author: "Beibei Lin (Beijing Jiaotong University)"
      - author: "Yu Liu (Beijing  Jiaotong  University)"
      - author: "Shunli Zhang (Beijing Jiaotong University)"
    all_authors: "Beibei Lin, Yu Liu and Shunli Zhang"
    code: ""
    keywords:
      - word: "Gait Recognition"
      - word: "Global Feature Representation"
      - word: "Local Feature Representation"
      - word: ""
    paper: "papers/0471.pdf"
    supp: ""
    abstract: "Gait recognition is an important biometric technology that identifies a person by using walking posture. Recently, most gait recognition methods either take the human gait as a whole to generate Global Feature Representations (GFR) or equivalently divide the human gait into multiple local regions to establish Local Feature Representations (LFR). However, we observe that LFR or GFR does not adequately represent the human gait because that LFR only focuses on the detailed information of each local region and GFR pays more attention to the global context information. On the other hand, the partition manner of the local regions is fixed, which only focuses on the local information of several specific regions. Motivated by this observation, we propose a novel mask-based network, named GaitMask, for gait recognition. GaitMask is built based on the Mask-based Local Augmentation (MLA), which is used to learn more comprehensive feature representations. MLA is a dual-branch structure consisting of a GFR extraction as the trunk and a mask-based LFR extraction as the branch. Specifically, the mask-based LFR extraction consists of a pair of complementary masks, where one mask randomly drops a region of the input feature maps and the other one only preserves this region. The complementary mask can be used to generate more comprehensive LFR and enhances the robustness of feature representations of the trunk. Experiments on two popular datasets demonstrate that our method achieves state-of-the-art results. Specifically, the proposed method significantly increases the performance in complex environments."
  - id: 473
    order: 114
    oral_session: 4
    poster_session: 2
    session_id: 4
    title: "Lane Line Detection based on Parallel Spatial Separation Convolution"
    authors:
      - author: "xile shen (Tsinghua University)"
      - author: "Zongqing Lu (Tsinghua University international Graduate School at Shenzhen)"
      - author: "youcheng zhang (Tsinghua University)"
      - author: "Jing-Hao Xue (University College London)"
    all_authors: "Xile Shen, Zongqing Lu, Youcheng Zhang and Jing-Hao Xue"
    code: ""
    keywords:
      - word: "Lane line detection"
      - word: "Decomposed convolution"
      - word: ""
    paper: "papers/0473.pdf"
    supp: ""
    abstract: "One of the fundamental tasks in autonomous driving is lane line detection. We aim to improve detection accuracy in complex scenarios and strike a better balance between performance and complexity of lane line detection networks. To this end, we first propose Parallel Spatial Separation Convolution (PSS-conv), a new convolution operation built on a new parallel spatial convolution decomposition and a channel-weighted feature merging strategy, to aggregate the features obtained from decomposed convolution. Then, we propose Parallel Spatial Separation Convolution with Message-Passing (PSSconv-MP), in which a new message passing module is added before feature merging to enable slice-by-slice information propagation. Based on the PSS-conv, PSS-conv-MP and residual connection, we construct a new lane line detection network called Parallel Spatial Separation Network (PSSNet), which can handle challenging scenes like curve and obscured lane lines. Extensive experiments show that PSSNet can achieve a superior performance on the challenging lane line detection benchmark CULane."
  - id: 476
    order: 364
    poster_session: 4
    session_id: 11
    title: "PRN: Psychology-Inspired Relation Network for Detecting Social Interaction Groups from Single Images"
    authors:
      - author: "Jiaqi Yu (Shanghai Jiao Tong University)"
      - author: "Jinhai Yang (Shanghai Jiao Tong University)"
      - author: "Hua Yang (Shanghai Jiao Tong University)"
      - author: "Guangtao Zhai (Shanghai Jiao Tong University)"
    all_authors: "Jiaqi Yu, Jinhai Yang, Hua Yang and Guangtao Zhai"
    code: ""
    keywords:
      - word: "Social Interaction"
      - word: "Relation Discovery"
      - word: "Crowd Analysis"
    paper: "papers/0476.pdf"
    supp: "supp/0476_supp.zip"
    abstract: "Detecting interaction groups is an essential task for understanding human behaviours and social activities.  However, it is still challenging to identify social interactions and the resulting crowd groups using purely visual cues, especially from single images. Prior works either require additional statistics, such as interpersonal angles and kinaesthetic information, or simply deduce the group memberships with the similarity of individual actions. In this paper, we present the Psychology-inspired Relation Network (PRN) to comprehensively understand the static social scenes and effectively model the interaction relations between individuals. More concretely, stimulated by recent advances in social psychology, we first predict the keypoint heatmap from an image with the human regions of interest as the visual representations of the key factors determining interaction groups: distance, orientation and postural openness. We then incorporate the personal and mutual influences together to compute the interaction strength matrix via self-attention, and finally utilise a perception to convert this matrix into dyadic interaction probability.  Moreover, we devise two loss functions, the dyad loss to optimise the dyadic interaction probability and the group loss to enhance the distinguishability among different social groups. To evaluate the performance of PRN, we introduce a novel dataset containing various scenes with different crowd densities, by merging representative databases and relabeling the group labels.  Our method achieves outstanding results on the proposed dataset."
  - id: 478
    order: 154
    poster_session: 2
    session_id: 5
    title: "Improving Text-to-Image Synthesis Using Contrastive Learning"
    authors:
      - author: "Hui Ye (Georgia State University)"
      - author: "Xiulong Yang (Georgia State University)"
      - author: "Martin   Takac (Mohamed bin Zayed University of Artificial Intelligence)"
      - author: "Rajshekhar Sunderraman (Georgia State University)"
      - author: "Shihao Ji (Georgia State University)"
    all_authors: "Hui Ye, Xiulong Yang, Martin   Takac, Rajshekhar Sunderraman and Shihao Ji"
    code: "https://github.com/huiyegit/T2I_CL"
    keywords:
      - word: "Text-to-Image Synthesis"
      - word: "Image Generation"
      - word: "Contrastive Learning"
      - word: "Image-text Matching"
      - word: "Siamese Structure"
      - word: "Cross-domain Representation Learning"
      - word: "Generative Adversarial Networks"
      - word: "Conditional Generative Adversarial Networks"
    paper: "papers/0478.pdf"
    supp: "supp/0478_supp.zip"
    abstract: "The goal of text-to-image synthesis is to generate a visually realistic image that matches a given text description. In practice, the captions annotated by humans for the same image have large variance in terms of contents and the choice
of words. The linguistic discrepancy between the captions of the identical image leads to the synthetic images deviating from the ground truth. To address this issue, we propose a contrastive learning approach to improve the quality
and enhance the semantic consistency of synthetic images. In the pretraining stage, we utilize the contrastive learning approach to learn the consistent textual representations for the captions corresponding to the same image. Furthermore, in the following stage of GAN training, we employ the contrastive learning method to enhance the consistency between the generated images from the captions related to the same image. We evaluate our approach over two popular
text-to-image synthesis models, AttnGAN and DM-GAN, on datasets CUB and COCO, respectively. Experimental results have shown that our approach can effectively improve the quality of synthetic images in terms of three metrics:
IS, FID and R-precision. Especially, on the challenging COCO dataset, our approach boosts the FID significantly by 29.60% over AttnGAn and by 21.96% over DM-GAN."
  - id: 483
    order: 260
    poster_session: 3
    session_id: 8
    title: "TEAM-Net: Multi-modal Learning for Video Action Recognition with Partial Decoding"
    authors:
      - author: "Zhengwei Wang (Trinity Collge Dublin)"
      - author: "Qi She (Bytedance AI Lab)"
      - author: "Aljosa Smolic (Trinity College Dublin, Ireland)"
    all_authors: "Zhengwei Wang, Qi She and Aljosa Smolic"
    code: "https://github.com/villawang/TEAM-Net"
    keywords:
      - word: "video action recognition"
      - word: "partially decoded video"
      - word: "multi-modal fusion"
    paper: "papers/0483.pdf"
    supp: "supp/0483_supp.zip"
    abstract: "Most of existing video action recognition models ingest raw RGB frames. However, the raw video stream requires enormous storage and contains significant temporal redundancy. Video compression (e.g., H.264, MPEG-4) reduces the superfluous information by representing the raw video stream using the concept of Group of Pictures (GOP). Each GOP is composed of the first I-frame (aka RGB image) followed by a number of P-frames, represented by motion vectors and residuals, which can be regarded and used as pre-extracted features. In this work, we 1) introduce sampling the input for the network from partially decoded videos based on GOP-level, and 2) propose a plug-and-play mulTi-modal lEArning Module (TEAM) for training the network using information from I-frames and P-frames in an end-to-end manner. We demonstrate the superior performance of TEAM-Net compared to the baseline using RGB only. TEAM-Net also achieves the state-of-the-art performance in the area of video action recognition with partial decoding."
  - id: 484
    order: 155
    poster_session: 2
    session_id: 5
    title: "Prototype-based Incremental Few-Shot Segmentation"
    authors:
      - author: "Fabio Cermelli (Politecnico di Torino)"
      - author: "Massimiliano Mancini (University of Tübingen)"
      - author: "Yongqin Xian (ETH Zurich)"
      - author: "Zeynep   Akata (University of Tübingen)"
      - author: "Barbara Caputo (Politecnico di Torino)"
    all_authors: "Fabio Cermelli, Massimiliano Mancini, Yongqin Xian, Zeynep   Akata and Barbara Caputo"
    code: "https://github.com/fcdl94/FSS"
    keywords:
      - word: "segmentation"
      - word: "incremental learning"
      - word: "continual learning"
      - word: "few shot learning"
      - word: "any shot learning"
      - word: "prototype"
      - word: "knowledge distillation"
      - word: ""
    paper: "papers/0484.pdf"
    supp: "supp/0484_supp.zip"
    abstract: "Semantic segmentation models have two fundamental weaknesses:  i) they require large training sets with costly pixel-level annotations, and ii) they have a static output space, constrained to the classes of the training set.  Toward addressing both problems, we introduce a new task, Incremental Few-Shot Segmentation (iFSS). The goal of iFSS is to extend a pretrained segmentation model with new classes from few annotated images and without access to old training data. To overcome the limitations of existing models iniFSS, we propose Prototype-based Incremental Few-Shot Segmentation (PIFS) that couples prototype learning and knowledge distillation. PIFS exploits prototypes to initialize the classifiers of new classes, fine-tuning the network to refine its features representation.  We design a prototype-based distillation loss on the scores of both old and new class prototypes to avoid overfitting and forgetting, and batch-renormalization to cope with non-i.i.d.few-shot data.  We create an extensive benchmark for iFSS showing that PIFS outperforms several few-shot and incremental learning methods in all scenarios"
  - id: 487
    order: 225
    oral_session: 6
    poster_session: 3
    session_id: 7
    title: "Everybody Is Unique: Towards Unbiased Human Mesh Recovery"
    authors:
      - author: "Ren Li (EPFL)"
      - author: "Srikrishna  Karanam (United Imaging Intelligence)"
      - author: "Meng Zheng (United Imaging Intelligence)"
      - author: "Terrence Chen (United Imaging Intelligence)"
      - author: "Ziyan Wu (United Imaging Intelligence)"
    all_authors: "Ren Li, Srikrishna  Karanam, Meng Zheng, Terrence Chen and Ziyan Wu"
    code: ""
    keywords:
      - word: "human mesh recovery"
      - word: "pose and shape"
      - word: "smpl"
      - word: "optimization"
      - word: ""
    paper: "papers/0487.pdf"
    supp: "supp/0487_supp.zip"
    abstract: "We consider the problem of obese human mesh recovery, i.e., fitting a parametric human mesh to images of obese people. Despite obese person mesh fitting being an important problem with numerous applications (e.g., healthcare), much recent progress in mesh recovery has been restricted to images of non-obese people. In this work, we identify this crucial gap in the current literature by presenting and discussing limitations of existing algorithms. Next, we present a simple baseline to address this problem that is scalable and can be easily used in conjunction with existing algorithms to improve their performance. Finally, we present a generalized human mesh optimization algorithm that substantially improves the performance of existing methods on both obese person images as well as community-standard benchmark datasets. A key innovation of this technique is that it does not rely on supervision from expensive-to-create mesh parameters. Instead, starting from widely and cheaply available 2D annotations, our method automatically generates mesh parameters that can in turn be used to re-train and fine-tune any existing mesh estimation algorithm. This way, we show our method acts as a drop-in to improve the performance of a wide variety of contemporary mesh estimation methods. We conduct extensive experiments on multiple datasets comprising both standard and obese person images and demonstrate the efficacy of our proposed techniques.
"
  - id: 491
    order: 40
    poster_session: 1
    session_id: 2
    title: "ContourletNet: A Generalized Rain Removal Architecture Using Multi-Direction Representation and Hierarchical Decomposition"
    authors:
      - author: "Wei-Ting Chen (National Taiwan University)"
      - author: "Cheng-Che Tsai (National Taiwan University)"
      - author: "Hao-Yu Fang (National Taiwan University)"
      - author: "I-HSIANG CHEN (National Taiwan University)"
      - author: "Jian-Jiun Ding (National Taiwan University)"
      - author: "Sy-Yen Kuo (National Taiwan University)"
    all_authors: "Wei-Ting Chen, Cheng-Che Tsai, Hao-Yu Fang, I-HSIANG CHEN, Jian-Jiun Ding and Sy-Yen Kuo"
    code: "https://github.com/cctakaet/ContourletNet-BMVC2021"
    keywords:
      - word: "deraining"
      - word: "rain removal"
      - word: "single image"
      - word: "low-level vision"
      - word: "dehazing"
      - word: "haze removal"
      - word: "image processing"
      - word: "contourlet transform"
      - word: "deep learning"
      - word: "convolutional neural network"
    paper: "papers/0491.pdf"
    supp: "supp/0491_supp.zip"
    abstract: "Images acquired from rainy scenes usually suffer from bad visibility which may damage the performance of the computer vision applications. The rainy scenarios can be categorized into two classes: moderate rain and heavy rain scenes. Moderate rain scene mainly consists of rain streaks while heavy rain scene contains both rain streaks and the veiling effect (similar to haze). Although existing methods have achieved excellent performance on these two cases individually, it still lacks a general architecture to address both heavy rain and moderate rain scenarios effectively. In this paper, we construct a hierarchical multi-direction representation network by using the contourlet transform (CT) to address both moderate rain and heavy rain scenarios. The CT divides the image into the multi-direction subbands (MS) and the semantic subband (SS). First, the rain streak information is retrieved to the MS based on the multi-orientation property of the CT. Second, a hierarchical architecture is proposed to reconstruct the background information including damaged semantic information and the veiling effect in the SS. Last, the multi-level subband discriminator with the feedback error map is proposed. By this module, all subbands can be well optimized to improve the contextual quality. Experiment shows that the proposed network achieves superior performance compared to state-of-the-art methods under both heavy rain and moderate rain scenarios. To the best of our knowledge, this is the first architecture that can address heavy rain and moderate rain effectively."
  - id: 492
    order: 156
    poster_session: 2
    session_id: 5
    title: "Generative Dynamic Patch Attack"
    authors:
      - author: "Xiang Li (Georgia State University)"
      - author: "Shihao Ji (Georgia State University)"
    all_authors: "Xiang Li and Shihao Ji"
    code: "https://github.com/lxuniverse/gdpa"
    keywords:
      - word: "patch attack"
      - word: "adversarial attack"
      - word: "adversarial defense"
      - word: "adversarial training"
      - word: "robustness"
      - word: "generative model"
      - word: ""
    paper: "papers/0492.pdf"
    supp: "supp/0492_supp.zip"
    abstract: "Adversarial patch attack is a family of attack algorithms that perturb a part of image to fool a deep neural network model. Existing patch attacks mostly consider injecting adversarial patches at input-agnostic locations: either a predefined location or a random location. This attack setup may be sufficient for attack but has considerable limitations when using it for adversarial training. Thus, robust models trained with existing patch attacks cannot effectively defend other adversarial attacks. In this paper, we first propose an end-to-end patch attack algorithm, Generative Dynamic Patch Attack (GDPA), which generates both patch pattern and patch location adversarially for each input image. We show that GDPA is a generic attack framework that can produce dynamic/static and visible/invisible patches with a few configuration changes. Secondly, GDPA can be readily integrated for adversarial training to improve model robustness to various adversarial attacks. Extensive experiments on VGGFace, Traffic Sign and ImageNet show that GDPA achieves higher attack success rates than state-of-the-art patch attacks, while adversarially trained model with GDPA demonstrates superior robustness to adversarial patch attacks than competing methods."
  - id: 499
    order: 261
    poster_session: 3
    session_id: 8
    title: "Audio-Visual Synchronisation in the wild"
    authors:
      - author: "Triantafyllos Afouras (University of Oxford)"
      - author: "Honglie Chen (University of Oxford)"
      - author: "Weidi Xie (University of Oxford)"
      - author: "Arsha Nagrani (Oxford University )"
      - author: "Andrea Vedaldi (Oxford University)"
      - author: "Andrew Zisserman (University of Oxford)"
    all_authors: "Triantafyllos Afouras, Honglie Chen, Weidi Xie, Arsha Nagrani, Andrea Vedaldi and Andrew Zisserman"
    code: "https://www.robots.ox.ac.uk/~vgg/research/avs"
    keywords:
      - word: "multimodal learning"
      - word: "self supervision"
      - word: "audio-visual synchronisation"
      - word: "dataset"
      - word: ""
    paper: "papers/0499.pdf"
    supp: ""
    abstract: "In this paper, we consider the problem of audio-visual synchronisation applied to videos ‘in-the-wild’ (i.e. of general classes beyond speech). As a new task, we identify and curate a test set with high audio-visual correlation, namely VGG-Sound Sync. We compare a number of transformer-based architectural variants specifically designed to model audio and visual signals of arbitrary length, while significantly reducing memory requirements during training. We further conduct an in-depth analysis on the curated dataset and define an evaluation metric for open domain audio-visual synchronisation. We apply our method on standard lip reading speech benchmarks, LRS2 and LRS3, with ablations in various aspects. Finally, we set the first benchmark for general audio-visual synchronisation with over 160 diverse classes in the new VGG-Sound Sync video dataset. In all cases, our proposed model outperforms the previous state-of-the-art by a significant margin."
  - id: 501
    order: 262
    poster_session: 3
    session_id: 8
    title: "2.5D-VoteNet: Depth Map based 3D Object Detection for Real-Time Applications"
    authors:
      - author: "Lanxiao Li (Karlsruher Institut fuer Technologie)"
      - author: "Michael Heizmann (Karlsruher Institut fuer Technologie)"
    all_authors: "Lanxiao Li and Michael Heizmann"
    code: ""
    keywords:
      - word: "3D Detection"
      - word: "realtime application"
      - word: "depth map"
      - word: ""
    paper: "papers/0501.pdf"
    supp: "supp/0501_supp.zip"
    abstract: "We address the 3D object detection task by capturing features directly on depth maps with a 2D CNN. Most existing 3D object detection methods take point clouds as input, even when each point cloud is converted from a single depth map. Although they have achieved impressive performance, point cloud based 3D detectors usually have high computational cost and complex structure, which limits their application on mobile devices and in real-time scenarios. Building on the state-of-the-art VoteNet, we propose 2.5-VoteNet, a powerful and efficient depth map based 3D detection pipeline. Since our models extract features directly on depth maps, most computation remains in 2D space and can be efficiently executed. Instead of using an off-the-shelf 2D CNN, we introduce relative depth convolution (RDConv) to learn robust local features. 
Our end-to-end pipeline achieves state-of-the-art results on the challenging SUN RGB-D benchmark and surpasses the baseline with a clear margin on ScanNet frame-level detection task. Meanwhile, our method reaches a significantly higher inference speed than existing methods (69 FPS)."
  - id: 502
    order: 157
    poster_session: 2
    session_id: 5
    title: "EBJR: Energy-Based Joint Reasoning for Adaptive Inference"
    authors:
      - author: "Mohammad Akbari (Huawei Technologies Canada Co. Ltd.)"
      - author: "Amin Banitalebi-Dehkordi (Huawei Technologies Canada Co., Ltd.)"
      - author: "Yong Zhang (Huawei Technologies Canada Co., Ltd.)"
    all_authors: "Mohammad Akbari, Amin Banitalebi-Dehkordi and Yong Zhang"
    code: "https://marketplace.huaweicloud.com/markets/aihub/notebook/detail/?id=7c9eb8ea-4aee-4616-9a70-85d186e51962"
    keywords:
      - word: "joint inference"
      - word: "energy-based models"
      - word: "adaptive inference"
      - word: "classification"
      - word: "regression"
    paper: "papers/0502.pdf"
    supp: "supp/0502_supp.zip"
    abstract: "State-of-the-art deep learning models have achieved significant performance levels on various benchmarks. However, the excellent performance comes at a cost of inefficient computational cost. Light-weight architectures, on the other hand, achieve moderate accuracies, but at a much more desirable latency. This paper presents a new method of jointly using the large accurate models together with the small fast ones. To this end, we propose an Energy-Based Joint Reasoning (EBJR) framework that adaptively distributes the samples between shallow and deep models to achieve an accuracy close to the deep model, but latency close to the shallow one. Our method is applicable to out-of-the-box pre-trained models as it does not require an architecture change nor re-training. Moreover, it is easy to use and deploy, especially for cloud services. Through a comprehensive set of experiments on different down-stream tasks, we show that our method outperforms strong state-of-the-art approaches with a considerable margin. In addition, we propose specialized EBJR, an extension of our method where we create a smaller specialized side model that performs the target task only partially, but yields an even higher accuracy and faster inference. We verify the strengths of our methods with both theoretical and experimental evaluations. Code and demo are available in the supplementary materials."
  - id: 506
    order: 326
    oral_session: 7
    poster_session: 4
    session_id: 9
    title: "Single Pixel Spectral Color Constancy"
    authors:
      - author: "Samu Koskinen (Huawei Technologies Oy (Finland) Co. Ltd)"
      - author: "Erman Acar (Huawei Technologies Oy (Finland) Co. Ltd)"
      - author: "Joni-Kristian Kamarainen (Tampere University)"
    all_authors: "Samu Koskinen, Erman Acar and Joni-Kristian Kamarainen"
    code: ""
    keywords:
      - word: "color constancy"
      - word: "spectral sensing"
      - word: "illuminant estimation"
      - word: "spectral data generation"
    paper: "papers/0506.pdf"
    supp: ""
    abstract: "We propose a novel approach for computational color constancy. Instead of raw RGB images used by the existing algorithms to estimate the scene white points, our approach is based on scene average color spectra - a single spectral pixel. We show that as few as 10-14 spectral channels are sufficient. Notably, the sensor output has five orders of magnitude less data than in raw RGB images of a 10MPix camera. The spectral sensor captures the “spectral fingerprints” of different light sources and the illuminant white point can be accurately estimated by a standard regressor. The regressor can be trained with generated measurements using the existing RGB color constancy datasets. To verify the results with real data, we collected a real spectral dataset with a commercial spectrometer. On all datasets the proposed Single Pixel Spectral Color Constancy obtains the highest accuracy in the single dataset and cross-dataset experiments. The method is particularly effective for the difficult scenes for which the average improvements are 40%-70% compared to state-of-the-arts."
  - id: 508
    order: 263
    poster_session: 3
    session_id: 8
    title: "Model Composition: Can Multiple Neural Networks Be Combined into a Single Network Using Only Unlabeled Data?"
    authors:
      - author: "Amin Banitalebi-Dehkordi (Huawei Technologies Canada Co., Ltd.)"
      - author: "Xinyu Kang (University of British Columbia)"
      - author: "Yong Zhang (Huawei Technologies Canada Co., Ltd.)"
    all_authors: "Amin Banitalebi-Dehkordi, Xinyu Kang and Yong Zhang"
    code: ""
    keywords:
      - word: "Model Composition"
      - word: "Combining Neural Networks"
      - word: "Pseudo Label"
      - word: "Self Training"
      - word: "Label Aggregation"
      - word: "Combining Models"
    paper: "papers/0508.pdf"
    supp: "supp/0508_supp.zip"
    abstract: "The diversity of deep learning applications, datasets, and neural network architectures necessitates a careful selection of the architecture and data that match best to a target application. As an attempt to mitigate this dilemma, this paper investigates the idea of combining multiple trained neural networks using unlabeled data. In addition, combining multiple models into one can speed up the inference, result in stronger, more capable models, and allows us to select efficient device-friendly target network architectures. To this end, the proposed method makes use of generation, filtering, and aggregation of reliable pseudo-labels collected from unlabeled data. Our method supports using an arbitrary number of input models with arbitrary architectures and categories. Extensive performance evaluations demonstrated that our method is very effective. For example, for the task of object detection and without using any ground-truth labels, an EfficientDet-D0 trained on Pascal-VOC and an EfficientDet-D1 trained on COCO, can be combined to a RetinaNet-ResNet50 model, with a similar mAP as the supervised training. If fine-tuned in a semi-supervised setting, the combined model achieves +18.6%, +12.6%, and +8.1% mAP improvements over supervised training with 1%, 5%, and 10% of labels."
  - id: 509
    order: 158
    poster_session: 2
    session_id: 5
    title: "Robust Semantic Segmentation with Superpixel-Mix"
    authors:
      - author: "Gianni Franchi (ENSTA Paris)"
      - author: "Nacim Belkhir (Safran)"
      - author: "Mai Lan Ha (University of Siegen)"
      - author: "Yufei Hu (ENSTA Paris)"
      - author: "Andrei Bursuc (valeo.ai)"
      - author: "Volker Blanz (University of Siegen)"
      - author: "Angela Yao (National University of Singapore)"
    all_authors: "Gianni Franchi, Nacim Belkhir, Mai Lan Ha, Yufei Hu, Andrei Bursuc, Volker Blanz and Angela Yao"
    code: "https://github.com/giannifranchi/deeplabv3-superpixelmix"
    keywords:
      - word: "robust AI"
      - word: "uncertainty"
      - word: "semantic segmentation"
      - word: "semi supervised learning"
      - word: "mathematical morphology"
    paper: "papers/0509.pdf"
    supp: "supp/0509_supp.zip"
    abstract: "Along with predictive performance and runtime speed, reliability is a key requirement for real-world semantic segmentation. Reliability encompasses robustness, predictive uncertainty and reduced bias.
To improve reliability, we introduce Superpixel-mix, a new superpixel-based data augmentation method with teacher-student consistency training. Unlike other mixing-based augmentation techniques, mixing superpixels between images 
is aware of object boundaries, while yielding consistent gains in segmentation accuracy. Our 
proposed technique achieves state-of-the-art results in semi-supervised semantic segmentation on the Cityscapes dataset. Moreover, Superpixel-mix 
improves the reliability of semantic segmentation by reducing network uncertainty and bias, as confirmed by competitive results under strong distributions shift (adverse weather, image corruptions) and when facing out-of-distribution data."
  - id: 510
    order: 41
    poster_session: 1
    session_id: 2
    title: "X-Distill: Improving Self-Supervised Monocular Depth via Cross-Task Distillation"
    authors:
      - author: "Hong Cai (Qualcomm AI Research)"
      - author: "Janarbek Matai (Qualcomm AI Research)"
      - author: "Shubhankar Borse (Qualcomm AI Research	)"
      - author: "Yizhe Zhang (Qualcomm AI Research)"
      - author: "Amin Ansari (Qualcomm Technologies, Inc.)"
      - author: "Fatih Porikli (Qualcomm AI Research)"
    all_authors: "Hong Cai, Janarbek Matai, Shubhankar Borse, Yizhe Zhang, Amin Ansari and Fatih Porikli"
    code: ""
    keywords:
      - word: "depth estimation"
      - word: "monocular depth estimation"
      - word: "3D"
      - word: "self-supervised learning"
      - word: "unsupervised learning"
      - word: "self-supervised monocular depth estimation"
      - word: "distillation"
    paper: "papers/0510.pdf"
    supp: "supp/0510_supp.zip"
    abstract: "In this paper, we propose a novel method, X-Distill, to improve the self-supervised training of monocular depth via cross-task knowledge distillation from semantic segmentation to depth estimation. More specifically, during training, we utilize a pretrained semantic segmentation teacher network and transfer its semantic knowledge to the depth network. In order to enable such knowledge distillation across two different visual tasks, we introduce a small, trainable network that translates the predicted depth map to a semantic segmentation map, which can then be supervised by the teacher network. In this way, this small network enables the backpropagation from the semantic segmentation teacher's supervision to the depth network during training. In addition, since the commonly used object classes in semantic segmentation are not directly transferable to depth, we study the visual and geometric characteristics of the objects and design a new way of grouping them that can be shared by both tasks. It is noteworthy that our approach only modifies the training process and does not incur additional computation during inference. We extensively evaluate the efficacy of our proposed approach on the standard KITTI benchmark and compare it with the latest state of the art. We further test the generalizability of our approach on Make3D. Overall, the results show that our approach significantly improves the depth estimation accuracy and outperforms the state of the art."
  - id: 514
    order: 159
    poster_session: 2
    session_id: 5
    title: "Jitter-CAM: Improving the Spatial Resolution of CAM-Based Explanations"
    authors:
      - author: "Thomas Hartley (Cardiff University)"
      - author: "Kirill Sidorov (Cardiff University)"
      - author: "Chris J Willis (BAE Systems AI Labs)"
      - author: "David Marshall (Cardiff University)"
    all_authors: "Thomas Hartley, Kirill Sidorov, Chris J Willis and David Marshall"
    code: "https://github.com/HartleyTW/Jitter-CAM"
    keywords:
      - word: "XAI"
      - word: "Explainable"
      - word: "Interpretable"
      - word: "Grad-CAM"
      - word: "Black-Box"
      - word: "Attribution"
    paper: "papers/0514.pdf"
    supp: "supp/0514_supp.zip"
    abstract: "Class Activation Mappings (CAMs) are a popular group of methods for creating visual explanations of the reasons behind a network's prediction. These techniques create explanations by weighting and visualising the output of the final convolution layer. Recent CAM techniques have sought to improve these explanations by introducing methods that aim to produce weights that more accurately represent how the networks informs its prediction. However, none of
these methods address the low spatial resolution of the final convolutional layer, leading to coarse explanations. In this paper, we propose Jitter-CAM, a method for producing and combining multiple CAM explanations that allow us to create
explanations with a higher spatial resolution than previous comparable methods. We use ImageNet and a number of well known architectures to show that our technique produces explanations that are both more accurate and better at
localising the target object. 
Anonymous code for Jitter-CAM is available at https://github.com/HartleyTW/Jitter-CAM."
  - id: 517
    order: 365
    poster_session: 4
    session_id: 11
    title: "Leveraging Human Selective Attention for Medical Image Analysis with Limited Training Data"
    authors:
      - author: "Yifei Huang (The University of Tokyo)"
      - author: "Xiaoxiao Li (The University of British Columbia)"
      - author: "Lijin Yang (The University of Tokyo)"
      - author: "Lin Gu (RIKEN，AIP / The University of Tokyo)"
      - author: "Yingying Zhu (University of Texas Arlington)"
      - author: "Hirofumi Seo (The University of Tokyo)"
      - author: "Qiuming Meng (The University of Tokyo)"
      - author: "Tatsuya Harada (The University of Tokyo / RIKEN)"
      - author: "Yoichi Sato (University of Tokyo)"
    all_authors: "Yifei Huang, Xiaoxiao Li, Lijin Yang, Lin Gu, Yingying Zhu, Hirofumi Seo, Qiuming Meng, Tatsuya Harada and Yoichi Sato"
    code: ""
    keywords:
      - word: "human gaze"
      - word: "medical imaging"
    paper: "papers/0517.pdf"
    supp: "supp/0517_supp.zip"
    abstract: "Human gaze is a cost-efficient physiological data that reveals human underlying attentional patterns. Also known as the selective attention mechanism, it helps the cognition system focus on task-relevant visual clues by ignoring the presence of distractors. Thanks to this ability, our human is able to efficiently learn from the very limited number of training samples. Here, we aim to leverage gaze for medical image analysis on both segmentation and classification. Our proposed framework includes a backbone encoder and a Selective Attention Network (SAN) that simulates the underlying attention. The SAN internally encodes information such as suspicious regions that is relevant to the medical diagnose tasks by estimating the actual human gaze. Then we design a novel Auxiliary Attention Block (AAB) to allow information from SAN to be utilized by the backbone encoder to focus on selective areas. Specifically, this block uses a modified version of a multi-head attention layer to simulate the human visual search procedure. With this design, the SAN and AAB can be plugged into different backbones, and the framework can be used for multiple medical image analysis tasks when equipped with task-specific heads. Our method is demonstrated to achieve superior performance on both tasks of 3D tumor segmentation and chest X-ray diagnosis. We also show that the estimated gaze probability map of the SAN is consistent with an actual gaze fixation map obtained by board-certified doctors."
  - id: 520
    order: 160
    poster_session: 2
    session_id: 5
    title: "Quantised Transforming Auto-Encoders: Achieving Equivariance to Arbitrary Transformations in Deep Networks"
    authors:
      - author: "Jianbo Jiao (University of Oxford)"
      - author: "Joao F Henriques (University of Oxford)"
    all_authors: "Jianbo Jiao and Joao F Henriques"
    code: "https://www.robots.ox.ac.uk/~vgg/research/qtae/"
    keywords:
      - word: "Equivariance"
      - word: "transformations"
      - word: "discretise"
      - word: "auto-encoder"
      - word: "compositionality"
      - word: "interpretability"
      - word: "capsule networks"
      - word: "novel views re-rendering"
      - word: "pose estimation"
    paper: "papers/0520.pdf"
    supp: "supp/0520_supp.zip"
    abstract: "In this work we investigate how to achieve equivariance to input transformations in deep networks, purely from data, without being given a model of those transformations. Convolutional Neural Networks (CNNs), for example, are equivariant to image translation, a transformation that can be easily modelled (by shifting the pixels vertically or horizontally). Other transformations, such as out-of-plane rotations, do not admit a simple analytic model. We propose an auto-encoder architecture whose embedding obeys an arbitrary set of equivariance relations simultaneously, such as translation, rotation, colour changes, and many others. This means that it can take an input image, and produce versions transformed by a given amount that were not observed before (e.g. a different point of view of the same object, or a colour variation). Despite extending to many (even non-geometric) transformations, our model reduces exactly to a CNN in the special case of translation-equivariance. Equivariances are important for the interpretability and robustness of deep networks, and we demonstrate results of successful re-rendering of transformed versions of input images on several synthetic and real datasets, as well as results on object pose estimation."
  - id: 524
    order: 42
    poster_session: 1
    session_id: 2
    title: "Multiple Fusion Adaptation: A Strong Framework for Unsupervised Semantic Segmentation Adaptation"
    authors:
      - author: "Kai Zhang (University of Chinese Academy of Sciences)"
      - author: "Yifan Sun (Baidu Research)"
      - author: "Rui Wang (Science and Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences)"
      - author: "Haichang Li (Science and Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences)"
      - author: "Xiaohui Hu (	Institute of Software Chinese Academy of Sciences)"
    all_authors: "Kai Zhang, Yifan Sun, Rui Wang, Haichang Li and Xiaohui Hu"
    code: ""
    keywords:
      - word: "domain adaptation"
      - word: "semantic segmentation"
      - word: "pseudo label learning"
    paper: "papers/0524.pdf"
    supp: "supp/0524_supp.zip"
    abstract: "This paper challenges the cross-domain semantic segmentation task, aiming to improve the segmentation accuracy on the unlabeled target domain without incurring additional annotation. Using the pseudo-label-based unsupervised domain adaptation (UDA) pipeline, we propose a novel and effective Multiple Fusion Adaptation (MFA) method. MFA basically considers three parallel information fusion strategies, i.e., the cross-model fusion, temporal fusion and a novel online-offline pseudo label fusion. Specifically, the online-offline pseudo label fusion encourages the adaptive training to pay additional attention to difficult regions that are easily ignored by offline pseudo labels, therefore retaining more informative details. While the other two fusion strategies may look standard, MFA pays significant efforts to raise the efficiency and effectiveness for integration, and succeeds in injecting all the three strategies into a unified framework. Experiments on two widely used benchmarks, i.e., GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes, show that our method significantly improves the semantic segmentation adaptation, and sets up new state of the art (58.2% and 62.5% mIoU, respectively). We will make the code publicly available."
  - id: 533
    order: 161
    poster_session: 2
    session_id: 5
    title: "Modeling Explicit Concerning States for Reinforcement Learning in Visual Dialogue"
    authors:
      - author: "Zipeng Xu (University of Trento)"
      - author: "Fandong Meng (Tencent WeChat AI - Pattern Recognition Center Tencent Inc.)"
      - author: "Xiaojie Wang (Beijing University of Posts and Telecommunications)"
      - author: "Duo Zheng (Beijing University of Posts and Telecommunications)"
      - author: "Chenxu Lv (Beijing University of Posts and Telecommunications)"
      - author: "Jie Zhou (Tencent)"
    all_authors: "Zipeng Xu, Fandong Meng, Xiaojie Wang, Duo Zheng, Chenxu Lv and Jie Zhou"
    code: "https://github.com/zipengxuc/ecs-visdial-rl"
    keywords:
      - word: "Visual Dialogue"
      - word: "Vision + Language"
      - word: "Reinforcement Learning"
      - word: "Visual Grounded Natural Language Generation"
      - word: ""
    paper: "papers/0533.pdf"
    supp: "supp/0533_supp.zip"
    abstract: "To encourage AI agents to conduct meaningful Visual Dialogue (VD), the use of Reinforcement Learning has been proven potential. In Reinforcement Learning, it is crucial to represent states and assign rewards based on the action-caused transitions of states. However, the state representation in previous Visual Dialogue works uses the textual information only and its transitions are implicit. In this paper, we propose Explicit Concerning States (ECS) to represent what visual contents are concerned at each round and what have been concerned throughout the Visual Dialogue. ECS is modeled from multimodal information and is represented explicitly. Based on ECS, we formulate two intuitive and interpretable rewards to encourage the Visual Dialogue agents to converse on diverse and informative visual information. Experimental results on the VisDial v1.0 dataset show our method enables the Visual Dialogue agents to generate more visual coherent, less repetitive and more visual informative dialogues compared with previous methods, according to multiple automatic metrics, human study and qualitative analysis."
  - id: 544
    order: 366
    poster_session: 4
    session_id: 11
    title: "OMAD: Object Model with Articulated Deformations for Pose Estimation and Retrieval"
    authors:
      - author: "Han Xue (Shanghai Jiao Tong University)"
      - author: "Liu Liu (Shanghai JiaoTong University)"
      - author: "Wenqiang Xu (Shanghai Jiao Tong University)"
      - author: "Haoyuan Fu (Shanghai Jiao Tong University)"
      - author: "Cewu Lu (Shanghai Jiao Tong University)"
    all_authors: "Han Xue, Liu Liu, Wenqiang Xu, Haoyuan Fu and Cewu Lu"
    code: "https://sites.google.com/view/omad-bmvc/"
    keywords:
      - word: "articulated objects"
      - word: "object pose estimation"
      - word: "object retrieval"
      - word: "object deformation model"
      - word: "articulated deformation"
      - word: "category-level pose estimation"
      - word: "3D representation"
      - word: ""
    paper: "papers/0544.pdf"
    supp: "supp/0544_supp.zip"
    abstract: "Articulated objects are pervasive in daily life. However, due to the intrinsic high-DoF structure, the joint states of the articulated objects are hard to be estimated. To model articulated objects, two kinds of shape deformations namely the geometric and the pose deformation should be considered. In this work, we present a novel category-specific parametric representation called Object Model with Articulated Deformations (OMAD) to explicitly model the articulated objects. In OMAD, a category is associated with a linear shape function with shared shape basis and a non-linear joint function. Both functions can be learned from a large-scale object model dataset and fixed as category-specific priors. Then we propose an OMADNet to predict the shape parameters and the joint states from an object's single observation. With the full representation of the object shape and joint states, we can address several tasks including category-level object pose estimation and the articulated object retrieval. To evaluate these tasks, we create a synthetic dataset based on PartNet-Mobility. Extensive experiments show that our simple OMADNet can serve as a strong baseline for both tasks. Our code and dataset will be made public."
  - id: 548
    order: 264
    poster_session: 3
    session_id: 8
    title: "MFE: Multi-scale Feature Enhancement for Object Detection"
    authors:
      - author: "Zhenhu Zhang (Shandong University)"
      - author: "Xueying Qin (Shandong University)"
      - author: "Fan Zhong (Shandong University)"
    all_authors: "Zhenhu Zhang, Xueying Qin and Fan Zhong"
    code: ""
    keywords:
      - word: "FPN"
      - word: "object detection"
      - word: "non-local attention"
      - word: "self-attention"
      - word: "one-stage"
      - word: "two-stage"
      - word: "anchor-free"
      - word: ""
    paper: "papers/0548.pdf"
    supp: ""
    abstract: "The state-of-the-art one-stage detectors are usually implemented with Feature Pyramid Network (FPN) as neck. FPN fuses multi-scale feature information so that the detector can better deal with objects with different scales. However, FPN has information loss due to feature dimension reduction. In this paper, we introduce a new feature enhancement architecture named Multi-scale Feature Enhancement (MFE). MFE includes Scale Fusion, CombineFPN and Pixel-Region Attention module. Scale Fusion can supplement the low-level information to the high-level features without the influence of semantic gap. CombineFPN further combines top-down and bottom-up structure to reduce the information loss of all scale features. Scale Fusion and CombineFPN can fully fuse features from different levels to enhance the multi-scale features.  Pixel-Region Module, a lightweight non-local attention method, is finally used to enhance features with distant neighborhood information.  For FCOS, RetinaNet and Mask R-CNN with ResNet50, using MFE can increase the Average Precision (AP)  by 1.2, 1.1 and 1.0 points on MS COCO test-dev.  For ATSS and FSAF with  ResNet101 as backbone, using  MFE can increase AP by 1.2 and 1.3 points. Our method also performs well on Pascal VOC dataset."
  - id: 549
    order: 162
    poster_session: 2
    session_id: 5
    title: "Variance-stationary Differentiable NAS"
    authors:
      - author: "Hyeokjun Choe (Seoul National University)"
      - author: "Byunggook Na (Seoul National University)"
      - author: "Jisoo Mok (Seoul National University)"
      - author: "Sungroh Yoon (Seoul National University)"
    all_authors: "Hyeokjun Choe, Byunggook Na, Jisoo Mok and Sungroh Yoon"
    code: ""
    keywords:
      - word: "darts"
      - word: "differentiable nas"
      - word: "one-shot nas"
      - word: "neural architecture search"
      - word: "nas"
      - word: "architecture parameter"
      - word: "automl"
      - word: "vs-darts"
      - word: "vsdarts"
      - word: "variance-stationary"
      - word: ""
    paper: "papers/0549.pdf"
    supp: "supp/0549_supp.pdf"
    abstract: "Differentiable architecture search (DARTS) has become the popular method of neural architecture search (NAS) due to its adaptability and low computational cost. However, following the publication of DARTS, it has been found that DARTS often yields a sub-optimal neural architecture because architecture parameters do not accurately represent operation strengths. Through extensive theoretical analysis and empirical observations, we reveal that this issue occurs as a result of the existence of unnormalized operations. Based on our finding, we propose a novel variance-stationary differentiable architecture search (VS-DARTS), which consists of node normalization, local adaptive learning rate, and sqrt(beta)-continuous relaxation. Comprehensively, VS-DARTS makes the architecture parameters a more reliable metric for deriving a desirable architecture without increasing the search cost. In addition to the theoretical motivation behind all components of VS-DARTS, we provide strong experimental results to demonstrate that they synergize to significantly improve the search performance. The architecture searched by VS-DARTS achieves the test error of 2.50% on CIFAR-10 and 24.7% on ImageNet."
  - id: 568
    order: 265
    poster_session: 3
    session_id: 8
    title: "Exploiting Scene Depth for Object Detection with Multimodal Transformers"
    authors:
      - author: "Hwanjun Song (NAVER AI Lab)"
      - author: "Eunyoung Kim (Google)"
      - author: "Varun Jampani (Google)"
      - author: "Deqing Sun (Google)"
      - author: "Jae-Gil Lee (KAIST)"
      - author: "Ming-Hsuan Yang (University of California at Merced)"
    all_authors: "Hwanjun Song, Eunyoung Kim, Varun Jampani, Deqing Sun, Jae-Gil Lee and Ming-Hsuan Yang"
    code: "https://github.com/songhwanjun/MEDUSA"
    keywords:
      - word: "RGB-D object detection"
      - word: "RGB-D"
      - word: "object detection"
      - word: "inferred depth"
      - word: "estimated depth"
    paper: "papers/0568.pdf"
    supp: "supp/0568_supp.zip"
    abstract: "We propose a generic framework MEDUSA (Multimodal Estimated-Depth Unification with Self-Attention) to fuse RGB and depth information using multimodal transformers in the context of object detection. Unlike previous methods that use the depth measured from various physical sensors such as Kinect and Lidar, we show that the depth maps inferred by a monocular depth estimator can play an important role to enhance the performance of modern object detectors. In order to make use of the estimated depth, MEDUSA encompasses a robust feature extraction phase, followed by multimodal transformers for RGB-D fusion. The main strength of MEDUSA lies in its broad applicability for any existing large-scale RGB datasets including PASCAL VOC and Microsoft COCO. Extensive experiments with three datasets show that MEDUSA achieves higher precision than several strong baselines."
  - id: 570
    order: 43
    poster_session: 1
    session_id: 2
    title: "ECINN: Efficient Counterfactuals from Invertible Neural Networks"
    authors:
      - author: "Frederik Hvilshøj (Aarhus University)"
      - author: "Alexandros Iosifidis (Aarhus University)"
      - author: "Ira Assent (Aarhus University)"
    all_authors: "Frederik Hvilshøj, Alexandros Iosifidis and Ira Assent"
    code: "https://github.com/fhvilshoj/ECINN"
    keywords:
      - word: "XAI"
      - word: "Explainability"
      - word: "Counterfactuals"
      - word: "ECINN"
    paper: "papers/0570.pdf"
    supp: "supp/0570_supp.zip"
    abstract: "Counterfactual examples identify how inputs can be altered to change the predicted class of a classifier, thus opening up the black-box nature of, e.g., deep neural networks. We propose a method, ECINN, that utilizes the generative capacities of invertible neural networks for image classification to generate counterfactual examples efficiently. In contrast to competing methods that sometimes need a thousand evaluations or more of the classifier, ECINN has a closed-form expression and generates a counterfactual in the time of only two evaluations. Arguably, the main challenge of generating counterfactual examples is to alter only input features that affect the predicted outcome, i.e., class-dependent features. Our experiments demonstrate how ECINN alters class-dependent image regions to change the perceptual and predicted class, producing more realistically looking counterfactuals three orders of magnitude faster than competing methods."
  - id: 575
    order: 163
    poster_session: 2
    session_id: 5
    title: "AttNAS: Searching Attentions for Lightweight Semantic Segmentation"
    authors:
      - author: "yingying jiang ( 	Samsung Research China,Beijing)"
      - author: "Zhuoxin  Gan (Samsung Research Institute China-Beijing (SRC-B)"
      - author: ")"
      - author: "Ke Lin (Samsung Research Institute China-Beijing (SRC-B))"
      - author: "Yong A (Samsung Research China, Beijing)"
    all_authors: "Yingying Jiang, Zhuoxin  Gan, ), Ke Lin and Yong A"
    code: ""
    keywords:
      - word: "Neural architecture search"
      - word: "semantic segmentation"
      - word: "AttNAS"
      - word: "lightweight"
      - word: "attention"
    paper: "papers/0575.pdf"
    supp: "supp/0575_supp.zip"
    abstract: "To use multiple attentions in semantic segmentation task, attentions are generally arranged in parallel and then combined by concatenation or summation. In this work, we use Neural Architecture Search (NAS) to explore attentions and their combination patterns (e.g., parallel, hybrid parallel+sequential) in semantic segmentation. We propose AttNAS, which searches both backbone and attention for a lightweight semantic segmentation model. In particular, we define a new attention search space with a two-layer structure and propose a unified differentiable formulation to support both attention type search and attention combination search. Our experimental results show that the model searched with AttNAS achieves state-of-the-art compared to existing lightweight methods on Cityscapes, CamVid and Pascal VOC 2012. Moreover, the attention patterns obtained by AttNAS have robust generalization capability and can be used in combination with existing backbones, such as MobileNetV2 and ResNet18, to improve segmentation performance."
  - id: 579
    order: 219
    oral_session: 5
    poster_session: 3
    session_id: 6
    title: "HairFIT: Pose-invariant Hairstyle Transfer via Flow-based Hair Alignment and Semantic-region-aware Inpainting"
    authors:
      - author: "Chaeyeon Chung (	Korea Advanced Institute of Science and Technology)"
      - author: "Taewoo Kim (KAIST)"
      - author: "Hyelin Nam (Chung-Ang University)"
      - author: "Seunghwan Choi (Korea Advanced Institute of Science and Technology )"
      - author: "Gyojung Gu (Hanyang University)"
      - author: "Sunghyun  Park (KAIST)"
      - author: "Jaegul Choo (Korea Advanced Institute of Science and Technology)"
    all_authors: "Chaeyeon Chung, Taewoo Kim, Hyelin Nam, Seunghwan Choi, Gyojung Gu, Sunghyun  Park and Jaegul Choo"
    code: ""
    keywords:
      - word: "hairstyle transfer"
      - word: "optical flow"
      - word: "conditional image generation"
      - word: "generative adversarial networks"
      - word: ""
    paper: "papers/0579.pdf"
    supp: "supp/0579_supp.zip"
    abstract: "Hairstyle transfer is the task of modifying a source hairstyle to a target one. Although recent hairstyle transfer models can reflect the delicate features of hairstyles, they still have two major limitations. First, the existing methods fail to transfer hairstyles when a source and a target image have different poses (e.g., viewing direction or face size), which is prevalent in the real world. Also, the previous models generate unrealistic im-ages when there is a non-trivial amount of regions in the source image occluded by its original hair. When modifying long hair to short hair, shoulders or backgrounds occluded by the long hair need to be inpainted. To address these issues, we propose a novel framework for pose-invariant hairstyle transfer, HairFIT. Our model consists of two stages: 1)flow-based hair alignment and 2) hair synthesis. In the hair alignment stage, we leverage a keypoint-based optical flow estimator to align a target hairstyle with a source pose. Then, we generate a final hairstyle-transferred image in the hair synthesis stage based on Semantic-region-aware Inpainting Mask (SIM) estimator. Our SIM estimator divides the occluded regions in the source image into different semantic regions to reflect their distinct features during the inpainting. To demonstrate the effectiveness of our model, we conduct quantitative and qualitative evaluations using multi-view datasets, K-hairstyle and VoxCeleb. The results indicate that HairFIT achieves a state-of-the-art performance by successfully transferring hairstyles between images of different poses, which have never been achieved before."
  - id: 580
    order: 266
    poster_session: 3
    session_id: 8
    title: "MOS: A Low Latency and Lightweight Framework for Face Detection, Landmark Localization, and Head Pose Estimation"
    authors:
      - author: "Yepeng Liu (UBTech Research)"
      - author: "Zaiwang Gu (UBTech Research)"
      - author: "Shenghua Gao (Shanghaitech University)"
      - author: "Dong Wang (UBTech Research)"
      - author: "Yusheng Zeng (UBTech Research)"
      - author: "Jun Cheng (Institute for Infocomm Research)"
    all_authors: "Yepeng Liu, Zaiwang Gu, Shenghua Gao, Dong Wang, Yusheng Zeng and Jun Cheng"
    code: "https://github.com/lyp-deeplearning/MOS-Multi-Task-Face-Detect"
    keywords:
      - word: "face detect"
      - word: "head pose estimation"
      - word: "multi-task"
      - word: "Low Latency"
      - word: ""
    paper: "papers/0580.pdf"
    supp: "supp/0580_supp.zip"
    abstract: "With the emergence of service robots and surveillance cameras, dynamic face recognition (DFR) in wild has received much attention in recent years. Face detection and head pose estimation are two important steps for DFR. Very
often, the pose is estimated after the face detection. However, such sequential computations lead to higher latency. In this paper, we propose a low latency and lightweight network for simultaneous face detection, landmark localization
and head pose estimation. Inspired by the observation that it is more challenging to locate the facial landmarks for faces with large angles, a pose loss is proposed to constrain the learning. Moreover, we also propose an uncertainty multi-task loss to learn the weights of individual tasks automatically. Another challenge is that robots often use low computational units like ARM based computing core and we often need to use lightweight networks instead of the heavy ones, which lead to performance drop especially for small and hard faces. In this paper, we propose online feedback sampling to augment the training samples across different scales, which increases the diversity of training data automatically. Through validation in commonly used WIDER FACE, AFLW and AFLW2000 datasets, the results show that the proposed method achieves the state-of-the-art performance in low computational resources."
  - id: 585
    order: 367
    poster_session: 4
    session_id: 11
    title: "Separable Batch Normalization for Robust Facial Landmark Localization"
    authors:
      - author: "Shuangping Jin (Southeast University)"
      - author: "Zhenhua Feng (University of Surrey)"
      - author: "Wankou Yang (Southeast University)"
      - author: "Josef Kittler (University of Surrey, UK)"
    all_authors: "Shuangping Jin, Zhenhua Feng, Wankou Yang and Josef Kittler"
    code: "https://github.com/FunkyKoki/Separable-Batch-Normalization-for-Robust-Facial-Landmark-Localization"
    keywords:
      - word: "face alignment"
      - word: "batch normalization"
      - word: "dynamic network"
      - word: ""
    paper: "papers/0585.pdf"
    supp: "supp/0585_supp.zip"
    abstract: "A big, diverse and balanced training data is the key to the success of deep neural network training. However, existing publicly available datasets used in facial landmark localization are usually much smaller than those for other computer vision tasks. To mitigate this issue, this paper presents a novel Separable Batch Normalization (SepBN) method. Different from the classical BN layer, the proposed SepBN module learns multiple sets of mapping parameters to adaptively scale and shift the normalized feature maps via a feed-forward attention mechanism. The channels of an input tensor are divided into several groups and different mapping parameter combinations are calculated for each group according to the attention weights to improve the parameter utilization. The experimental results obtained on several well-known benchmarking datasets demonstrate the effectiveness and merits of the proposed method."
  - id: 590
    order: 44
    poster_session: 1
    session_id: 2
    title: "Progressive Growing of Points with Tree-structured Generators"
    authors:
      - author: "Hyeontae Son (NAVER LABS)"
      - author: "Young Min Kim (Seoul National University)"
    all_authors: "Hyeontae Son and Young Min Kim"
    code: "https://github.com/countywest/progressive_growing_of_points"
    keywords:
      - word: "Point cloud auto-encoder"
      - word: "Progressive growing"
      - word: "Tree-structured generators"
    paper: "papers/0590.pdf"
    supp: "supp/0590_supp.zip"
    abstract: "We present progressive growing of points with tree-structured networks that generates high-fidelity point cloud. Because point cloud data lacks the information of inherent topology or connectivity between neighboring points, the data generated from deep neural networks usually fails to faithfully produce local details. Inspired by the recent success of the progressive generation of images and curriculum learning, we suggest that the hierarchical structure of the tree-based network architecture can endow contextual information to enforce the progressive generation of point clouds. When the tree-structured network is incrementally trained by progressively adding the subsequent layers of depth, the quality of generated point cloud is superior to the data generated by the same network structure with naïve end-to-end training. Furthermore, our pipeline simultaneously learns the hierarchical structure within the data set and finds a consistent spatial decomposition of 3D shapes by coherently positioning the nodes with the same ancestors. Extensive experiments show that our method can produce a high-fidelity shape when applied to shape generation and completion as well as auto-encoding point clouds."
  - id: 591
    order: 335
    oral_session: 8
    poster_session: 4
    session_id: 10
    title: "How Important is Importance Sampling for Deep Budgeted Training?"
    authors:
      - author: "Eric Arazo (Insight Centre for Data Analytics (DCU))"
      - author: "Diego Ortego (Insight Centre for Data Analytics (DCU))"
      - author: "Paul Albert (Insight Centre for Data Analytics (DCU))"
      - author: "Noel O'Connor (Dublin City University (DCU))"
      - author: "Kevin McGuinness (Insight Centre for Data Analytics)"
    all_authors: "Eric Arazo, Diego Ortego, Paul Albert, Noel O'Connor and Kevin McGuinness"
    code: "https://git.io/JKHa3"
    keywords:
      - word: "Budgeted training"
      - word: "importance sampling"
      - word: "data augmentation"
      - word: "deep learning"
    paper: "papers/0591.pdf"
    supp: ""
    abstract: "Long iterative training processes for Deep Neural Networks (DNNs) are commonly required to achieve state-of-the-art performance in many computer vision tasks. Importance sampling approaches might play a key role in budgeted training regimes, i.e. when limiting the number of training iterations. These approaches aim at dynamically estimating the importance of each sample to focus on the most relevant and speed up convergence. This work explores this paradigm and how a budget constraint interacts with importance sampling approaches and data augmentation techniques. We show that under budget restrictions, importance sampling approaches do not provide a consistent improvement over uniform sampling. We suggest that, given a specific budget, the best course of action is to disregard the importance and introduce adequate data augmentation; e.g. when reducing the budget to a 30% in CIFAR-10/100, RICAP data augmentation maintains accuracy, while importance sampling does not. We conclude from our work that DNNs under budget restrictions benefit greatly from variety in the training set and that finding the right samples to train on is not the most effective strategy when balancing high performance with low computational requirements. Source code available at: https://git.io/JKHa3"
  - id: 595
    order: 164
    poster_session: 2
    session_id: 5
    title: "Few-shot Semantic Segmentation with Self-supervision from Pseudo-classes"
    authors:
      - author: "Yiwen Li (University of Oxford)"
      - author: "Gratianus Wesley Putra Data (University of Oxford)"
      - author: "Yunguan Fu (University College London)"
      - author: "Yipeng Hu (University College London)"
      - author: "Victor Adrian Prisacariu (University of Oxford)"
    all_authors: "Yiwen Li, Gratianus Wesley Putra Data, Yunguan Fu, Yipeng Hu and Victor Adrian Prisacariu"
    code: ""
    keywords:
      - word: "few-shot semantic segmentation"
      - word: "self-supervision"
    paper: "papers/0595.pdf"
    supp: "supp/0595_supp.zip"
    abstract: "Despite the success of deep learning methods for semantic segmentation, few-shot semantic segmentation remains a challenging task due to the limited training data and the generalisation requirement for unseen classes. While recent progress has been particularly encouraging, we discover that existing methods tend to have poor performance in terms of meanIoU when query images contain other semantic classes besides the target class.  To address this issue, we propose a novel self-supervised task that generates random pseudo-classes in the background of the query images, providing extra training data that would otherwise be unavailable when predicting individual target classes.  To that end, we adopted superpixel segmentation for generating the pseudo-classes.  With this extra supervision, we improved the meanIoU performance of the state-of-the-art method by 2.5% and 5.1% on the one-shot tasks, as well as 6.7% and 4.4% on the five-shot tasks, on the PASCAL-5i and COCO benchmarks, respectively."
  - id: 600
    order: 368
    poster_session: 4
    session_id: 11
    title: "Cascading Feature Extraction for Fast Point Cloud Registration"
    authors:
      - author: "Yoichiro Hisadome (The University of Tokyo)"
      - author: "Yusuke Matsui (The University of Tokyo)"
    all_authors: "Yoichiro Hisadome and Yusuke Matsui"
    code: "https://github.com/yoichiro0406/cascading_feature_extraction"
    keywords:
      - word: "point cloud registration"
      - word: ""
    paper: "papers/0600.pdf"
    supp: ""
    abstract: "We propose a method for speeding up a 3D point cloud registration through a cascading feature extraction. The current approach with the highest accuracy is realized by iteratively executing feature extraction and registration using deep features. However, iterative feature extraction takes time. Our proposed method significantly reduces the computational cost using cascading shallow layers. Our idea is to omit redundant computations that do not always contribute to the final accuracy. The proposed approach is approximately three times faster than the existing methods without a loss of accuracy."
  - id: 601
    order: 267
    poster_session: 3
    session_id: 8
    title: "Channel DropBlock: An Improved Regularization Method for Fine-Grained Visual Classification"
    authors:
      - author: "Yifeng Ding (Beijing University of Posts and Telecommunications)"
      - author: "Shuwei Dong (Beijing University of Posts and Telecommunications)"
      - author: "Yujun Tong (北京邮电大学)"
      - author: "Zhanyu Ma (Beijing University of Posts and Telecommunications)"
      - author: "Bo Xiao (Beijing University of Posts and Telecommunications)"
      - author: "Haibin Ling (Stony Brook University)"
    all_authors: "Yifeng Ding, Shuwei Dong, Yujun Tong, Zhanyu Ma, Bo Xiao and Haibin Ling"
    code: "https://github.com/PRIS-CV/DropChannelBlock_Pytorch_master"
    keywords:
      - word: "regularization"
      - word: "fine-grained"
      - word: ""
    paper: "papers/0601.pdf"
    supp: ""
    abstract: "Classifying the sub-categories of an object from the same super-category (e.g., bird) in a fine-grained visual classification (FGVC) task highly relies on mining multiple discriminative features. Existing approaches tackle this problem mainly by introducing attention mechanisms to locate the discriminative parts or feature encoding approaches to extract the highly parameterized features in a weakly-supervised fashion. In this work, we propose a lightweight yet effective regularization method named Channel DropBlock (CDB), in combination with two alternative correlation metrics, to address this problem. The key idea is to randomly mask out a group of correlated channels during training to destruct features from co-adaptations, and thus enhance feature representations. Extensive experiments on three benchmark FGVC datasets show that CDB effectively improves the performance."
  - id: 604
    order: 369
    poster_session: 4
    session_id: 11
    title: "Center-wise Local Image Mixture For Contrastive Representation Learning"
    authors:
      - author: "hao li (Hikvision Digital Technology Co. Ltd)"
      - author: "xiaopeng zhang (Huawei Cloud EI )"
      - author: "Hongkai Xiong (Shanghai Jiao Tong University)"
    all_authors: "Hao Li, Xiaopeng Zhang and Hongkai Xiong"
    code: ""
    keywords:
      - word: "self-supervised learning"
      - word: "contrastive learning"
      - word: "representation learning；"
    paper: "papers/0604.pdf"
    supp: "supp/0604_supp.zip"
    abstract: "Contrastive learning based on instance discrimination trains model to discriminate different transformations of the anchor sample from other samples, which does not consider the semantic similarity among samples. This paper proposes a new kind of contrastive learning method, named CLIM, which uses positives from other samples in the dataset. This is achieved by searching local similar samples of the anchor, and selecting samples that are closer to the corresponding cluster center, which we denote as center-wise local image selection. The selected samples are instantiated via an data mixture strategy, which performs as a smoothing regularization. As a result, CLIM encourages both local similarity and global aggregation in a robust way, which we find is beneficial for feature representation. Besides, we introduce emph{multi-resolution} augmentation, which enables the representation to be scale invariant. We reach 75.5% top-1 accuracy with linear evaluation over ResNet-50, and 59.3% top-1 accuracy when fine-tuned with only 1% labels."
  - id: 606
    order: 370
    poster_session: 4
    session_id: 11
    title: "Multi-Stream Attention Learning for Monocular Vehicle Velocity and Inter-Vehicle Distance Estimation"
    authors:
      - author: "Kuan-Chih Huang (National Taiwan University)"
      - author: "Yu-Kai Huang (National Taiwan University)"
      - author: "Winston H. Hsu (National Taiwan University)"
    all_authors: "Kuan-Chih Huang, Yu-Kai Huang and Winston H. Hsu"
    code: ""
    keywords:
      - word: "velocity estimation"
      - word: "distance estimation"
      - word: "relative constraint"
      - word: "autonomous driving"
      - word: "autonomous vehicle"
      - word: "perception"
      - word: "ADAS"
      - word: ""
    paper: "papers/0606.pdf"
    supp: ""
    abstract: "Vehicle velocity and inter-vehicle distance estimation are essential for ADAS (Advanced driver-assistance systems) and autonomous vehicles. To save the cost of expensive ranging sensors, recent studies focus on using a low-cost monocular camera to perceive the environment around the vehicle in a data-driven fashion. Existing approaches treat each vehicle independently for perception and cause inconsistent estimation. 
Furthermore, important information like context and spatial relation in 2D object detection is often neglected in the velocity estimation pipeline.
In this paper, we explore the relationship between vehicles of the same frame with a global-relative-constraint (GLC) loss to encourage consistent estimation. A novel multi-stream attention network (MSANet) is proposed to extract different aspects of features, e.g., spatial and contextual features, for joint vehicle velocity and inter-vehicle distance estimation.
Experiments show the effectiveness and robustness of our proposed approach. MSANet outperforms state-of-the-art algorithms on both the KITTI dataset and TuSimple velocity dataset."
  - id: 610
    order: 268
    poster_session: 3
    session_id: 8
    title: "With a Little Help from my Temporal Context: Multimodal Egocentric Action Recognition"
    authors:
      - author: "Evangelos Kazakos (University of Bristol)"
      - author: "Jaesung Huh (University of Oxford)"
      - author: "Arsha Nagrani (Google )"
      - author: "Andrew Zisserman (University of Oxford)"
      - author: "Dima Damen (University of Bristol)"
    all_authors: "Evangelos Kazakos, Jaesung Huh, Arsha Nagrani, Andrew Zisserman and Dima Damen"
    code: "https://github.com/ekazakos/MTCN"
    keywords:
      - word: "egocentric action recognition"
      - word: "multimodal"
      - word: "temporal context"
    paper: "papers/0610.pdf"
    supp: "supp/0610_supp.zip"
    abstract: "In egocentric videos, actions occur in quick succession. We capitalise on the action's temporal context and propose a method that learns to attend to surrounding actions in order to improve recognition performance. To incorporate the temporal context, we propose a transformer-based multimodal model that ingests video and audio as input modalities, with an explicit language model providing action sequence context to enhance the predictions. We test our approach on EPIC-KITCHENS and EGTEA datasets reporting state-of-the-art performance. Our ablations showcase the advantage of utilising temporal context as well as incorporating audio input modality and language model to rescore predictions. Code and models at: https://github.com/ekazakos/MTCN."
  - id: 611
    order: 269
    poster_session: 3
    session_id: 8
    title: "HR-RCNN: Hierarchical Relational Reasoning for Object Detection"
    authors:
      - author: "Hao Chen (University of Maryland)"
      - author: "Abhinav Shrivastava (University of Maryland)"
    all_authors: "Hao Chen and Abhinav Shrivastava"
    code: ""
    keywords:
      - word: "object detection"
      - word: "visual reasoning"
      - word: "attention"
      - word: "hierarchical reasoning"
    paper: "papers/0611.pdf"
    supp: "supp/0611_supp.zip"
    abstract: "Incorporating relational reasoning in neural networks for object recognition remains an open problem. Although many attempts have been made for relational reasoning, they generally only consider a single type of relationship. For example, pixel relations through self-attention (e.g., non-local networks), scale relations through feature fusion (e.g., feature pyramid networks), or object relations through graph convolutions (e.g., reasoning-RCNN). Little attention has been given to more generalized frameworks that can reason across these relationships. In this paper, we propose a hierarchical relational reasoning framework (HR-RCNN) for object detection, which utilizes a novel graph attention module (GAM). This GAM is a concise module that enables reasoning across heterogeneous nodes by operating on the graph’s edges directly. Leveraging heterogeneous relationships, our HR-RCNN shows great improvement on COCO dataset, for both object detection and instance segmentation."
  - id: 614
    order: 165
    poster_session: 2
    session_id: 5
    title: "Class-Balanced Distillation for Long-Tailed Visual Recognition"
    authors:
      - author: "Ahmet Iscen (Google)"
      - author: "Andre Araujo (Google)"
      - author: "Boqing Gong (Google)"
      - author: "Cordelia Schmid (Google)"
    all_authors: "Ahmet Iscen, Andre Araujo, Boqing Gong and Cordelia Schmid"
    code: ""
    keywords:
      - word: "Long tailed recognition"
      - word: "dataset imbalance"
    paper: "papers/0614.pdf"
    supp: "supp/0614_supp.zip"
    abstract: "Real-world imagery is often characterized by a significant imbalance of the number of images per class, leading to long-tailed distributions. An effective and simple approach to long-tailed visual recognition is to  learn feature representations and a classifier separately, with instance and class-balanced sampling, respectively. In this work, we introduce a new  framework, by making the key observation that a feature representation learned with instance sampling is far from optimal in a long-tailed setting. Our main contribution is a new training method, referred to as Class-Balanced Distillation (CBD), that leverages knowledge distillation to enhance feature representations. CBD allows the feature representation to evolve in the second training stage, guided by the teacher learned in the first stage.  The second stage uses class-balanced sampling, in order to focus on under-represented classes. This framework can naturally accommodate the usage of multiple teachers, unlocking the information from an ensemble of models to enhance recognition capabilities. Our experiments show that the proposed technique consistently outperforms the state of the art on long-tailed recognition benchmarks such as ImageNet-LT, iNaturalist17 and iNaturalist18. "
  - id: 615
    order: 270
    poster_session: 3
    session_id: 8
    title: "Move to See Better: Self-Improving Embodied Object Detection"
    authors:
      - author: "Zhaoyuan Fang (Carnegie Mellon University)"
      - author: "Ayush Jain (Carnegie Mellon University)"
      - author: "Gabriel Sarch (Carnegie Mellon University)"
      - author: "Adam Harley (Carnegie Mellon University)"
      - author: "Katerina Fragkiadaki (Carnegie Mellon University)"
    all_authors: "Zhaoyuan Fang, Ayush Jain, Gabriel Sarch, Adam Harley and Katerina Fragkiadaki"
    code: "https://github.com/ayushjain1144/SeeingByMoving"
    keywords:
      - word: "embodied"
      - word: "object detection"
      - word: "self-supervised"
    paper: "papers/0615.pdf"
    supp: "supp/0615_supp.zip"
    abstract: "Passive methods for object detection and segmentation treat images of the same scene as individual samples, and do not exploit object permanence across multiple views. Generalization to novel or difficult viewpoints thus requires additional training with lots of annotations. In contrast, humans often recognize objects by simply moving around, to get more informative viewpoints. In this paper, we propose a method for improving object detection in testing environments, assuming nothing but an embodied agent with a pre-trained 2D object detector. Our agent collects multi-view data, generates 2D and 3D pseudo-labels, and fine-tunes its detector in a self-supervised manner. Experiments on both indoor and outdoor datasets show that (1) our method obtains high quality 2D and 3D pseudo-labels from multi-view RGB-D data; (2) fine-tuning with these pseudo-labels improves the 2D detector significantly in the test environment; (3) training a 3D detector with our pseudo-labels outperforms a prior self-supervised method by a large margin; (4) given weak supervision, our method can generate better pseudo-labels for novel objects."
  - id: 619
    order: 45
    poster_session: 1
    session_id: 2
    title: "Learning Neural Transmittance for Efficient Rendering of Reflectance Fields"
    authors:
      - author: "Mohammad Shafiei (University of California San Diego)"
      - author: "Sai Bi (Adobe Research)"
      - author: "Zhengqin Li (UC San Diego)"
      - author: "Aidas Liaudanskas (Fyusion Inc.)"
      - author: "Rodrigo J Ortiz Cayon (Fyusion)"
      - author: "Ravi Ramamoorthi (University of California San Diego)"
    all_authors: "Mohammad Shafiei, Sai Bi, Zhengqin Li, Aidas Liaudanskas, Rodrigo J Ortiz Cayon and Ravi Ramamoorthi"
    code: ""
    keywords:
      - word: "Visibility"
      - word: "Light Transport"
      - word: "Precomputation"
      - word: ""
    paper: "papers/0619.zip"
    supp: ""
    abstract: "Recently neural volumetric representations such as neural reflectance fields have been widely applied to faithfully reproduce the appearance of real-world objects and scenes under novel viewpoints and lighting conditions. However, it remains challenging and time-consuming to render such representations under complex lighting such as environment maps, which requires individual ray marching towards each single light to calculate the transmittance at every sampled point. In this paper, we propose  a novel method based on precomputed Neural Transmittance Functions  to accelerate the rendering of neural reflectance fields. Our neural transmittance functions enable us to efficiently query the transmittance at an arbitrary point in space along an arbitrary ray  without tedious ray marching, which effectively reduces the time-complexity of the rendering. 
We propose a novel formulation for the neural transmittance function, and train it jointly with the neural reflectance fields on images captured under collocated camera and light,  while enforcing monotonicity. Results on real and synthetic scenes demonstrate almost two order of magnitude speedup for renderings under environment maps with minimal accuracy loss. "
  - id: 620
    order: 327
    oral_session: 7
    poster_session: 4
    session_id: 9
    title: "Few-Shot Domain Adaptation for Low Light RAW Image Enhancement"
    authors:
      - author: "Ram K Prabhakar (Indian Institute of Science)"
      - author: "Vishal Vinod (Indian Institute of Science)"
      - author: "Nihar R. Sahoo (Indian Institute of Science)"
      - author: "Venkatesh Babu RADHAKRISHNAN (Indian Institute of Science)"
    all_authors: "Ram K Prabhakar, Vishal Vinod, Nihar R. Sahoo and Venkatesh Babu RADHAKRISHNAN"
    code: ""
    keywords:
      - word: "RAW image denoising"
      - word: "Domain Adaptation"
      - word: "CNN"
      - word: "Image denoising"
      - word: "Low level image processing"
      - word: "few shot learning"
    paper: "papers/0620.pdf"
    supp: "supp/0620_supp.zip"
    abstract: "Enhancing practical low light raw images is a difficult task due to severe noise and color distortions from short exposure time and limited illumination. Despite the success of existing Convolutional Neural Network (CNN) based methods, their performance is not adaptable to different camera domains. In addition, such methods also require large datasets with short-exposure and corresponding long-exposure ground truth raw images for each camera domain, which is tedious to compile. To address this, we present a novel few-shot domain adaptation method to utilize the existing source camera labeled data with few labeled samples from the target camera to improve the target domain's enhancement quality in extreme low-light imaging. Our experiments show that only ten or less labeled samples from the target camera domain is sufficient to achieve similar or better enhancement performance as compared to training a model with a large labeled target camera dataset. To support research in this direction, we also present a new low-light raw image dataset captured with a Nikon camera, comprising of short-exposure and their corresponding long-exposure ground truth images."
  - id: 625
    order: 46
    poster_session: 1
    session_id: 2
    title: "Intrinsic Examples: Robust Fingerprinting of Deep Neural Networks"
    authors:
      - author: "Siyue Wang (Northeastern University)"
      - author: "Pu Zhao (Northeastern University)"
      - author: "Xiao Wang (Boston University)"
      - author: "Sang Chin (Boston University)"
      - author: "Thomas Wahl (Northeastern University)"
      - author: "Yunsi Fei (NEU )"
      - author: "Qi Alfred Chen (UC Irvine)"
      - author: "Xue Lin (Northeastern University)"
    all_authors: "Siyue Wang, Pu Zhao, Xiao Wang, Sang Chin, Thomas Wahl, Yunsi Fei, Qi Alfred Chen and Xue Lin"
    code: ""
    keywords:
      - word: "DNN fingerprinting"
      - word: "DNN functionality verification"
      - word: "intrinsic examples"
      - word: "security and privacy"
      - word: "artificial intelligence"
      - word: "fault injection attack"
      - word: "model compression"
      - word: "embedded system"
    paper: "papers/0625.pdf"
    supp: "supp/0625_supp.zip"
    abstract: "This paper proposes to use intrinsic examples as a DNN fingerprinting technique for the functionality verification of DNN models implemented on edge devices. The proposed intrinsic examples do not affect the normal DNN training and can enable the black-box testing capability for DNN models packaged into edge device applications. We provide three algorithms for deriving intrinsic examples of the pre-trained model (the model before the DNN system design and implementation procedure) to retrieve the knowledge learnt from the training dataset for the detection of adversarial third-party attacks such as transfer learning and fault injection attack that may happen during the system implementation procedure. Besides, they can accommodate the model transformations due to various DNN model compression methods used by the system designer."
  - id: 628
    order: 271
    poster_session: 3
    session_id: 8
    title: "Joint Detection of Motion Boundaries and Occlusions"
    authors:
      - author: "Hannah H Kim (Duke University)"
      - author: "Shuzhi Yu (Duke University)"
      - author: "Carlo Tomasi (Duke University)"
    all_authors: "Hannah H Kim, Shuzhi Yu and Carlo Tomasi"
    code: "https://github.com/hannahhalin/MONet"
    keywords:
      - word: "motion boundary"
      - word: "occlusion"
      - word: "optical flow"
      - word: "motion estimation"
      - word: ""
    paper: "papers/0628.pdf"
    supp: ""
    abstract: "We propose MONet, a convolutional neural network that jointly detects motion boundaries (MBs) and occlusion regions (Occs) in video both forward and backward in time. Detection is difficult because optical flow is discontinuous along MBs and undefined in Occs, while many flow estimators assume smoothness and a flow defined everywhere. To reason in the two time directions simultaneously, we direct-warp the estimated maps between the two frames. Since appearance mismatches between frames often signal vicinity to MBs or Occs, we construct a cost block that for each feature in one frame records the lowest discrepancy with matching features in a search range. This cost block is two-dimensional, and much less expensive than the four-dimensional cost volumes used in flow analysis. Cost-block features are computed by an encoder, and MB and Occ estimates are computed by a decoder. We found that arranging decoder layers fine-to-coarse, rather than coarse-to-fine, improves performance. MONet outperforms or closely matches the prior state of the art for both tasks on the Sintel and FlyingChairsOcc benchmarks without any fine-tuning on them."
  - id: 631
    order: 166
    poster_session: 2
    session_id: 5
    title: "Learning to Generate Novel Classes for Deep Metric Learning"
    authors:
      - author: "Kyungmoon Lee (POSTECH)"
      - author: "Sungyeon Kim (POSTECH)"
      - author: "Seunghoon Hong (KAIST)"
      - author: "Suha Kwak (POSTECH)"
    all_authors: "Kyungmoon Lee, Sungyeon Kim, Seunghoon Hong and Suha Kwak"
    code: ""
    keywords:
      - word: "deep metric learning"
      - word: "metric learning"
      - word: "data augmentation"
      - word: "sample generation"
    paper: "papers/0631.pdf"
    supp: "supp/0631_supp.zip"
    abstract: "Deep metric learning aims to learn an embedding space where the distance between data reflects their class equivalence, even when their classes are unseen during training. However, the limited number of classes available in training precludes generalization of the learned embedding space. Motivated by this, we introduce a new data augmentation approach that synthesizes novel classes and their embedding vectors. Our approach can provide rich semantic information to an embedding model and improve its generalization by augmenting training data with novel classes unavailable in the original data. We implement this idea by learning and exploiting a conditional generative model, which, given a class label and a noise, produces a random embedding vector of the class. Our proposed generator allows the loss to use richer class relations by augmenting realistic and diverse classes, resulting in better generalization to unseen samples. Experimental results on public benchmark datasets demonstrate that our method clearly enhances the performance of proxy-based losses."
  - id: 634
    order: 167
    poster_session: 2
    session_id: 5
    title: "Unsupervised View-Invariant Human Posture Representation"
    authors:
      - author: "Faegheh Sardari (University of Bristol)"
      - author: "Bjorn Ommer (University of Munich)"
      - author: "Majid Mirmehdi (University of Bristol)"
    all_authors: "Faegheh Sardari, Bjorn Ommer and Majid Mirmehdi"
    code: "https://github.com/fsardari/U-VI"
    keywords:
      - word: "Representation Learning"
      - word: "Self-supervised Learning"
      - word: "Unsupervised 3D Pose Estimation"
      - word: "View-Invariant Pose Estimation"
      - word: "View-Invariant Action Recognition"
      - word: "View-Invariant Action Assessment"
      - word: "View-Invariant Human Movemnet Assessment"
      - word: "Human Posture Representation"
      - word: "Unsupervised Action Recognition"
      - word: "Unsupervised Action Assessment"
    paper: "papers/0634.pdf"
    supp: "supp/0634_supp.zip"
    abstract: "Most recent view-invariant action recognition and performance assessment approaches rely on a large amount of annotated 3D skeleton data to extract view-invariant features. However, acquiring skeleton data can be cumbersome, if not impractical, in in-the-wild scenarios. To overcome this problem, we present a novel unsupervised approach that learns to extract view-invariant 3D human pose representation from a 2D image without using 3D joint data.
Our model is trained by exploiting the intrinsic view-invariant properties of human pose between simultaneous frames from different viewpoints and their equivariant properties between augmented frames from the same viewpoint. We evaluate the learned view-invariant pose representations for two downstream tasks. We perform comparative experiments that show improvements on the state-of-the-art unsupervised cross-view action classification accuracy on NTU RGB+D by a significant margin, on both RGB and depth images. We also show the efficiency of transferring the learned representations from NTU RGB+D to obtain  the first ever unsupervised cross-view and cross-subject rank correlation results on the multi-view human quality of movement dataset, QMAR, and marginally improve on the-state-of-the-art supervised results for this dataset. We also carry out ablation studies to examine the contributions of the different components of our proposed network. Our code is available at https://github.com/fsardari/U-VI."
  - id: 636
    order: 168
    poster_session: 2
    session_id: 5
    title: "Self-Supervised Training Enhances Online Continual Learning"
    authors:
      - author: "Gianmarco J Gallardo (ROCHESTER INSTITUTE OF TECHNOLOGY)"
      - author: "Tyler L Hayes (RIT)"
      - author: "Christopher Kanan (Rochester Institute of Technology)"
    all_authors: "Gianmarco J Gallardo, Tyler L Hayes and Christopher Kanan"
    code: ""
    keywords:
      - word: "online continual learning"
      - word: "self-supervised learning"
      - word: "catastrophic forgetting"
      - word: "pre-training"
    paper: "papers/0636.pdf"
    supp: "supp/0636_supp.zip"
    abstract: "In continual learning, a system must incrementally learn from a non-stationary data stream without catastrophic forgetting. Recently, multiple methods have been devised for incrementally learning classes on large-scale image classification tasks, such as ImageNet. State-of-the-art continual learning methods use an initial supervised pre-training phase, in which the first 10% - 50% of the classes in a dataset are used to learn representations in an offline manner before continual learning of new classes begins. We hypothesize that self-supervised pre-training could yield features that generalize better than supervised learning, especially when the number of samples used for pre-training is small. We test this hypothesis using the self-supervised MoCo-V2, Barlow Twins, and SwAV algorithms. On ImageNet, we find that these methods outperform supervised pre-training considerably for online continual learning, and the gains are larger when fewer samples are available. Our findings are consistent across three online continual learning algorithms. Our best system achieves a 14.95% relative increase in top-1 accuracy on class incremental ImageNet over the prior state of the art for online continual learning."
  - id: 637
    order: 47
    poster_session: 1
    session_id: 2
    title: "Edge-aware Bidirectional Diffusion for Dense Depth Estimation from Light Fields"
    authors:
      - author: "Numair Khan (Brown University)"
      - author: "Min H. Kim (KAIST)"
      - author: "James Tompkin (Brown University)"
    all_authors: "Numair Khan, Min H. Kim and James Tompkin"
    code: "https://visual.cs.brown.edu/lightfielddepth/"
    keywords:
      - word: "light fields"
      - word: "lightfields"
      - word: "depth estimation"
      - word: "dense depth"
      - word: "diffusion"
      - word: "image editing"
      - word: "occlusion edges"
      - word: "edge aware"
    paper: "papers/0637.pdf"
    supp: "supp/0637_supp.zip"
    abstract: "We present an algorithm for estimating fast and accurate depth maps from light fields that recovers a complete description of depth via a sparse set of depth edges and gradients. Our proposed approach is based around the idea that true depth edges are more sensitive than texture edges to local constraints, and so they can be reliably disambiguated through a bidirectional diffusion process. First, we use epipolar-plane images to estimate sub-pixel disparity at a sparse set of pixels. To find sparse points efficiently, we propose an entropy-based refinement approach to a line estimate from a limited set of oriented filter banks. Next, to estimate the diffusion direction away from sparse points, we optimize constraints at these points via our bidirectional diffusion method. This resolves the ambiguity of which surface the edge belongs to and reliably separates depth from texture edges, allowing us to diffuse the sparse set in a depth-edge and occlusion-aware manner to obtain accurate dense depth maps. We demonstrate the utility of our approach for light field editing tasks. https://visual.cs.brown.edu/lightfielddepth/"
  - id: 638
    order: 371
    poster_session: 4
    session_id: 11
    title: "GaussiGAN: Controllable Image Synthesis with 3D Gaussians from Unposed Silhouettes"
    authors:
      - author: "Youssef Alami Mejjati (University of Bath)"
      - author: "Isa Milefchik (Brown University)"
      - author: "Aaron K Gokaslan (Cornell)"
      - author: "Oliver Wang (Adobe Systems Inc)"
      - author: "Kwang In Kim (UNIST)"
      - author: "James Tompkin (Brown University)"
    all_authors: "Youssef Alami Mejjati, Isa Milefchik, Aaron K Gokaslan, Oliver Wang, Kwang In Kim and James Tompkin"
    code: "https://visual.cs.brown.edu/gaussigan"
    keywords:
      - word: "structured representation"
      - word: "3D representation"
      - word: "3D Gaussians"
      - word: "image generation"
      - word: "image synthesis"
      - word: "image editing"
      - word: "controlled generation"
      - word: "GANs"
      - word: ""
    paper: "papers/0638.pdf"
    supp: "supp/0638_supp.zip"
    abstract: "We present an algorithm that learns a coarse 3D representation of objects from unposed multi-view 2D mask supervision, then uses it to generate detailed mask and image texture. In contrast to existing voxel-based methods for unposed object reconstruction, our approach learns to represent the generated shape and pose with a set of self-supervised canonical 3D anisotropic Gaussians via a perspective camera, and a set of per-instance transforms. We show that this approach can robustly estimate a 3D space for the camera and object, while recent baselines sometimes struggle to reconstruct coherent 3D spaces in this setting. We show results on synthetic datasets with realistic lighting, and demonstrate object insertion with interactive posing. With our work, we help move towards structured representations that handle more real-world variation in learning-based object reconstruction. https://visual.cs.brown.edu/gaussigan"
  - id: 642
    order: 48
    poster_session: 1
    session_id: 2
    title: "ZeBRA: Precisely Destroying Neural Networks with Zero-Data Based Repeated Bit Flip Attack"
    authors:
      - author: "Dahoon Park (Hongik University)"
      - author: "Kon-Woo  Kwon (Hongik University)"
      - author: "Sunghoon Im (DGIST)"
      - author: "Jaeha Kung (DGIST)"
    all_authors: "Dahoon Park, Kon-Woo  Kwon, Sunghoon Im and Jaeha Kung"
    code: "https://github.com/pdh930105/ZeBRA"
    keywords:
      - word: "deep learning"
      - word: "deep learning security"
      - word: "adversarial attack"
      - word: "adversarial weight attack"
      - word: "data-free deep learning"
      - word: "row hammering"
      - word: "quantization"
      - word: ""
    paper: "papers/0642.pdf"
    supp: "supp/0642_supp.zip"
    abstract: "In this paper, we present Zero-data Based Repeated bit flip Attack (ZeBRA) that precisely destroys deep neural networks (DNNs) by synthesizing its own attack datasets. Many prior works on adversarial weight attack require not only the weight parameters, but also the training or test dataset in searching vulnerable bits to be attacked. We propose to synthesize the attack dataset, named distilled target data, by utilizing the statistics of batch normalization layers in the victim DNN model. Equipped with the distilled target data, our ZeBRA algorithm can search vulnerable bits in the model without accessing training or test dataset. Thus, our approach makes the adversarial weight attack more fatal to the security of DNNs. Our experimental results show that 2.0x (CIFAR-10) and 1.6x (ImageNet) less number of bit flips are required on average to destroy DNNs compared to the previous attack method. Our code is available at https://github.com/pdh930105/ZeBRA."
  - id: 643
    order: 372
    poster_session: 4
    session_id: 11
    title: "360-Degree Gaze Estimation in the Wild Using Multiple Zoom Scales"
    authors:
      - author: "Ashesh Ashesh (NTU)"
      - author: "Chu-Song Chen (National Taiwan University)"
      - author: "Hsuan-Tien Lin (National Taiwan University)"
    all_authors: "Ashesh Ashesh, Chu-Song Chen and Hsuan-Tien Lin"
    code: "https://github.com/ashesh-0/MultiZoomGaze"
    keywords:
      - word: "3D Gaze estimation"
      - word: "Gaze estimation"
      - word: "Gaze estimation in the wild"
      - word: "multi scale"
      - word: "360 degree gaze estimation"
      - word: ""
    paper: "papers/0643.pdf"
    supp: "supp/0643_supp.zip"
    abstract: "Gaze estimation involves predicting where the person is looking at within an image or video. Technically, the gaze information can be inferred from two different magnification levels: face orientation and eye orientation. The inference is not always feasible for gaze estimation in the wild, given the lack of clear eye patches in conditions like extreme left/right gazes or occlusions. In this work, we design a model that mimics humans' ability to estimate the gaze by aggregating from focused looks, each at a different magnification level of the face area. The model avoids the need to extract clear eye patches and at the same time addresses another important issue of face-scale variation for gaze estimation in the wild. We further extend the model to handle the challenging task of 360-degree gaze estimation by encoding the backward gazes in the polar representation along with a robust averaging scheme. Experiment results on the ETH-XGaze dataset, which does not contain scale-varying faces, demonstrate the model's effectiveness to assimilate information from multiple scales. For other benchmark datasets with many scale-varying faces (Gaze360 and RT-GENE), the proposed model achieves state-of-the-art performance for gaze estimation when using either images or videos. Our code and pretrained models can be accessed at https://github.com/ashesh-0/MultiZoomGaze."
  - id: 646
    order: 272
    poster_session: 3
    session_id: 8
    title: "Boosting Adversarial Transferability through Enhanced Momentum"
    authors:
      - author: "Xiaosen Wang (Huazhong University of Science and Technology)"
      - author: "Jiadong Lin (Huazhong University of Science and Technology)"
      - author: "Han Hu (Microsoft Research Asia)"
      - author: "Jingdong Wang (Baidu)"
      - author: "Kun   He (Huazhong University of Science and Technology)"
    all_authors: "Xiaosen Wang, Jiadong Lin, Han Hu, Jingdong Wang and Kun   He"
    code: "https://github.com/JHL-HUST/EMI"
    keywords:
      - word: "adversarial transferability"
      - word: "adversarial attack"
      - word: "adversarial examples"
      - word: "optimization"
    paper: "papers/0646.pdf"
    supp: "supp/0646_supp.zip"
    abstract: "Deep learning models are known to be vulnerable to adversarial examples crafted by adding human-imperceptible perturbations on benign images. Many existing adversarial attacks have achieved great white-box attack performance, but exhibit low transferability when attacking other models. Various momentum iterative gradient-based methods are shown to be effective to improve the adversarial transferability. In what follows, we propose an enhanced momentum iterative gradient-based method to further enhance the adversarial transferability. Specifically, instead of only accumulating the gradient during the iterative process, we additionally accumulate the average gradient of the data points sampled in the gradient direction of the previous iteration so as to stabilize the update direction and escape from poor local maxima. Extensive experiments on the standard ImageNet dataset demonstrate that our method could improve the adversarial transferability of momentum-based methods by a large margin of 11.1% on average. Moreover, by incorporating with various input transformations, the adversarial transferability could be further improved significantly. We also attack several extra advanced defense models in the ensemble-model setting, and the enhancements are at least 7.8% on average."
  - id: 651
    order: 273
    poster_session: 3
    session_id: 8
    title: "Deep Video Decaptioning"
    authors:
      - author: "Pengpeng Chu (Donghua University)"
      - author: "Weize Quan (NLPR, Institute of Automation, Chinese Academy of Sciences)"
      - author: "Tong Wang (Donghua University)"
      - author: "Pan Wang ( Ali baba)"
      - author: "Peiran Ren (Alibaba )"
      - author: "Dong-Ming Yan (NLPR, Institute of Automation, Chinese Academy of Sciences)"
    all_authors: "Pengpeng Chu, Weize Quan, Tong Wang, Pan Wang, Peiran Ren and Dong-Ming Yan"
    code: "https://github.com/Linya-lab/Video_Decaptioning"
    keywords:
      - word: "video decaptioning"
      - word: "caption mask extraction"
      - word: "frame attention"
      - word: "real time"
      - word: ""
    paper: "papers/0651.pdf"
    supp: "supp/0651_supp.zip"
    abstract: "Video decaptioning aims to remove subtitles from and repair occluded areas in videos. However, recent deep-learning-based inpainting methods mostly require the masks indicating the corrupted parts, and these masks are unavailable for the input subtitled videos. Moreover, useful information hidden in the background of subtitles might be lost when these masked areas are directly regarded as invalid as the common setting of inpainting methods. In addition, existing blind video decaptioning methods often suffer from incomplete subtitles removal. In this paper, we propose a generic framework for video decaptioning, which consists of a caption mask extraction network and a frame-attention-based decaptioning network. The former is trained with supervision information using our proposed automatic annotation method, and predicts the position of the subtitle and background. The latter adopts an encoder-decoder architecture with the skip connection. The encoder extracts the features of all input frames. Then, multiple frame attention modules are used to aggregate these features from the spatial and temporal dimensions. Finally, the fused features are reconstructed into a target frame using the decoder. Extensive experiments demonstrate that our proposed method can accurately remove subtitles from videos in real time (60+ FPS), and outperforms the state-of-the-art approaches. Code is available at https://github.com/Linya-lab/Video_Decaptioning."
  - id: 673
    order: 169
    poster_session: 2
    session_id: 5
    title: "SemGIF: A Semantics Guided Incremental Few-shot Learning Framework with Generative Replay"
    authors:
      - author: "S Divakar Bhat (Indian Institute of Technology, Bombay)"
      - author: "Biplab Banerjee (Indian Institute of Technology Bombay)"
      - author: "Subhasis Chaudhuri (Indian Institute of Technology Bombay)"
    all_authors: "S Divakar Bhat, Biplab Banerjee and Subhasis Chaudhuri"
    code: ""
    keywords:
      - word: "Incremental few shot learning"
      - word: "Few shot learning"
      - word: "Incremental learning"
      - word: "feature augmentation"
      - word: "cross dataset"
      - word: "heterogenous"
    paper: "papers/0673.pdf"
    supp: ""
    abstract: "We address the problem of incremental few-shot learning (IFSL) by leveraging the notion of generative feature replay. Learning novel concepts while preserving old knowledge is a long-lasting challenge in machine learning. The main concern in IFSL is to combat the catastrophic forgetting of the base classes whose training data are not available during the incremental stage while ensuring good generalization for the few-shot classes. Existing techniques prefer to preserve some base class samples to tackle forgetting, which does not comply with the intention of incremental learning.
To this end, we propose a novel framework called Semantics Guided IFSL (SemGIF), which trains a generative model to synthesize base class samples on demand during the incremental step. Considering the importance of modeling a discriminative feature space in IFSL for separating the base and the novel classes, we propose a feature augmentation strategy where the visual embeddings are supplemented with the semantic features obtained from a word-embedding space. Such a feature space is found to produce enriched class prototypes to be utilized during classification. Experimental results on CIFAR-100, CUB, mini-ImageNet, and tiered-ImageNet in the homogeneous (within-dataset) and a novel heterogeneous (cross-dataset) setup showcase sharp improvements than the literature."
  - id: 675
    order: 274
    poster_session: 3
    session_id: 8
    title: "Paying Attention to Varying Receptive Fields: Object Detection with Atrous Filters and Vision Transformers"
    authors:
      - author: "Arthur Jian Shun Lam (Monash University)"
      - author: "Jun Yi Lim (Monash University)"
      - author: "Ricky Sutopo (Monash University)"
      - author: "Vishnu Monn Baskaran (Monash University)"
    all_authors: "Arthur Jian Shun Lam, Jun Yi Lim, Ricky Sutopo and Vishnu Monn Baskaran"
    code: ""
    keywords:
      - word: "object detection"
      - word: "atrous convolution"
      - word: "vision transformers"
      - word: "attention mechanism"
      - word: ""
    paper: "papers/0675.pdf"
    supp: "supp/0675_supp.zip"
    abstract: "Object detection represents a critical component in computer vision based on its unique ability to identify the location of one or more objects in an image or video. Given its importance, various approaches were proposed in an attempt to extract meaningful and representative features across different image scales. One such approach would be to vary the receptive fields during the feature extraction process. However, varying and adjusting the receptive field adds complexity to the process of scene understanding by introducing a higher degree of unimportant semantics into the feature maps. To solve this problem, we propose a novel object detection framework by unifying dilation modules (or atrous convolutions) with a vision transformer (DIL-ViT). The proposed model leverages atrous convolutions to generate rich multi-scale feature maps and employs a self-attention mechanism to enrich important backbone features. Specifically, the dilation (i.e., DIL) module enables feature fusions across varying scales from a single input feature map of specific scales. Through this method, we incorporate coarse semantics and fine details into the feature maps by convolving the features with different atrous rates in a multi-branch multi-level structure. By embedding DIL into various object detectors, we observe notable improvements in all of the compared evaluation metrics using the MS-COCO dataset. To further enhance the feature maps produced by the DIL, we then apply channel-wise attention using a vision transformer (i.e., ViT). Crucially, this approach removes unnecessary semantics present in the fused multi-scale feature map. Experimental results of DIL-ViT on the MS-COCO dataset exhibit substantial improvements in all of the compared evaluation metrics."
  - id: 678
    order: 49
    poster_session: 1
    session_id: 2
    title: "Enhancing Local Feature Learning for 3D Point Cloud Processing using Unary-Pairwise Attention"
    authors:
      - author: "Haoyi Xiu (National Institute of Advanced Industrial Science and Technology)"
      - author: "Xin Liu (National Institute of Advanced Industrial Science and Technology (AIST))"
      - author: "Weimin Wang (Dalian University of Technology / National Institute of Advanced Industrial Science and Technology (AIST))"
      - author: "Kyoung-Sook Kim (Artificial Intelligence Research Center)"
      - author: "Takayuki Shinohara (Tokyo institute of technology)"
      - author: "Qiong Chang (Tokyo Institute of Technology)"
      - author: "Masashi Matsuoka (Tokyo Institute of Technology)"
    all_authors: "Haoyi Xiu, Xin Liu, Weimin Wang, Kyoung-Sook Kim, Takayuki Shinohara, Qiong Chang and Masashi Matsuoka"
    code: ""
    keywords:
      - word: "3D point clouds"
      - word: "self-attention"
      - word: "3D point cloud deep learning"
    paper: "papers/0678.pdf"
    supp: "supp/0678_supp.zip"
    abstract: "We present a simple but effective attention named the unary-pairwise attention (UPA) for modeling the relationship between 3D point clouds. Our idea is motivated by the analysis that the standard self-attention (SA) that operates globally tends to produce almost the same attention maps for different query positions, revealing difficulties for learning query-independent and query-dependent information jointly. Therefore, we reformulate the SA and propose query-independent (Unary) and query-dependent (Pairwise) components to facilitate the learning of both terms. In contrast to the SA, the UPA ensures query dependence via operating locally. Extensive experiments show that the UPA outperforms the SA consistently on various point cloud understanding tasks including shape classification, part segmentation, and scene segmentation. Moreover, simply equipping the popular PointNet++ method with the UPA even outperforms or is on par with the state-of-the-art attention-based approaches. In addition, the UPA systematically boosts the performance of both standard and modern networks when it is integrated into them as a compositional module."
  - id: 684
    order: 275
    poster_session: 3
    session_id: 8
    title: "CAFENet: Class-Agnostic Few-Shot Edge Detection Network"
    authors:
      - author: "YoungHyun Park (KAIST)"
      - author: "Jun Seo (Korea Advanced Institute of Science and Technology(KAIST))"
      - author: "Jaekyun Moon (Korea Advanced Institute of Science and Technology)"
    all_authors: "YoungHyun Park, Jun Seo and Jaekyun Moon"
    code: ""
    keywords:
      - word: "few-shot edge detection"
      - word: "few-shot learning"
      - word: "edge detection"
      - word: "meta learning"
    paper: "papers/0684.pdf"
    supp: "supp/0684_supp.zip"
    abstract: "We tackle a novel few-shot learning challenge, few-shot semantic edge detection, aiming to localize boundaries of novel categories using only a few labeled samples. Reliable boundary information has been shown to boost the performance of semantic segmentation and localization, while also playing a key role in its own right in object reconstruction, image generation and medical imaging. However, existing semantic edge detection techniques require a large amount of labeled data to train a model. To overcome this limitation, we present Class-Agnostic Few-shot Edge detection Network (CAFENet) based on a meta-learning strategy. CAFENet employs a semantic segmentation module in small-scale to compensate for the lack of semantic information in edge labels. To effectively fuse the semantic information and low-level cues, CAFENet also utilizes an attention module which dynamically generates multi-scale attention map, as well as a novel regularization method that splits high-dimensional features into several low-dimensional features and conducts multiple metric learning. Since there are no existing datasets for few-shot semantic edge detection, we construct two new datasets, FSE-1000 and SBD-5i, and evaluate the performance of the proposed CAFENet on them. Extensive simulation results confirm that CAFENet achieves better performance compared to the baseline methods using fine-tuning or few-shot segmentation."
  - id: 685
    order: 170
    poster_session: 2
    session_id: 5
    title: "Feature Fusion Vision Transformer for Fine-Grained Visual Categorization"
    authors:
      - author: "Jun Wang (University of Warwick)"
      - author: "Xiaohan Yu (Griffith University)"
      - author: "Yongsheng Gao (Griffith University)"
    all_authors: "Jun Wang, Xiaohan Yu and Yongsheng Gao"
    code: "https://github.com/Markin-Wang/FFVT"
    keywords:
      - word: "Fine-grained visual categorization"
      - word: "Vision transformer"
      - word: "Self-attention"
      - word: "Feature Fusion"
      - word: ""
    paper: "papers/0685.pdf"
    supp: ""
    abstract: "The core for tackling the fine-grained visual categorization (FGVC) is to learn subtle yet discriminative features. Most previous works achieve this by explicitly selecting the discriminative parts or integrating the attention mechanism via CNN-based approaches. However, these methods enhance the computational complexity and make the model dominated by the regions containing the most of the objects. Recently, vision trans- former (ViT) has achieved SOTA performance on general image recognition tasks. The self-attention mechanism aggregates and weights the information from all patches to the classification token, making it perfectly suitable for FGVC. Nonetheless, the classifi- cation token in the deep layer pays more attention to the global information, lacking the local and low-level features that are essential for FGVC. In this work, we propose a novel pure transformer-based framework Feature Fusion Vision Transformer (FFVT) where we aggregate the important tokens from each transformer layer to compensate the local, low-level and middle-level information. We design a novel token selection mod- ule called mutual attention weight selection (MAWS) to guide the network effectively and efficiently towards selecting discriminative tokens without introducing extra param- eters. We verify the effectiveness of FFVT on four benchmarks where FFVT achieves the state-of-the-art performance. Code is available at this link."
  - id: 686
    order: 373
    poster_session: 4
    session_id: 11
    title: "DEX: Domain Embedding Expansion for Generalized Person Re-identification"
    authors:
      - author: "Phuay Wee Eugene Ang (Nanyang Technological University)"
      - author: "Shan Lin (Nanyang Technological University)"
      - author: "Alex Kot (Nanyang Technological University)"
    all_authors: "Phuay Wee Eugene Ang, Shan Lin and Alex Kot"
    code: ""
    keywords:
      - word: "person re-identification"
      - word: "domain generalization"
      - word: "data augmentation"
    paper: "papers/0686.pdf"
    supp: "supp/0686_supp.zip"
    abstract: "In recent years, supervised Person Re-identification (Person ReID) approaches have demonstrated excellent performance. However, when these methods are applied to inputs from a different camera network, they typically suffer from significant performance degradation. Different from most domain adaptation (DA) approaches addressing this issue, we focus on developing a domain generalization (DG) Person ReID model that can be deployed without additional fine-tuning or adaptation. In this paper, we propose the Domain Embedding Expansion (DEX) module. DEX dynamically manipulates and augments deep features based on person and domain labels during training, significantly improving the generalization capability and robustness of Person ReID models to unseen domains. We also developed a light version of DEX (DEXLite), applying negative sampling techniques to scale to larger datasets and reduce memory usage for multi-branch networks. Our proposed DEX and DEXLite can be combined with many existing methods, Bag-of-Tricks (BagTricks), the Multi-Granularity Network (MGN), and Part-Based Convolutional Baseline (PCB), in a plug-and-play manner. With DEX and DEXLite, existing methods can gain significant improvements when tested on other unseen datasets, thereby demonstrating the general applicability of our method. Our solution outperforms the state-of-the-art DG Person ReID methods in all large-scale benchmarks as well as in most the small-scale benchmarks."
  - id: 690
    order: 374
    poster_session: 4
    session_id: 11
    title: "Mitigating Mode Collapse by Sidestepping Catastrophic Forgetting"
    authors:
      - author: "Karttikeya Mangalam (UC Berkeley)"
      - author: "Rohin Garg (IIT Kanpur)"
    all_authors: "Karttikeya Mangalam and Rohin Garg"
    code: "https://github.com/gargrohin/AMAT"
    keywords:
      - word: "Generative Adversarial Learning"
      - word: "GAN"
      - word: "Adaptive Learning"
      - word: "Representation Learning"
      - word: "Image Generation"
      - word: "Image Synthesis"
      - word: "Catastrophic Forgetting"
      - word: "Mode Collapse"
      - word: "GAN Learning"
      - word: ""
    paper: "papers/0690.pdf"
    supp: "supp/0690_supp.zip"
    abstract: "Generative Adversarial Networks (GANs) are a class of generative models used for various applications, but they have been known to suffer from the mode collapse problem, in which some modes of the target distribution are ignored by the generator. Investigative study using a new data generation procedure indicates that the mode collapse of the generator is driven by the discriminator's inability to maintain classification accuracy on previously seen samples, a phenomenon called Catastrophic Forgetting in continual learning. Motivated by this observation, we introduce a novel training procedure that adaptively spawns additional discriminators to remember previous modes of generation. On several datasets, we show that our training scheme can be plugged-in to existing GAN frameworks to mitigate mode collapse and improve standard metrics for GAN evaluation. "
  - id: 691
    order: 375
    poster_session: 4
    session_id: 11
    title: "Privacy Preserving for Medical Image Analysis via Non-Linear Deformation Proxy"
    authors:
      - author: "Bach Kim (ETS Montreal)"
      - author: "Jose Dolz (ETS Montreal)"
      - author: "Christian Desrosiers (ETS, Canada)"
      - author: "Pierre-Marc Jodoin (Universite de Sherbrooke)"
    all_authors: "Bach Kim, Jose Dolz, Christian Desrosiers and Pierre-Marc Jodoin"
    code: ""
    keywords:
      - word: "privacy preserving"
      - word: "medical image"
      - word: "segmentation"
      - word: "neural network"
      - word: "adversarial learning"
      - word: "deformation"
      - word: "flow-field"
    paper: "papers/0691.pdf"
    supp: "supp/0691_supp.zip"
    abstract: "We propose a client-server system which allows for the analysis of multi-centric medical images while preserving patient identity. In our approach, the client protects the patient identity by applying a pseudo-random non-linear deformation to the input image. This results into a proxy image which is sent to the server for processing. The server then returns back the deformed processed image which the client reverts to a canonical form. Our system has three components: 1) a flow-field generator which produces a pseudo-random deformation function, 2) a Siamese discriminator that learns the patient identity from the processed image, 3) a medical image processing network that analyzes the content of the proxy images. The system is trained end-to-end in an adversarial manner. By fooling the discriminator, the flow-field generator learns to produce a bi-directional non-linear deformation which allows to remove and recover the identity of the subject from both the input image and output result. After end-to-end training, the flow-field generator is deployed on the client side and the segmentation network is deployed on the server side.
The proposed method is validated on the task of MRI brain segmentation using images from two different datasets. Results show that the segmentation accuracy of our method is similar to a system trained on non-encoded images, while considerably reducing the ability to recover subject identity."
  - id: 692
    order: 376
    poster_session: 4
    session_id: 11
    title: "Depth-aware Object Segmentation and Grasp Detection for Robotic Picking Tasks"
    authors:
      - author: "Stefan Ainetter (Graz University of Technology)"
      - author: "Christoph Böhm (Universität Klagenfurt)"
      - author: "Rohit Dhakate (Control of Networked Systems / AAU)"
      - author: "Stephan Weiss ()"
      - author: "Friedrich Fraundorfer (Graz University of Technology)"
    all_authors: "Stefan Ainetter, Christoph Böhm, Rohit Dhakate, Stephan Weiss and Friedrich Fraundorfer"
    code: ""
    keywords:
      - word: "Robotic picking"
      - word: "grasp detection"
      - word: "depth-aware object segmentation"
      - word: ""
    paper: "papers/0692.pdf"
    supp: "supp/0692_supp.zip"
    abstract: "In this paper, we present a novel deep neural network architecture for joint class-agnostic object segmentation and grasp detection for robotic picking tasks using a parallel-plate gripper. We introduce depth-aware Coordinate Convolution (CoordConv), a method to increase accuracy for point proposal based object instance segmentation in complex scenes without adding any additional network parameters or computation complexity. Depth-aware CoordConv uses depth data to extract prior information about the location of an object to achieve highly accurate object instance segmentation. These resulting segmentation masks, combined with predicted grasp candidates, lead to a complete scene description for grasping using a parallel-plate gripper. We evaluate the accuracy of grasp detection and instance segmentation on challenging robotic picking datasets, namely Siléane and OCID_grasp, and show the benefit of joint grasp detection and segmentation on a real-world robotic picking task."
  - id: 694
    order: 276
    poster_session: 3
    session_id: 8
    title: "Re-ID-AR: Improved Person Re-identification in Videovia Joint Weakly Supervised Action Recognition"
    authors:
      - author: "Aishah Alsehaim (Durham university)"
      - author: "Toby P Breckon (Durham University)"
    all_authors: "Aishah Alsehaim and Toby P Breckon"
    code: ""
    keywords:
      - word: "person Re-ID"
    paper: "papers/0694.pdf"
    supp: "supp/0694_supp.zip"
    abstract: "We uniquely consider the task of joint person re-identification (Re-ID) and action recognition in video as a multi-task problem. In addition to the broader potential of joint Re-ID and action recognition within the context of automated multi-camera surveillance, we show that the consideration of action recognition in addition to Re-ID results in a model that learns discriminative feature representations that both improve Re-ID performance and are capable of providing viable per-view (clip-wise) action recognition. Our approach uses a single 2D Convolutional Neural Network (CNN) architecture comprising a common ResNet50-IBN backbone CNN architecture, to extract frame-level features with subsequent temporal attention for clip level feature extraction, followed by two sub-branches:- the IDentification (sub-)Network (IDN) for person Re-ID and the Action Recognition (sub-)Network for per-view action recognition. The IDN comprises a single fully connected layer while the ARN comprises multiple attention blocks on a one-to-one ratio with the number of actions to be recognised. This is subsequently trained as a joint Re-ID and action recognition task using a combination of two task-specific, multi-loss terms via weakly labelled actions obtained over two leading benchmark Re-ID datasets (MARS, LPW). Our consideration of  Re-ID and action recognition as a multi-task problem results in a multi-branch 2D CNN architecture that outperforms prior work in the field (rank-1 (mAP) – MARS: $93.21% (87.23%)$, LPW: $78.15%$) without any reliance  3D convolutions  or  multi-stream  networks  architectures as found in other contemporary work. Our work represents the first benchmark performance for such a joint Re-ID and action recognition video understanding task, hitherto unapproached in the literature, and is accompanied by a new public dataset of supplementary action labels for the seminal MARS and LPW Re-ID datasets."
  - id: 697
    order: 50
    poster_session: 1
    session_id: 2
    title: "Deep Image Matting with Flexible Guidance Input"
    authors:
      - author: "Hang Cheng (Shanghai University)"
      - author: "Shugong Xu (Shanghai University)"
      - author: "Xiufeng Jiang (Shanghai University)"
      - author: "Rongrong Wang (Shanghai University)"
    all_authors: "Hang Cheng, Shugong Xu, Xiufeng Jiang and Rongrong Wang"
    code: "https://github.com/Charch-630/FGI-Matting"
    keywords:
      - word: "matting"
      - word: "data augmentation"
      - word: "guidance information"
      - word: "trimap-free"
      - word: ""
    paper: "papers/0697.pdf"
    supp: ""
    abstract: "Image matting is an important computer vision problem. Many existing matting methods require a hand-made trimap to provide auxiliary information, which is very expensive and limits the real world usage. Recently, some trimap-free methods have been proposed, which completely get rid of any user input. However, their performance lag far behind trimap-based methods due to the lack of semantic guidance. In this paper, we propose a matting method that use Flexible Guidance Input as user hint, which means our method can use trimap, scribblemap or clickmap as guidance information or even work without any guidance input. To achieve this, we propose Progressive Trimap Deformation(PTD) scheme that gradually shrink the area of the foreground and background of the trimap with the training step increases and finally become a scribblemap. To make our network robust to any user scribble and click, we randomly sample points on foreground and background and perform curve fitting. Moreover, we propose Semantic Fusion Module(SFM) which utilize the Feature Pyramid Enhancement Module(FPEM) and Joint Pyramid Upsampling(JPU) modules in matting task for the first time. The experiments show that our method can achieve state-of-the-art results comparing with existing trimap-based and trimap-free methods."
  - id: 698
    order: 377
    poster_session: 4
    session_id: 11
    title: "Ray-ONet: Efficient 3D Reconstruction From A Single RGB Image"
    authors:
      - author: "Wenjing Bian (University of Oxford)"
      - author: "Zirui Wang (University of Oxford)"
      - author: "Kejie Li (University of Oxford)"
      - author: "Victor Adrian Prisacariu (University of Oxford)"
    all_authors: "Wenjing Bian, Zirui Wang, Kejie Li and Victor Adrian Prisacariu"
    code: "https://github.com/ActiveVisionLab/ray-onet/"
    keywords:
      - word: "3D reconstruction"
      - word: "shape representation"
      - word: "single-view reconstruction"
      - word: "occupancy networks"
      - word: "3D deep learning"
    paper: "papers/0698.pdf"
    supp: "supp/0698_supp.zip"
    abstract: "We propose Ray-ONet to reconstruct detailed 3D models from monocular images efficiently.  By predicting a series of occupancy probabilities along a ray that is back-projected from a pixel in the camera coordinate, our method Ray-ONet improves the reconstruction accuracy in comparison with Occupancy Networks (ONet), while reducing the network inference complexity to O(N^2).  As a result, Ray-ONet achieves state-of-the-art performance on the ShapeNet benchmark with more than 20×speed-up at 128^3 resolution and maintains a similar memory footprint during inference."
  - id: 699
    order: 171
    poster_session: 2
    session_id: 5
    title: "Multi-Source Domain Adaptation via supervised contrastive learning and confident consistency regularization"
    authors:
      - author: "Marin Scalbert (CentraleSupélec)"
      - author: "Florent Couzinié-Devy (VitaDX international)"
      - author: "Maria Vakalopoulou (CentraleSupélec)"
    all_authors: "Marin Scalbert, Florent Couzinié-Devy and Maria Vakalopoulou"
    code: "https://gitlab.com/vitadx/articles/cmsda"
    keywords:
      - word: "unsupervised domain adaptation"
      - word: "contrastive learning"
      - word: "semi-supervised learning"
      - word: "consistency regularization"
      - word: "domain shift"
    paper: "papers/0699.pdf"
    supp: "supp/0699_supp.zip"
    abstract: "Multi-Source Unsupervised Domain Adaptation (multi-source UDA) aims to learn a model from several labeled source domains while performing well on a different target domain where only unlabeled data are available at training time.
To align source and target features distributions, several recent works use source and target explicit statistics matching such as features moments or class centroids. Yet, these approaches do not guarantee class conditional distributions alignment across domains. 
In this work, we propose a new framework called Contrastive Multi-Source Domain Adaptation (CMSDA) for multi-source UDA that addresses this limitation. Discriminative features are learned from interpolated source examples via cross entropy minimization and from target examples via consistency regularization and hard pseudo-labeling. Simultaneously, interpolated source examples are leveraged to align source class conditional distributions through an interpolated version of the supervised contrastive loss. This alignment leads to more general and transferable features which further improve the generalization on the target domain. Extensive experiments have been carried out on three standard multi-source UDA datasets where our method reports state of the art results."
  - id: 701
    order: 277
    poster_session: 3
    session_id: 8
    title: "TICaM: A Time-of-flight In-car Cabin Monitoring Dataset"
    authors:
      - author: "Jigyasa S Katrolia (DFKI)"
      - author: "Ahmed ElSherif (DFKI)"
      - author: "Hartmut Feld (DFKI)"
      - author: "Bruno Mirbach (DFKI)"
      - author: "Jason Rambach (DFKI)"
      - author: "Didier Stricker (DFKI)"
    all_authors: "Jigyasa S Katrolia, Ahmed ElSherif, Hartmut Feld, Bruno Mirbach, Jason Rambach and Didier Stricker"
    code: ""
    keywords:
      - word: "driving dataset"
      - word: "depth images"
      - word: "multi-modal data"
      - word: "multi-modal annotations"
    paper: "papers/0701.pdf"
    supp: "supp/0701_supp.zip"
    abstract: "We present TICaM, a Time-of-flight In-car Cabin Monitoring dataset for vehicle interior monitoring using a single wide-angle depth camera. Our dataset goes beyond currently available in-car cabin datasets in terms of the ambit of labeled classes, recorded scenarios and annotations provided; all at the same time. We recorded an exhaustive list of actions performed while driving and provide for them multi-modal labeled images (depth, RGB and IR), with complete annotations for 2D and 3D object detection, instance and semantic segmentation as well as activity annotations for RGB frames. Additional to real recordings, we provide a synthetic dataset of in-car cabin images with the same multi-modality of images and annotations, contributing a unique and extremely beneficial combination of synthetic and real data for effectively training cabin monitoring systems and also evaluating domain adaptation approaches. We provide baseline evaluation for object detection, segmentation and transfer learning tasks on our dataset. The dataset is made publicly available."
  - id: 702
    order: 172
    poster_session: 2
    session_id: 5
    title: "Fine-grained Multi-Modal Self-Supervised Learning"
    authors:
      - author: "Duo Wang (University of Cambridge)"
      - author: "Salah Karout (Huawei)"
    all_authors: "Duo Wang and Salah Karout"
    code: ""
    keywords:
      - word: "self-supervised learning"
      - word: "multi-modal learning"
    paper: "papers/0702.pdf"
    supp: "supp/0702_supp.zip"
    abstract: "Multi-Modal Self-Supervised Learning from videos have been shown to improve model's performance on various downstream tasks. However, such Self-Supervised pre-training requires large batch sizes and large amount of computation resources due to the noise present in the uncurated data. This is partly due to that the prevalent training scheme are trained on coarse-grained setting, in which vectors representing the whole video clips or natural language sentences are used for computing similarity. Such scheme makes training noisy as part of the video clips can be totally not correlated with the other-modality input such as text description. In this paper, we propose a fine-grained Multi-Modal Self-Supervised training scheme that computes similarity between embeddings at finer-scale (such as individual feature map embeddings and embeddings of phrases),and uses attention mechanism to reduce noisy pairs' weighting in the loss function. We show that with the proposed pre-training scheme, we can train smaller models, with smaller batch-size and much less computational resource to achieve downstream tasks performances comparable to State-Of-The-Art, for tasks including action recognition and text-image retrievals."
  - id: 706
    order: 278
    poster_session: 3
    session_id: 8
    title: "Spatiotemporal Deformable Scene Graphs for Complex Activity Detection"
    authors:
      - author: "Salman Khan (Oxford Brookes University)"
      - author: "Fabio Cuzzolin (Oxford Brookes University)"
    all_authors: "Salman Khan and Fabio Cuzzolin"
    code: "https://github.com/salmank255/SDSG_Complex_Activity"
    keywords:
      - word: "action detection"
      - word: "activity detection"
      - word: "complex activity detection"
      - word: "scene graph"
      - word: "graph convolutional network"
      - word: "autonomous driving"
      - word: "surgical robotics"
      - word: "deformable pooling"
      - word: "parts deformation"
      - word: ""
    paper: "papers/0706.pdf"
    supp: "supp/0706_supp.zip"
    abstract: "Long-term complex activity recognition and localisation can be crucial for decision making in autonomous systems such as smart cars and surgical robots. Here we address the problem via a novel deformable, spatiotemporal scene graph approach, consisting of three main building blocks: (i) action tube detection, (ii) the modelling of the deformable geometry of parts, and (iii) a graph convolutional network.  Firstly, action tubes are detected in a series of snippets.  Next, a new 3D deformable RoI pooling layer is designed for learning the flexible, deformable geometry of the constituent action tubes. Finally, a scene graph is constructed by considering all parts as nodes and connecting them based on different semantics such as order of appearance, sharing the same action label and feature similarity. We also contribute fresh temporal complex activity annotation for the recently released ROAD autonomous driving and SARAS-ESAD surgical action datasets and show the adaptability of our framework to different domains. Our method is shown to significantly outperform graph-based competitors on both augmented datasets."
  - id: 709
    order: 226
    oral_session: 6
    poster_session: 3
    session_id: 7
    title: "End-to-End Object Detection with Adaptive Clustering Transformer"
    authors:
      - author: "Minghang Zheng (Peking University)"
      - author: "Peng Gao (Chinese university of hong kong)"
      - author: "Renrui Zhang (Shanghai AI Lab)"
      - author: "Kunchang Li (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences)"
      - author: "Hongsheng Li (Chinese University of Hong Kong)"
      - author: "Hao Dong (Peking University)"
    all_authors: "Minghang Zheng, Peng Gao, Renrui Zhang, Kunchang Li, Hongsheng Li and Hao Dong"
    code: "https://github.com/gaopengcuhk/SMCA-DETR/tree/main/Adaptive_Cluster_Transformer"
    keywords:
      - word: "transformer"
      - word: "object detection"
      - word: ""
    paper: "papers/0709.pdf"
    supp: "supp/0709_supp.zip"
    abstract: "End-to-end Object Detection with Transformer (DETR) performs object detection with Transformer and achieves comparable performance with two-stage object detection like Faster-RCNN. However, DETR needs huge computational resources for training and inference due to the high-resolution spatial inputs. In this paper, a novel variant of transformer named Adaptive Clustering Transformer (ACT) has been proposed to reduce the computation cost for high-resolution input. ACT clusters the query features adaptively using Locality Sensitive Hashing (LSH) and approximates the query-key interaction using the prototype-key interaction. ACT can reduce the quadratic O(N^2) complexity inside self-attention into O(NK) where K is the number of prototypes in each layer. ACT can be a drop-in module replacing the original self-attention module without any training. ACT achieves a good balance between accuracy and computation cost (FLOPs). The code is available as supplementary for the ease of experiment replication and verification."
  - id: 711
    order: 279
    poster_session: 3
    session_id: 8
    title: "Learning Not to Reconstruct Anomalies"
    authors:
      - author: "Marcella Astrid (University of Science & Technology)"
      - author: "Muhammad Zaigham Zaheer (University of Science & Technology)"
      - author: "Jae-Yeong Lee (Electronics and Telecommunications Research Institute)"
      - author: "Seung-Ik Lee (Electronics and Telecommunication Research Institute (ETRI))"
    all_authors: "Marcella Astrid, Muhammad Zaigham Zaheer, Jae-Yeong Lee and Seung-Ik Lee"
    code: "https://github.com/aseuteurideu/LearningNotToReconstructAnomalies"
    keywords:
      - word: "anomaly detection"
      - word: "one class classification"
      - word: "pseudo anomaly"
      - word: "surveillance"
      - word: "autoencoder"
      - word: "reconstruction"
      - word: "augmentation"
    paper: "papers/0711.pdf"
    supp: "supp/0711_supp.zip"
    abstract: "Video anomaly detection is often seen as one-class classification (OCC) problem due to the limited availability of anomaly examples. Typically, to tackle this problem, an autoencoder (AE) is trained to reconstruct the input with training set consisting only of normal data. At test time, the AE is then expected to well reconstruct the normal data while poorly reconstructing the anomalous data. However, several studies have shown that, even with only normal data training, AEs can often start reconstructing anomalies as well which depletes the anomaly detection performance. To mitigate this problem, we propose a novel methodology to train AEs with the objective of reconstructing only normal data, regardless of the input (i.e., normal or abnormal). Since no real anomalies are available in the OCC settings, the training is assisted by pseudo anomalies that are generated by manipulating normal data to simulate the out-of-normal-data distribution. We additionally propose two ways to generate pseudo anomalies: patch and skip frame based. Extensive experiments on three challenging video anomaly datasets demonstrate the effectiveness of our method in improving conventional AEs, achieving state-of-the-art performance."
  - id: 718
    order: 8
    oral_session: 2
    poster_session: 1
    session_id: 1
    title: "Adaptive GMM Convolution for Point Cloud Learning"
    authors:
      - author: "Fei Yang (Nanjing University of Science and Technology)"
      - author: "Huan Wang (Nanjing University of Science & Technology)"
      - author: "Zhong Jin (Nanjing University of Science and Technology)"
    all_authors: "Fei Yang, Huan Wang and Zhong Jin"
    code: "https://github.com/yangfei1223/AGMMConv"
    keywords:
      - word: "point cloud learning"
      - word: "point cloud segmentation"
      - word: "discrete convolution"
      - word: "adaptive kernel representation"
      - word: "rotation invariance"
      - word: "local pattern matching"
      - word: "Gaussian mixture model"
      - word: "mixture density network"
    paper: "papers/0718.pdf"
    supp: "supp/0718_supp.zip"
    abstract: "The success of CNNs (Convolutional Neural Networks) is mainly attributed to the (translation) invariance and local pattern matching effect of convolution kernels. Accordingly, generalizing discrete convolution operation with such two properties (invariance and local pattern matching) to point cloud domain is enlightening for point cloud learning. Inspired by this, we propose an adaptive GMM (Gaussian Mixture Model) convolution (AGMMConv) operation for point cloud learning. Considering the irregularity of point clouds, we propose to represent the kernel points with a GMM, where the mean vectors denote coordinates of kernel points and the covariance matrices determine the shape of each kernel point. Meanwhile, the GMM is a distribution representation of the local geometric surface learned from the local observation, which makes the kernel adaptive to local geometric structures. The proposed convolution is intrinsically invariant to permutation and translation. Besides, potential rotation invariance can be induced from the probability representation, which is an important prior for 3D objects recognition. In convolution, a series of shared weights are associated with each GMM kernel point to match local patterns of point clouds, which allows us to learn rich features with various learnable templates by analogy to the classical image convolution. Experiments on various datasets including object-level and scene-level tasks demonstrate the effectiveness and robustness of the proposed method. Code is available at https://github.com/yangfei1223/AGMMConv."
  - id: 724
    order: 173
    poster_session: 2
    session_id: 5
    title: "Logsig-RNN: a novel network for robust and efficient skeleton-based action recognition"
    authors:
      - author: "Hao Ni (University College London)"
      - author: "Shujian Liao (UCL)"
      - author: "Weixin Yang (Mathematical Institute, University of Oxford)"
      - author: "Kevin Schlegel (University College London)"
      - author: "Terry J Lyons (University of Oxford)"
    all_authors: "Hao Ni, Shujian Liao, Weixin Yang, Kevin Schlegel and Terry J Lyons"
    code: "https://github.com/steveliao93/GCN_LogsigRNN"
    keywords:
      - word: "skeleton-based action recognition"
      - word: "recurrent neural network"
      - word: "log-signature"
    paper: "papers/0724.pdf"
    supp: "supp/0724_supp.zip"
    abstract: "This paper contributes to the challenge of learning human actions from skeleton-based video data. The key step is to develop a generic network architecture to extract discriminative features for the spatio-temporal skeleton data. In this paper, we propose a novel module, namely Logsig-RNN, which is the combination of the log-signature layer and recurrent type neural networks (RNNs). The former one comes from the mathematically principled technology of signatures and log-signatures as representations for streamed data, which can manage high sample rate streams, non-uniform sampling and time series of variable length. It serves as an enhancement of the recurrent layer, which can be conveniently plugged into neural networks. Besides we propose two path transformation layers to significantly reduce path dimension while retaining the essential information fed into the Logsig-RNN module.  Finally, numerical results demonstrate that replacing the RNN module by the Logsig-RNN module in SOTA networks consistently improves the performance on both Chalearn data and NTU RGB+D 120 skeletal action data in terms of accuracy and robustness. In particular, we achieve state-of-the-art accuracy on the Chalearn2013 gesture data by combining simple path transformation layers with the Logsig-RNN."
  - id: 730
    order: 378
    poster_session: 4
    session_id: 11
    title: "Self-Supervised Monocular Depth Estimation with Internal Feature Fusion"
    authors:
      - author: "Hang Zhou (University of East Anglia)"
      - author: "David Greenwood (University of East Anglia)"
      - author: "Sarah Taylor (University of East Anglia)"
    all_authors: "Hang Zhou, David Greenwood and Sarah Taylor"
    code: "https://github.com/brandleyzhou/DIFFNet"
    keywords:
      - word: "depth estimation"
      - word: "structure from motion"
    paper: "papers/0730.pdf"
    supp: "supp/0730_supp.pdf"
    abstract: "Self-supervised learning for depth estimation uses geometry in image sequences for supervision and shows promising results. Like many computer vision tasks, depth network performance is determined by the capability to learn accurate spatial and semantic representations from images. Therefore, it is natural to exploit semantic segmentation networks for depth estimation. In this work, based on a well-developed semantic segmentation network HRNet, we propose a novel depth estimation network DIFFNet, which can make use of semantic information in down and up sampling procedures. By applying feature fusion and an attention mechanism, our proposed method outperforms the state-of-the-art monocular depth estimation methods on the KITTI benchmark. Our method also demonstrates greater potential on higher resolution training data. We propose an additional model evaluation strategy by establishing a test set of challenging cases, empirically derived from the standard benchmark."
  - id: 732
    order: 51
    poster_session: 1
    session_id: 2
    title: "Spatial Aggregation for Scene Text Recognition"
    authors:
      - author: "Yili Huang (Shanghai Jiaotong University)"
      - author: "Chengyu Gu (Shanghai Jiao Tong University)"
      - author: "shilin wang (SEIEE, Shanghai Jiaotong University)"
      - author: "Zheng Huang (Shanghai Jiao Tong University)"
      - author: "Kai Chen (sjtu)"
    all_authors: "Yili Huang, Chengyu Gu, Shilin Wang, Zheng Huang and Kai Chen"
    code: ""
    keywords:
      - word: "Scene text recognition"
      - word: "Vocabulary reliance"
      - word: "Spatial aggregation"
    paper: "papers/0732.pdf"
    supp: ""
    abstract: "Text recognition in natural images is an important research topic that has attracted widespread interest in recent years. Without character-level annotations, most existing state-of-the-art scene text recognition methods adopt CTC or attention-based decoders in the prediction stage to obtain the final word-level recognition results. However, these methods bring strong vocabulary reliance and fail to obtain satisfactory results when the predicting sample is out of the vocabulary in the training set. Moreover, predicting character-by-character in serial also limits efficiency. To solve these problems, in this paper, a new structure for the prediction stage is proposed to alleviate vocabulary reliance and accelerate prediction. In the new prediction stage, two classification layers are adopted on each feature vector to predict i) the character and ii) the order of the character in the word from the image region represented by the feature vector. Then, a spatial aggregation layer is designed to comprehensively integrate the character classification and the order estimation results to derive text recognition. In addition, a self-attention layer is adopted between the feature extraction stage and prediction stage to model the context. The experiment results on various benchmarks have demonstrated that compared with several state-of-the-art approaches, the proposed model achieves better performance in recognition accuracy and efficiency."
  - id: 735
    order: 379
    poster_session: 4
    session_id: 11
    title: "DeepSportLab: a Unified Framework for Ball Detection, Player Instance Segmentation and Pose Estimation in Team Sports Scenes"
    authors:
      - author: "Seyed Abolfazl Ghasemzadeh (UCLouvain)"
      - author: "Gabriel Van Zandycke (UCLouvain)"
      - author: "Maxime Istasse (UCLouvain, ICTEAM, ELEN, ISPGroup)"
      - author: "Niels Sayez (UCLouvain)"
      - author: "Amirafshar Moshtaghpour (Rosalind Franklin Institute)"
      - author: "Christophe De Vleeschouwer (Université Catholique de Louvain)"
    all_authors: "Seyed Abolfazl Ghasemzadeh, Gabriel Van Zandycke, Maxime Istasse, Niels Sayez, Amirafshar Moshtaghpour and Christophe De Vleeschouwer"
    code: "https://github.com/ispgroupucl/DeepSportLab"
    keywords:
      - word: "instance segmentation"
      - word: "ball detection"
      - word: "pose estimation"
      - word: "team sport"
      - word: "sport scenes"
      - word: "multitask learning"
      - word: "multi-task learning"
      - word: "multi task learning"
      - word: "basketball"
      - word: "COCO"
      - word: "DeepSport"
      - word: "panoptic quality"
      - word: ""
    paper: "papers/0735.pdf"
    supp: "supp/0735_supp.zip"
    abstract: "This paper presents a unified framework to (i) locate the ball, (ii) predict the pose, and (iii) segment the instance mask of players in team sports scenes. Those problems are of high interest in automated sports analytics, production, and broadcast. A common practice is to individually solve each problem by exploiting universal state-of-the-art models, e.g., Panoptic-DeepLab for player segmentation. In addition to the increased complexity resulting from the multiplication of single-task models, the use of the off-the-shelf models also impedes the performance due to the complexity and specificity of the team sports scenes, such as strong occlusion, and motion blur. To circumvent those limitations, our paper proposes to train a single model that simultaneously predicts the ball and the player mask and pose by combining the part intensity fields and the spatial embeddings principles. Part intensity fields provide the ball and player location, as well as player joints location. Spatial embeddings are then exploited to associate player instance pixels to their respective player center, but also to group player joints into skeletons.  
We demonstrate the effectiveness of the proposed model on the DeepSport basketball dataset, achieving comparable performance to the state-of-the-art models addressing each individual task separately."
  - id: 736
    order: 280
    poster_session: 3
    session_id: 8
    title: "Towards Dynamic and Scalable Active Learning with Neural Architecture Adaption for Object Detection"
    authors:
      - author: "Fuhui Tang (Huawei Noah’s Ark Lab)"
      - author: "ChenHan Jiang (Noah's Ark Lab, Huawei Inc.)"
      - author: "Dafeng Wei (Shanghai Jiao Tong University)"
      - author: "Hang Xu (Huawei Noah's Ark Lab)"
      - author: "Andi Zhang (University of Cambridge)"
      - author: "Wei Zhang (	Noah's Ark Lab, Huawei Technologies)"
      - author: "Hongtao Lu (Shanghai Jiao Tong University)"
      - author: "Chunjing Xu (Huawei Noah's Ark Lab)"
    all_authors: "Fuhui Tang, ChenHan Jiang, Dafeng Wei, Hang Xu, Andi Zhang, Wei Zhang, Hongtao Lu and Chunjing Xu"
    code: ""
    keywords:
      - word: "active learning"
      - word: "neural architecture adaption"
      - word: "object detection"
      - word: "dirichlet calibration"
      - word: "clustering sampling"
      - word: "network morphism modifications"
      - word: "uncertainty"
      - word: "dimension reduction"
      - word: "sample diversity"
      - word: "swap-expand strategy"
      - word: ""
    paper: "papers/0736.pdf"
    supp: ""
    abstract: "Active learning aims to reduce the annotation cost by selecting those informative samples to improve training efficiency and network accuracy. However, current active learning methods for object detection have three drawbacks: a) the network architectures of the detector during active learning are fixed without considering its saturation; (b) the detector may fall short in giving credible prediction probabilities on unlabeled data; (c) existing uncertainty measures may lead to homogenization of the samples. To overcome these problems, we propose a novel active learning strategy with dynamic neural architecture adaption for object detection. Specially, we incorporate a neural architecture adaption module that modifies and expands the current detector structure for the variation of incoming data stream. We design several network morphism modifications to enable an efficient and optimal adaption, which avoids the retraining of the detector after changing the network architecture in each round. Furthermore, we introduce Dirichlet calibration to correct the classifier for obtaining credible prediction, and present a clustering sampling scheme to select diverse samples. Experimental results show that the proposed method outperforms the previous state-of-the-art active learning methods with fixed architectures, improving 1.9% mAP on BDD and 1.6% mAP on COCO."
  - id: 737
    order: 380
    poster_session: 4
    session_id: 11
    title: "Visual Keyword Spotting with Attention"
    authors:
      - author: "Prajwal K R (VGG, Oxford)"
      - author: "Liliane Momeni (University of Oxford)"
      - author: "Triantafyllos Afouras (University of Oxford)"
      - author: "Andrew Zisserman (University of Oxford)"
    all_authors: "Prajwal K R, Liliane Momeni, Triantafyllos Afouras and Andrew Zisserman"
    code: ""
    keywords:
      - word: "visual keyword spotting"
      - word: "lip reading"
    paper: "papers/0737.pdf"
    supp: ""
    abstract: "In this paper, we consider the task of spotting spoken keywords in silent video sequences -- also known as visual keyword spotting. To this end, we investigate Transformer-based models that ingest two streams, a visual encoding of the video and a phonetic encoding of the keyword, and output the temporal location of the keyword if present. Our contributions are as follows: (1) We propose a novel architecture, the Transpotter, that uses full cross-modal attention between the visual and phonetic streams; (2) We show through extensive evaluations that our model outperforms the prior state-of-the-art visual keyword spotting and lip reading methods on the challenging LRW, LRS2, LRS3 datasets by a large margin; (3) We demonstrate the ability of our model to spot words under the extreme conditions of isolated mouthings in sign language videos. 
"
  - id: 739
    order: 281
    poster_session: 3
    session_id: 8
    title: "Zero-Shot Action Recognition from Diverse Object-Scene Compositions"
    authors:
      - author: "Carlo Bretti (University of Amsterdam)"
      - author: "Pascal Mettes (University of Amsterdam)"
    all_authors: "Carlo Bretti and Pascal Mettes"
    code: "https://github.com/carlobretti/object-scene-compositions-for-actions"
    keywords:
      - word: "action recognition"
      - word: "zero-shot learning"
      - word: "object-scene compositions"
      - word: ""
    paper: "papers/0739.pdf"
    supp: ""
    abstract: "This paper investigates the problem of zero-shot action recognition, in the setting where no training videos with seen actions are available. For this challenging scenario, the current leading approach is to transfer knowledge from the image domain by recognizing objects in videos using pre-trained networks, followed by a semantic matching between objects and actions. Where objects provide a local view on the content in videos, in this work we also seek to include a global view of the scene in which actions occurs. We find that scenes on their own are also capable of recognizing unseen actions, albeit more marginally than objects, and a direct combination of object-based and scene-based scores actually degrades the action recognition performance. To get the best out of objects and scenes, we propose to construct them as a Cartesian product of all possible compositions. We outline how to determine the likelihood of object-scene compositions in videos, as well as a semantic matching from object-scene compositions to actions that enforces diversity among the most relevant compositions for each action. While simple, our composition-based approach outperforms object-based approaches and even state-of-the-art zero-shot approaches that rely on large-scale video datasets with hundreds of seen actions for training and knowledge transfer."
  - id: 743
    order: 174
    poster_session: 2
    session_id: 5
    title: "CvS: Classification via Segmentation For Small Datasets"
    authors:
      - author: "Nooshin Mojab (University of Illinois at Chicago)"
      - author: "Philip  S Yu (UIC)"
      - author: "Joelle Hallak (University of Illinois at Chicago	)"
      - author: "Darvin  Yi (University of Illinois at Chicago	)"
    all_authors: "Nooshin Mojab, Philip  S Yu, Joelle Hallak and Darvin  Yi"
    code: ""
    keywords:
      - word: "Small Datasets"
      - word: "Segmentation"
      - word: "Image Classification"
      - word: "Weakly Supervised Learning Deep Learning"
      - word: "Multi-task Learning"
    paper: "papers/0743.pdf"
    supp: ""
    abstract: "Deep learning models have shown promising results in a wide range of computer vision applications across various domains. The success of deep learning methods relies heavily on the availability of a large amount of data. Deep neural networks are prone to overfitting when data is scarce. This problem becomes even more severe for neural network with classification head with access to only a few data points. However, acquiring large-scale datasets is very challenging, laborious, or even infeasible in some domains. Hence, developing classifiers that are able to perform well in small data regimes is crucial for applications with limited data. This paper presents CvS, a cost-effective classifier for small datasets that derives the classification labels from predicting the segmentation maps. We employ the label propagation method to achieve a fully segmented dataset with only a handful of manually segmented data. We evaluate the effectiveness of our framework on diverse problems showing that CvS is able to achieve much higher classification results compared to previous methods when given only a handful of examples."
  - id: 745
    order: 282
    poster_session: 3
    session_id: 8
    title: "Grounded Situation Recognition with Transformers"
    authors:
      - author: "Junhyeong Cho (POSTECH)"
      - author: "Youngseok Yoon (POSTECH)"
      - author: "Hyeonjun Lee (POSTECH)"
      - author: "Suha Kwak (POSTECH)"
    all_authors: "Junhyeong Cho, Youngseok Yoon, Hyeonjun Lee and Suha Kwak"
    code: "https://github.com/jhcho99/gsrtr"
    keywords:
      - word: "grounded situation recognition"
      - word: "situation recognition"
      - word: "transformers"
      - word: "scene understanding"
    paper: "papers/0745.pdf"
    supp: "supp/0745_supp.zip"
    abstract: "Grounded Situation Recognition (GSR) is the task that not only classifies a salient action (verb), but also predicts entities (nouns) associated with semantic roles and their locations in the given image. Inspired by the remarkable success of Transformers in vision tasks, we propose a GSR model based on a Transformer encoder-decoder architecture. The attention mechanism of our model enables accurate verb classification by capturing high-level semantic feature of an image effectively, and allows the model to flexibly deal with the complicated and image-dependent relations between entities for improved noun classification and localization. Our model is the first Transformer architecture for GSR, and achieves the state of the art in every evaluation metric on the SWiG benchmark. Our code is available at https://github.com/jhcho99/gsrtr."
  - id: 747
    order: 52
    poster_session: 1
    session_id: 2
    title: "SIR-SRGAN: Super-Resolution Generative Adversarial Networks with Self-Interpolation Ranker"
    authors:
      - author: "Jun-Hong Huang (Sichuan Normal University)"
      - author: "Hai-Kun Wang (Sichuan Normal University)"
      - author: "Zhi-Wu Liao (Sichuan Normal University)"
    all_authors: "Jun-Hong Huang, Hai-Kun Wang and Zhi-Wu Liao"
    code: "http://github.com/huang-junhong/SIRSRGAN"
    keywords:
      - word: "SISR"
      - word: "Ranker"
    paper: "papers/0747.pdf"
    supp: "supp/0747_supp.zip"
    abstract: "	Super-Resolution Generative Adversarial Networks (SRGAN) and follow-up perceptual single image super-resolution(SISR) method has shown us impressive texture generation capability. However, these models do not fully exploit the difference between the reconstructed image and the original image. In this paper, we propose a Self-Interpolation Ranker(SI-Ranker) to take advantage of the difference between the reconstructed image and the original image. SI-Ranker sorts the interpolated image of the reconstruction image and the original image and guides the image reconstruction during training. This method allows the generator to focus on the difference between the reconstruction image and the original image to improve the quality of the reconstructed image while obtaining a reconstruction image closer to the original image. In addition, we propose Patch Distance Loss (PDL) constrain the reconstruction of the image, which constrains the reconstruction of the image by cutting the image into small pieces and calculating the cosine similarity between the two. Experiments show that SIR-SRGAN improves consistency with the original at both pixel and feature levels, allowing it to be compared to state-of-the-art methods."
  - id: 748
    order: 381
    poster_session: 4
    session_id: 11
    title: "Multimodal Semi-Supervised Learning for 3D Objects"
    authors:
      - author: "Zhimin Chen (Clemson University)"
      - author: "Longlong Jing (The City University of New York)"
      - author: "Yang  Liang  (City College of New York)"
      - author: "YingLi Tian (City University of New York)"
      - author: "Bing Li (Clemson University)"
    all_authors: "Zhimin Chen, Longlong Jing, Yang  Liang, YingLi Tian and Bing Li"
    code: ""
    keywords:
      - word: "Semi-supervised learning"
      - word: "Multimodal learning"
      - word: "Representation learning"
    paper: "papers/0748.pdf"
    supp: "supp/0748_supp.zip"
    abstract: "In recent years, semi-supervised learning has been widely explored and shows excellent data efficiency for 2D data. There is an emerging need to improve data efficiency for 3D tasks due to the scarcity of labeled 3D data. This paper explores how the coherence of different modalities of 3D data (e.g. point cloud, image, and mesh) can be used to improve data efficiency for both 3D classification and retrieval tasks. We propose a novel multimodal semi-supervised learning framework by introducing instance-level consistency constraint and a novel multimodal contrastive prototype (M2CP) loss. The instance-level consistency enforces the network to generate consistent representations for multimodal data of the same object regardless of its modality. The M2CP maintains a multimodal prototype for each class and learns features with small intra-class variations by minimizing the feature distance of each object to its prototype while maximizing the distance to the others. Our proposed framework significantly outperforms all the state-of-the-art counterparts for both classification and retrieval tasks by a large margin on the modelNet10 and ModelNet40 datasets."
  - id: 749
    order: 175
    poster_session: 2
    session_id: 5
    title: "HS3: Learning with Proper Task Complexity in Hierarchically Supervised Semantic Segmentation"
    authors:
      - author: "Shubhankar Borse (Qualcomm AI Research	)"
      - author: "Hong Cai (Qualcomm AI Research)"
      - author: "Yizhe Zhang (Qualcomm AI Research)"
      - author: "Fatih Porikli (Qualcomm AI Research)"
    all_authors: "Shubhankar Borse, Hong Cai, Yizhe Zhang and Fatih Porikli"
    code: ""
    keywords:
      - word: "semantic segmentation"
      - word: "hierarchically supervised training"
      - word: "deeply supervised networks"
      - word: "supervised learning"
      - word: ""
    paper: "papers/0749.pdf"
    supp: "supp/0749_supp.zip"
    abstract: "While deeply supervised networks are common in recent literature, they typically impose the same learning objective on all transitional layers despite their varying representation powers. 

In this paper, we propose Hierarchically Supervised Semantic Segmentation (HS3), a training scheme that supervises intermediate layers in a segmentation network to learn meaningful representations by varying task complexity. To enforce a consistent performance vs. complexity trade-off throughout the network, we derive various sets of class clusters to supervise each transitional layer of the network. Furthermore, we devise a fusion framework, HS3-Fuse, to aggregate the hierarchical features generated by these layers, which can provide rich semantic contexts and further enhance the final segmentation. Extensive experiments show that our proposed HS3 scheme considerably outperforms vanilla deep supervision with no added inference cost. Our proposed HS3-Fuse framework further improves segmentation predictions and achieves state-of-the-art results on two large segmentation benchmarks: NYUD-v2 and Cityscapes."
  - id: 753
    order: 53
    poster_session: 1
    session_id: 2
    title: "A Mixed Quantization Network for Efficient Mobile Inverse Tone Mapping"
    authors:
      - author: "Juan Borrego-Carazo (Samsung Research UK)"
      - author: "Mete Ozay (Samsung Research UK)"
      - author: "Frederik Laboyrie (Samsung)"
      - author: "Paul Wisbey (Samsung)"
    all_authors: "Juan Borrego-Carazo, Mete Ozay, Frederik Laboyrie and Paul Wisbey"
    code: "https://github.com/BCJuan/ITMMQNet"
    keywords:
      - word: "inverse tone mapping"
      - word: "high dynamic range"
      - word: "efficient neural networks"
      - word: "quantization"
      - word: "mobile inference"
      - word: "attention"
    paper: "papers/0753.pdf"
    supp: "supp/0753_supp.zip"
    abstract: "Recovering a high dynamic range (HDR) image from a single low dynamic range (LDR) image, namely inverse tone mapping (ITM), is challenging due to the lack of information in over- and under-exposed regions. Current methods focus exclusively on training high performing but computationally inefficient models, which in turn hinders deployment of ITM models in resource constrained environments and applications. In the present work, we propose a computationally efficient neural network with reduced latency and cost compared to state-of-the-art ITM models in order to employ ITM in environments with limited computing power. 

To this end, we propose combining efficient operations with a novel mixed quantization scheme to produce a well performing but a computationally efficient mixed quantization network (MQN) which can perform single image ITM on a mobile platform.  In the experimental analyses,  ITM models trained using our proposed MQN perform on par with the state-of-the-art methods on benchmark datasets and obtain up to 10 times improvement on latency."
  - id: 756
    order: 54
    poster_session: 1
    session_id: 2
    title: "Simpler Does It: Generating Semantic Labels with Objectness Guidance"
    authors:
      - author: "Md Amirul Islam (Ryerson University)"
      - author: "Matthew Kowal (Ryerson University)"
      - author: "Sen Jia (University of Waterloo)"
      - author: "Konstantinos Derpanis (Ryerson University)"
      - author: "Neil Bruce (University of Guelph)"
    all_authors: "Md Amirul Islam, Matthew Kowal, Sen Jia, Konstantinos Derpanis and Neil Bruce"
    code: ""
    keywords:
      - word: "Weakly supervised segmentation"
      - word: "semi supervised segmentation"
      - word: "Pseudo-label generation"
      - word: "Class Activation Maps"
      - word: "Objectness"
      - word: "Saliency"
    paper: "papers/0756.pdf"
    supp: "supp/0756_supp.zip"
    abstract: "Existing weakly or semi-supervised semantic segmentation methods often utilize image or box-level supervision to generate pseudo-labels for weakly labeled images. However, due to the lack of strong supervision, the generated pseudo-labels are often noisy near the object boundaries, which is critical for learning strong representations within semantic regions. To address this problem, we present a novel framework that generates pseudo-labels for training images, which are then used to train a multi-task segmentation model. To generate pseudo-labels, we combine information from: (i) a class agnostic `objectness’ network that learns to recognize object-like regions, and (ii) either image-level or bounding box annotations. We show the efficacy of our approach by demonstrating how the objectness network can naturally be leveraged to generate object-like regions for unseen categories. We then propose an end-to-end multi-task learning strategy, that jointly learns to segment semantics and objectness using the generated pseudo-labels. Extensive experiments demonstrate the high quality of our generated pseudo-labels and effectiveness of the proposed framework in a variety of domains. Our proposed approach achieves better or competitive performance compared to existing weakly-supervised and semi-supervised methods. The code and trained models will be released upon acceptance. "
  - id: 757
    order: 176
    poster_session: 2
    session_id: 5
    title: "Mode-Guided Feature Augmentation for Domain Generalization"
    authors:
      - author: "Muhammad Haris Khan (Muhammad Bin Zayed University of Artificial Intelligence)"
      - author: "Syed Muhammad talha Zaidi (Kansas state university )"
      - author: "Salman Khan (MBZUAI/ANU)"
      - author: "Fahad Shahbaz Khan (MBZUAI)"
    all_authors: "Muhammad Haris Khan, Syed Muhammad talha Zaidi, Salman Khan and Fahad Shahbaz Khan"
    code: ""
    keywords:
      - word: "out-of-domain robustness"
      - word: "domain generalization"
      - word: "domain adaptation"
      - word: "convolutional neural networks"
      - word: "data augmentation"
      - word: "feature augmentation"
      - word: "subspace similarity"
      - word: "covariate shift"
      - word: "in-domain generalization"
      - word: "robust objective function"
    paper: "papers/0757.pdf"
    supp: "supp/0757_supp.zip"
    abstract: "This paper tackles domain generalization (DG) problem, the task of utilizing only source domain(s) to learn a model that generalizes well to unseen domains. A key challenge faced by DG is often the limited diversity in available source domain(s) that restricts the network's ability in learning a generalized model. Existing DG approaches leveraging data augmentation to address this problem mostly rely on compute-intensive auxiliary networks coupled with various losses and also suffer from additional training overhead. To this end, we propose a simple and efficient DG approach to augment source domain(s). We hypothesize the existence of favourable correlation between the source and target domain's major modes of variation, and upon exploring those modes in the source domain we can realize meaningful alterations to {background, appearance, pose and texture of object classes}. Inspired by this, our new DG approach performs feature-space augmentation by identifying the dominant modes of change in the source domain and implicitly including the augmented versions along those directions to achieve a better generalization across domains. Our method shows competitive performance against the current state-of-the-art methods on three popular DG benchmarks. Further, encouraging results on challenging single-source setting validate strong domain generalization capabilities of our approach."
  - id: 761
    order: 382
    poster_session: 4
    session_id: 11
    title: "Improving Face Recognition with Large Age Gaps by Learning to Distinguish Children"
    authors:
      - author: "Jungsoo Lee (Graduate School of Artificial Intelligence, KAIST)"
      - author: "JooYeol Yun (Korea Advanced Institute of Science and Technology)"
      - author: "Sunghyun  Park (KAIST)"
      - author: "Yonggyu Kim (Korea University)"
      - author: "Jaegul Choo (Korea Advanced Institute of Science and Technology)"
    all_authors: "Jungsoo Lee, JooYeol Yun, Sunghyun  Park, Yonggyu Kim and Jaegul Choo"
    code: "https://github.com/leebebeto/Inter-Prototype"
    keywords:
      - word: "face recognition"
      - word: "large age gaps"
      - word: "prototype vectors"
      - word: ""
    paper: "papers/0761.pdf"
    supp: "supp/0761_supp.zip"
    abstract: "Despite the unprecedented improvement of face recognition, existing face recognition models still show considerably low performances in determining whether pairs with large age gaps (i.e., child and adult image pairs) belong to the same identity. Previous approaches mainly focused on increasing the similarity between child and adult images of an identity to overcome the discrepancy of facial appearances due to aging. However, we observe that reducing the similarity between child images of different identities is crucial for learning discriminative features of children and thus improving face recognition with large age gaps. Based on this intuition, we propose a novel loss function called the Inter-Prototype loss which minimizes the similarity between child images. Unlike the previous studies, the Inter-Prototype loss does not require additional child images or training additional learnable parameters. Our extensive experiments and in-depth analyses show that our approach outperforms existing baselines in recognizing the child-adult pairs with large age gaps. "
  - id: 762
    order: 5
    oral_session: 2
    poster_session: 1
    session_id: 1
    title: "Collision Replay: What Does Bumping Into Things Tell You About Scene Geometry?"
    authors:
      - author: "Alexander Raistrick (University of Michigan)"
      - author: "Nilesh Kulkarni (University of Michigan)"
      - author: "David Fouhey (University of Michigan)"
    all_authors: "Alexander Raistrick, Nilesh Kulkarni and David Fouhey"
    code: ""
    keywords:
      - word: "collision"
      - word: "depth estimation"
      - word: "random walk"
      - word: "scene layout estimation"
      - word: "floorplan"
    paper: "papers/0762.pdf"
    supp: "supp/0762_supp.zip"
    abstract: "What does bumping into things in a scene tell you about scene geometry? In this paper, we investigate the idea of learning from collisions. At the heart of our approach is the idea of collision replay, where we use examples of a collision to provide supervision for observations at a past frame. We use collision replay to train convolutional neural networks to predict a distribution over collision time from new images. This distribution conveys information about the navigational affordances (e.g., corridors vs open spaces) and, as we show, can be converted into the distance function for the scene geometry. We analyze this approach with an agent that has noisy actuation in a photorealistic simulator."
  - id: 764
    order: 177
    poster_session: 2
    session_id: 5
    title: "Surprisingly Simple Semi-Supervised Domain Adaptation with Pretraining and Consistency"
    authors:
      - author: "Samarth Mishra (Boston University)"
      - author: "Kate Saenko (Boston University)"
      - author: "Venkatesh Saligrama (Boston University)"
    all_authors: "Samarth Mishra, Kate Saenko and Venkatesh Saligrama"
    code: "https://github.com/samarth4149/PAC"
    keywords:
      - word: "Domain Adaptation"
      - word: "Semi-supervised learning"
      - word: "Representation Learning"
      - word: ""
    paper: "papers/0764.pdf"
    supp: "supp/0764_supp.zip"
    abstract: "Most modern unsupervised domain adaptation (UDA) approaches are rooted in domain alignment, i.e., learning to align source and target features to learn a target domain classifier using source labels. In semi-supervised domain adaptation (SSDA), when the learner can access few target domain labels, prior approaches have followed UDA theory to use domain alignment for learning. We show that the case of SSDA is different and a good target classifier can be learned without needing alignment. We use self-supervised pretraining (via rotation prediction) and consistency regularization to achieve well separated target clusters, aiding in learning a low error target classifier. With our Pretraining and Consistency (PAC) approach, we achieve state of the art target accuracy on this semi-supervised domain adaptation task, surpassing multiple adversarial domain alignment methods, across multiple datasets. Notably, PAC outperforms all recent approaches by 3-5% on the large and challenging DomainNet benchmark, showing the strength of these simple techniques in fixing errors made by adversarial alignment."
  - id: 766
    order: 55
    poster_session: 1
    session_id: 2
    title: "AMICO: Amodal Instance Composition"
    authors:
      - author: "Peiye Zhuang (UIUC)"
      - author: "Denis Demandolx (Facebook)"
      - author: "Ayush Saraf (Facebook)"
      - author: "Xuejian Rong (Facebook)"
      - author: "Changil Kim (Facebook)"
      - author: "Jia-Bin Huang (Virginia Tech)"
    all_authors: "Peiye Zhuang, Denis Demandolx, Ayush Saraf, Xuejian Rong, Changil Kim and Jia-Bin Huang"
    code: ""
    keywords:
      - word: "image composition"
      - word: "GANs"
      - word: ""
    paper: "papers/0766.pdf"
    supp: "supp/0766_supp.zip"
    abstract: "Image composition aims to blend multiple objects to form a harmonized image. Existing approaches often assume precisely segmented and intact objects. Such assumptions, however, are hard to satisfy in unconstrained scenarios. We present Amodal Instance Composition for compositing imperfect---potentially incomplete and/or coarsely segmented---objects onto a target image. We first develop object shape prediction and content completion modules to synthesize the amodal contents. We then propose a neural composition model to blend the objects seamlessly. Our primary technical novelty lies in using separate foreground/background representations and blending mask prediction to alleviate segmentation errors. Our results show state-of-the-art performance on public COCOA and KINS benchmarks and attain favorable visual results across diverse scenes. We demonstrate various image composition applications such as object insertion and de-occlusion."
  - id: 770
    order: 178
    poster_session: 2
    session_id: 5
    title: "Neighborhood-Aware Neural Architecture Search"
    authors:
      - author: "Xiaofang Wang (Carnegie Mellon University)"
      - author: "Shengcao Cao (University of Illinois Urbana-Champaign)"
      - author: "Mengtian Li (Carnegie Mellon University)"
      - author: "Kris Kitani (Carnegie Mellon University)"
    all_authors: "Xiaofang Wang, Shengcao Cao, Mengtian Li and Kris Kitani"
    code: ""
    keywords:
      - word: "Neural Architecture Search"
      - word: "Generalization"
      - word: "Flat Minima"
    paper: "papers/0770.pdf"
    supp: "supp/0770_supp.zip"
    abstract: "Existing neural architecture search (NAS) methods often return an architecture with good search performance but generalizes poorly to the test setting. To achieve better generalization, we propose a novel neighborhood-aware NAS formulation to identify flat-minima architectures in the search space, with the assumption that flat minima generalize better than sharp minima. The phrase ``flat-minima architecture'' refers to architectures whose performance is stable under small perturbations in the architecture (e.g., replacing a convolution with a skip connection). Our formulation takes the ``flatness'' of an architecture into account by aggregating the performance over the neighborhood of this architecture. We demonstrate a principled way to apply our formulation to existing search algorithms, including sampling-based algorithms and gradient-based algorithms. To facilitate the application to gradient-based algorithms, we also propose a differentiable representation for the neighborhood of architectures. Based on our formulation, we propose neighborhood-aware random search (NA-RS) and neighborhood-aware differentiable architecture search (NA-DARTS). Notably, by simply augmenting DARTS with our formulation, NA-DARTS outperforms DARTS and achieves state-of-the-art performance on established benchmarks, including CIFAR-10, CIFAR-100 and ImageNet."
  - id: 772
    order: 383
    poster_session: 4
    session_id: 11
    title: "Bird’s Eye View Segmentation Using Lifted 2D Semantic Features"
    authors:
      - author: "Isht Dwivedi (Honda Research Institute USA)"
      - author: "Srikanth Malla (Honda Research Institute)"
      - author: "Yi-Ting Chen (National Chiao Tung University)"
      - author: "Behzad Dariush (Honda Research Institute US)"
    all_authors: "Isht Dwivedi, Srikanth Malla, Yi-Ting Chen and Behzad Dariush"
    code: "https://usa.honda-ri.com/eposh"
    keywords:
      - word: "segmentation"
      - word: "bird's eye view"
      - word: "pseudo-lidar"
      - word: "video understanding"
      - word: "autonomous driving"
      - word: "monocular camera"
      - word: "depth estimation"
    paper: "papers/0772.pdf"
    supp: "supp/0772_supp.zip"
    abstract: "We consider the problem of Bird's Eye View (BEV) segmentation with perspective monocular camera view as input. An effective solution to this problem is important in many autonomous navigation tasks such as behavior prediction and planning, being that the BEV segmented image provides an explainable intermediate representation that captures both the geometry and layout of the surrounding scene. 
Our approach to this problem involves a novel view transformation layer that effectively exploits depth maps to transform 2D image features to the BEV space. The framework includes the design of a neural network architecture to produce BEV segmentation maps using the proposed transformation layer. Of particular interest is evaluation of the proposed method in complex scenarios involving highly unstructured scenes that are not represented in static maps. In the absence of an appropriate dataset for this task, we introduce the EPOSH road-scene dataset that consists of 560 video-clips of highly unstructured construction scenes, annotated with unique labels in both perspective and BEV. For evaluation, we compare our approach with several competitive baselines and recently published works and show improvement over state of the art on the Nuscenes and on our EPOSH dataset. We plan to release the dataset, code and our trained models used in the paper at https://usa.honda-ri.com/eposh"
  - id: 773
    order: 283
    poster_session: 3
    session_id: 8
    title: "Unsupervised Discovery of Actions in Instructional Videos"
    authors:
      - author: "AJ Piergiovanni (Google)"
      - author: "Anelia Angelova (Google)"
      - author: "Michael S Ryoo (Google"
      - author: "Stony Brook University)"
      - author: "Irfan Essa (Google)"
    all_authors: "AJ Piergiovanni, Anelia Angelova, Michael S Ryoo (Google, Stony Brook University) and Irfan Essa"
    code: ""
    keywords:
      - word: "unsupervised learning"
      - word: "activity recognition"
      - word: "video understanding"
      - word: "video segmentation"
    paper: "papers/0773.pdf"
    supp: "supp/0773_supp.zip"
    abstract: "In this paper we address the problem of automatically discovering atomic actions from instructional videos. Instructional videos contain complex activities and are a rich source of information for intelligent agents, such as, autonomous robots or virtual assistants, which can, for example, automatically `read' the steps from an instructional video and execute them. However, videos are rarely annotated with atomic activities, their boundaries or duration. We present an unsupervised approach to learn atomic actions of structured human tasks from a variety of instructional videos. We propose a sequential stochastic autoregressive model for temporal segmentation of videos, which learns to represent and discover the sequential relationship between different atomic actions of the task, and which provides automatic and unsupervised self-labeling for videos. Our approach outperforms the state-of-the-art unsupervised methods with large margins.  Code will be open sourced."
  - id: 777
    order: 384
    poster_session: 4
    session_id: 11
    title: "Decentralised Person Re-Identification with Selective Knowledge Aggregation"
    authors:
      - author: "Shitong Sun (Queen Mary University of London)"
      - author: "Guile Wu (Queen Mary University of London)"
      - author: "Shaogang Gong (Queen Mary University of London)"
    all_authors: "Shitong Sun, Guile Wu and Shaogang Gong"
    code: ""
    keywords:
      - word: "person re-identification"
      - word: "federated learning"
      - word: "decentralised learning"
      - word: "normalisation"
    paper: "papers/0777.pdf"
    supp: ""
    abstract: "Existing person re-identification (Re-ID) methods mostly follow a centralised learning paradigm  which  shares  all  training data  to  a  collection  for  model  learning.   This paradigm is limited when data from different sources cannot be shared due to privacy concerns.  To resolve this problem, two recent works have introduced decentralised (federated) Re-ID learning for constructing a globally generalised model (server)without any direct access to local training data nor shared data across different source domains (clients).   However,  these methods are poor on how to adapt the generalised model to maximise its performance on individual client domain Re-ID tasks having different Re-ID label spaces, due to a lack of understanding of data heterogeneity across domains.   We call this poor ‘model personalisation’.   In this work,  we present a new Selective Knowledge Aggregation approach to decentralised person Re-ID to optimise the trade-off between model personalisation and generalisation.  Specifically, we incorporate attentive normalisation into the normalisation layers in a deep ReID model and propose to learn local normalisation layers specific to each domain, which are decoupled from the global model aggregation in federated Re-ID learning.  This helps to preserve model personalisation knowledge on each local client domain and learn instance-specific information. Further, we introduce a dual local normalisation mechanism to learn generalised normalisation layers in each local model, which are then transmitted to the global model for central aggregation.  This facilitates selective knowledge aggregation on the server to construct a global generalised model for out-of-the-box deployment on unseen novel domains.   Extensive experiments on  eight  person  Re-ID datasets show that the proposed approach to decentralised Re-ID significantly outperforms the state-of-the-art decentralised methods on both seen client domains and unseen novel domains."
  - id: 778
    order: 56
    poster_session: 1
    session_id: 2
    title: "Noisy Annotation Refinement for Object Detection"
    authors:
      - author: "Jiafeng Mao (The University of Tokyo)"
      - author: "Qing Yu (The University of Tokyo)"
      - author: "Yoko Yamakata (University of Tokyo, Japan)"
      - author: "Kiyoharu Aizawa (The University of Tokyo)"
    all_authors: "Jiafeng Mao, Qing Yu, Yoko Yamakata and Kiyoharu Aizawa"
    code: ""
    keywords:
      - word: "noise-resistant object detection"
      - word: "robust learning"
      - word: "annotation refinement"
    paper: "papers/0778.pdf"
    supp: "supp/0778_supp.zip"
    abstract: "Supervised training of object detectors requires well-annotated large-scale datasets, whose production is costly. Therefore, some efforts have been made to obtain annotations in economical ways, such as cloud sourcing. However, datasets obtained by these methods tend to contain noisy annotations such as inaccurate bounding boxes and incorrect class labels. In this study, we propose a new problem setting of training object detectors on datasets with entangled noises of annotations of class labels and bounding boxes. Our proposed method efficiently decouples the entangled noises, corrects the noisy annotations, and subsequently trains the detector using the corrected annotations. We verified the effectiveness of our proposed method and compared it with the baseline on noisy datasets with different noise levels. The experimental results show that our proposed method significantly outperforms the baseline."
  - id: 783
    order: 331
    oral_session: 8
    poster_session: 4
    session_id: 10
    title: "Selection of Source Images Heavily Influences the Effectiveness of Adversarial Attacks"
    authors:
      - author: "Utku   Ozbulak (Ghent University)"
      - author: "Esla  Timothy Anzaku (Ghent University)"
      - author: "Wesley De Neve (Ghent Univ.)"
      - author: "Arnout   Van Messem (Ghent University)"
    all_authors: "Utku   Ozbulak, Esla  Timothy Anzaku, Wesley De Neve and Arnout   Van Messem"
    code: "https://github.com/utkuozbulak/imagenet-adversarial-image-evaluation"
    keywords:
      - word: "adversarial attacks"
      - word: "adversarial examples"
      - word: "adversarial perturbation"
      - word: "adversarial transferability"
      - word: "adversarial vulnerability"
      - word: "model-to-model transferability"
      - word: "robustness"
      - word: "source images"
      - word: "image suitability"
      - word: "imagenet"
    paper: "papers/0783.pdf"
    supp: "supp/0783_supp.zip"
    abstract: "Although the adoption rate of deep neural networks (DNNs) has tremendously increased in recent years, a solution for their vulnerability against adversarial examples has not yet been found. As a result, substantial research efforts are dedicated to fix this weakness, with many studies typically using a subset of source images to generate adversarial examples, treating every image in this subset as equal. We demonstrate that, in fact, not every source image is equally suited for this kind of assessment. To do so, we devise a large-scale model-to-model transferability scenario for which we meticulously analyze the properties of adversarial examples, generated from every suitable source image in ImageNet by making use of three of the most frequently deployed attacks. In this transferability scenario, which involves seven distinct DNN models, including the recently proposed vision transformers, we reveal that it is possible to have a difference of up to $12.5%$ in model-to-model transferability success, $1.01$ in average $L_2$ perturbation, and $0.03$ ($8/225$) in average $L_{infty}$ perturbation when $1,000$ source images are sampled randomly among all suitable candidates. We then take one of the first steps in evaluating the robustness of images used to create adversarial examples, proposing a number of simple but effective methods to identify unsuitable source images, thus making it possible to mitigate extreme cases in experimentation and support high-quality benchmarking."
  - id: 784
    order: 284
    poster_session: 3
    session_id: 8
    title: "VIN: Voxel-based Implicit Network for Joint3D Object Detection and Segmentation for Lidars"
    authors:
      - author: "Yuanxin Zhong (University of Michigan)"
      - author: "Minghan Zhu (University of Michigan, Ann Arbor)"
      - author: "Huei Peng (University of Michigan)"
    all_authors: "Yuanxin Zhong, Minghan Zhu and Huei Peng"
    code: "https://github.com/cmpute/mmdetection3d/tree/vin"
    keywords:
      - word: "3D object detection"
      - word: "point cloud segmentation"
      - word: "implicit representation"
      - word: "implicit network"
    paper: "papers/0784.pdf"
    supp: "supp/0784_supp.zip"
    abstract: "A unified neural network structure is presented for joint 3D object detection and point cloud segmentation in this paper. We leverage rich supervision from both detection and segmentation labels rather than using just one of them. In addition, an extension based on single-stage object detectors is proposed based on the implicit function widely used in 3D scene and object understanding. The extension branch takes the final feature map from the object detection module as input, and produces an implicit function that generates semantic distribution for each point for its corresponding voxel center. We demonstrated the performance of our structure on nuScenes-lidarseg, a large-scale outdoor dataset. Our solution achieves competitive results against state-of-the-art methods in both 3D object detection and point cloud segmentation with little additional computation load compared with object detection solutions. The capability of efficient weakly supervision semantic segmentation of the proposed method is also validated by experiments."
  - id: 794
    order: 385
    poster_session: 4
    session_id: 11
    title: "Weakly Supervised Semantic Segmentation: From Box to Tag and Back"
    authors:
      - author: "Zongliang  Ji (University of Waterloo)"
      - author: "Olga Veksler (University of Waterloo)"
    all_authors: "Zongliang  Ji and Olga Veksler"
    code: "https://github.com/Jerryji007/Box2TagBack-bmvc2021"
    keywords:
      - word: "weak supervision"
      - word: "bounding box supervision"
      - word: "semantic segmentation"
      - word: "image tag supervision"
    paper: "papers/0794.pdf"
    supp: "supp/0794_supp.zip"
    abstract: "We propose an approach for semantic segmentation with weak supervision using bounding box annotations. 
Most previous work relies on segmenting bounding boxes into the  object and the background. Each box is segmented independently from the other boxes. We argue that the collection of boxes for the same class naturally provides a dataset from which we can learn a model to segment that object class. Learned model, in turn, leads to a better segmentation of each individual box. Thus for each class, we propose to train a segmentation CNN from the dataset consisting of the bounding boxes for that class. This step transforms the bounding box weak supervision into to several image-tag weak supervision tasks. Each image-tag weak supervision task is on a dataset with a single object class. After we train these single-class CNNs, we apply them back to the training bounding boxes to obtain object/background segmentations and merge them to construct pseudo-ground truth. The obtained pseudo-ground truth is used for training a standard segmentation CNN.  
We improve the state of the art on Pascal VOC 2012 benchmark in bounding box weak supervision setting. "
  - id: 795
    order: 285
    poster_session: 3
    session_id: 8
    title: "Wide and Narrow: Video Prediction from Context and Motion"
    authors:
      - author: "Jaehoon Cho (Yonsei Univ.)"
      - author: "Jiyoung Lee (Yonsei University)"
      - author: "Changjae Oh (Queen Mary University of London)"
      - author: "Wonil Song (Yonsei University)"
      - author: "Kwanghoon Sohn  (Yonsei Univ.)"
    all_authors: "Jaehoon Cho, Jiyoung Lee, Changjae Oh, Wonil Song and Kwanghoon Sohn"
    code: ""
    keywords:
      - word: "video prediction"
      - word: "local filter memory networks"
      - word: "adaptive filter kernels"
      - word: "global context propagation networks"
      - word: "non-local neighboring representations"
      - word: ""
    paper: "papers/0795.pdf"
    supp: "supp/0795_supp.zip"
    abstract: " Video prediction, forecasting the future frames from a sequence of input frames, is a challenging task since the view changes are influenced by various factors, such as the global context surrounding the scene and local motion dynamics. In this paper, we propose a new framework to integrate these complementary attributes to predict complex pixel dynamics through deep networks. We present global context propagation networks that iteratively aggregate the non-local neighboring representations to preserve the contextual information over the past frames. To capture the local motion pattern of objects, we also devise local filter memory networks that generate adaptive filter kernels by storing the prototypical motion of moving objects in the memory. The proposed framework, utilizing the outputs from both networks, can address blurry predictions and color distortion. We conduct experiments on Caltech pedestrian and UCF101 datasets, and demonstrate state-of-the-art results. Especially for multi-step prediction, we obtain an outstanding performance in quantitative and qualitative evaluation."
  - id: 803
    order: 386
    poster_session: 4
    session_id: 11
    title: "Context-Aware Unsupervised Clustering for Person Search"
    authors:
      - author: "Byeong-Ju Han (Ulsan National Institute of Science and Technology	)"
      - author: "Kuhyeun Ko (Ulsan National Institute of Science and Technology)"
      - author: "Jae-Young Sim (Ulsan National Institute of Science and Technology)"
    all_authors: "Byeong-Ju Han, Kuhyeun Ko and Jae-Young Sim"
    code: "https://github.com/VIP-Lab-UNIST/CUCPS_official"
    keywords:
      - word: "person search"
      - word: "person detection"
      - word: "person re-identification"
      - word: "context-aware"
      - word: ""
    paper: "papers/0803.pdf"
    supp: "supp/0803_supp.zip"
    abstract: "The existing person search methods use the annotated labels of person identities to train deep networks in a supervised manner that requires a huge amount of time and effort for human labeling. In this paper, we first introduce a novel framework of person search that is able to train the network in the absence of the person identity labels, and propose efficient unsupervised clustering methods to substitute the supervision process using annotated person identity labels.  Specifically, we propose a hard negative mining scheme based on the uniqueness property that only a single person has the same identity to a given query person in each image. We also propose a hard positive mining scheme by using the contextual information of co-appearance that neighboring persons in one image tend to appear simultaneously in other images. The experimental results show that the proposed method achieves comparable performance to that of the state-of-the-art supervised person search methods, and furthermore outperforms the extended unsupervised person re-identification methods on the benchmark person search datasets."
  - id: 807
    order: 109
    oral_session: 3
    poster_session: 2
    session_id: 3
    title: "Hierarchical Contrastive Motion Learning for Video Action Recognition"
    authors:
      - author: "Xitong Yang (University of Maryland)"
      - author: "Xiaodong Yang (NVIDIA Research)"
      - author: "Sifei Liu (NVIDIA)"
      - author: "Deqing Sun (Google)"
      - author: "Larry Davis (University of Maryland)"
      - author: "Jan Kautz (NVIDIA)"
    all_authors: "Xitong Yang, Xiaodong Yang, Sifei Liu, Deqing Sun, Larry Davis and Jan Kautz"
    code: ""
    keywords:
      - word: "action recognition"
      - word: "motion hierarchy"
      - word: "motion representation"
      - word: "contrastive learning"
    paper: "papers/0807.pdf"
    supp: "supp/0807_supp.zip"
    abstract: "One central question for video action recognition is how to model motion. In this paper, we present hierarchical contrastive motion learning, a new self-supervised learning framework to extract effective motion representations from raw video frames. Our approach progressively learns a hierarchy of motion features that correspond to different abstraction levels in a network. This hierarchical design bridges the semantic gap between low-level motion cues and high-level recognition tasks, and promotes the fusion of appearance and motion information at multiple levels. At each level, an explicit motion self-supervision is provided via contrastive learning to enforce the motion features at the current level to predict the future ones at the previous level. Thus, the motion features at higher levels are trained to gradually capture semantic dynamics and evolve more discriminative for action recognition. Our motion learning module is lightweight and flexible to be embedded into various backbone networks. Extensive experiments on four benchmarks show that our approach compares favorably against the state-of-the-art methods yet without requiring optical flow or supervised pre-training."
  - id: 812
    order: 387
    poster_session: 4
    session_id: 11
    title: "Hierarchical Graph Networks for 3D Human Pose Estimation"
    authors:
      - author: "Han Li (Shanghai Jiao Tong University)"
      - author: "Bowen Shi (Shanghai Jiao Tong University)"
      - author: "Wenrui Dai (Shanghai Jiao Tong University)"
      - author: "Yabo Chen (Shanghai Jiao Tong University )"
      - author: "Botao Wang (Qualcomm AI Research)"
      - author: "Yu Sun (Qualcomm)"
      - author: "MIN GUO (1976)"
      - author: "Chenglin Li (Shanghai Jiao Tong University)"
      - author: "Junni Zou (Shanghai Jiao Tong University)"
      - author: "Hongkai Xiong (Shanghai Jiao Tong University)"
    all_authors: "Han Li, Bowen Shi, Wenrui Dai, Yabo Chen, Botao Wang, Yu Sun, MIN GUO, Chenglin Li, Junni Zou and Hongkai Xiong"
    code: ""
    keywords:
      - word: "pose estimation"
      - word: "computer vision"
      - word: "3D vision"
      - word: "graph convolution"
    paper: "papers/0812.pdf"
    supp: "supp/0812_supp.zip"
    abstract: "Recent 2D-to-3D human pose estimation works tend to utilize the graph structure formed by the topology of the human skeleton. However, we argue that this skeletal topology is too sparse to reflect the body structure and suffer from serious 2D-to-3D ambiguity problem. To overcome these weaknesses, we propose a novel graph convolution network architecture, Hierarchical Graph Networks (HGN). It is based on denser graph topology generated by our multi-scale graph structure building strategy, thus providing more delicate geometric information. The proposed architecture contains three sparse-to-fine representation subnetworks organized in parallel, in which multi-scale graph-structured features are processed and exchange information through a novel feature fusion strategy, leading to rich hierarchical representations. We also introduce a 3D coarse mesh constraint to further boost detail-related feature learning. Extensive experiments demonstrate that our HGN achieves the state-of-the-art performance with reduced network parameters. "
  - id: 813
    order: 57
    poster_session: 1
    session_id: 2
    title: "Quality Level Prediction of Image Compression using Block-wise Confidence-aware CNN"
    authors:
      - author: "Kyuwon Kim (Samsung Electronics)"
      - author: "Chulju Yang (Samsung)"
    all_authors: "Kyuwon Kim and Chulju Yang"
    code: ""
    keywords:
      - word: "convolutional neural network (CNN)"
      - word: "compression artifacts removal"
      - word: "compression quality prediction"
      - word: "confidence estimation"
    paper: "papers/0813.pdf"
    supp: "supp/0813_supp.zip"
    abstract: "Almost all images are compressed to be transferred or stored, exhibiting various visual artifacts. For this reason, identifying the compression quality level with only visual cues is the starting point to enhance the image quality. This paper introduces a compression quality prediction method, named Q1Net, which yields a single quality level with over 99%-accuracy in a matter of milliseconds on mobile devices regardless of the image resolution. This real-time and high-accurate performance is attributed to the observation that most image compression methods are based upon transform coding on small blocks of different characteristics. To separately investigate and exploit the distinct visual deformations induced by one transform coding, our method measures the compression quality level on various image patches containing a basic coding block and its neighboring pixels. Our approach then elaborately selects promising candidate patches that can indicate the compression quality reliably through CNN-based statistical confidence estimation. In order to make a final decision, the proposed method fuses the prediction results from a selected number of input patches, which makes it scalable and operable on mobile devices with varying computational capabilities. According to the extensive experiments on the DIV2K dataset and an off-the-shelf smartphone, our block-wise confidence-aware Q1Net achieves better performance in compression quality prediction than other well-known CNN-based methods in terms of speed and accuracy."
  - id: 817
    order: 388
    poster_session: 4
    session_id: 11
    title: "Local and Global Point Cloud Reconstruction for 3D Hand Pose Estimation"
    authors:
      - author: "Ziwei Yu (National University of Singapore)"
      - author: "Linlin Yang ( University of Bonn)"
      - author: "shicheng chen (National University of Singapore)"
      - author: "Angela Yao (National University of Singapore)"
    all_authors: "Ziwei Yu, Linlin Yang, Shicheng Chen and Angela Yao"
    code: ""
    keywords:
      - word: "local and global"
      - word: "point cloud"
      - word: "3D hand pose estimation"
      - word: "multi-view dataset"
    paper: "papers/0817.pdf"
    supp: "supp/0817_supp.zip"
    abstract: "This paper addresses the 3D point cloud reconstruction and 3D pose estimation of the human hand from a single RGB image. To that end, we present a novel pipeline for local and global point cloud reconstruction using a 3D hand template while learning a latent representation for pose estimation. To demonstrate our method, we introduce a new multi-view hand posture dataset to obtain complete 3D point clouds of the hand in the real world. Experiments on our newly proposed dataset and four public benchmarks demonstrate the model's strengths. Our method outperforms competitors in 3D pose estimation while reconstructing realistic-looking complete 3D hand point clouds."
  - id: 820
    order: 179
    poster_session: 2
    session_id: 5
    title: "Self-supervised Knowledge Distillation for Few-shot Learning"
    authors:
      - author: "Jathushan Rajasegaran ( 	University of Moratuwa)"
      - author: "Salman Khan (MBZUAI/ANU)"
      - author: "Munawar Hayat (Monash University)"
      - author: "Fahad Shahbaz Khan (MBZUAI)"
      - author: "Mubarak Shah (University of Central Florida)"
    all_authors: "Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan and Mubarak Shah"
    code: "https://github.com/brjathu/SKD"
    keywords:
      - word: "Self-supervision"
      - word: "Knowledge Distillation"
      - word: "Few-shot Learning"
    paper: "papers/0820.pdf"
    supp: ""
    abstract: "
Real-world contains an overwhelmingly large number of object classes, learning all of which at once is infeasible. Few-shot learning provides a promising learning paradigm due to its ability to quickly adapt to novel out of order distributions with only a few samples. Recent works show that simply learning a good feature embedding can outperform more sophisticated meta-learning and metric learning algorithms for few-shot learning. This paper proposes a self supervised knowledge distillation approach, which learns a strong equivariant feature embedding for few shot learning, by faithfully encoding inter-class relationships and preserving intra class diversity. To this end, we follow a two-stage learning process: first, we train our model using a self-supervised auxiliary loss to maximize the entropy of the feature embedding, thus creating an optimal output manifold. In the second stage, we minimize the entropy on feature embedding by bringing self-supervised positive twins together, while constraining the learned manifold with a student-teacher distillation. Our experiments show that, even in the first stage, features learnt by self-supervision can outperform current state-of-the-art methods, with further gains achieved by our second stage distillation process."
  - id: 831
    order: 58
    poster_session: 1
    session_id: 2
    title: "Teacher-Class Network: A Neural Network Compression Mechanism"
    authors:
      - author: "Shaiq Munir Malik (Lahore University of Management Sciences)"
      - author: "Fnu Mohbat (Rensselaer Polytechnic Institute )"
      - author: "Muhammad Umair  Haider (Lahore University of Management Sciences.)"
      - author: "Muhammad Musab Rasheed (Lahore University of Management Sciences (LUMS) )"
      - author: "Murtaza Taj (Lahore University of Management Sciences)"
    all_authors: "Shaiq Munir Malik, Fnu Mohbat, Muhammad Umair  Haider, Muhammad Musab Rasheed and Murtaza Taj"
    code: "https://github.com/musab-r/TCN"
    keywords:
      - word: "model compression"
      - word: "knowledge distillation"
      - word: "teacher-student network"
      - word: ""
    paper: "papers/0831.pdf"
    supp: ""
    abstract: "To reduce the overwhelming size of Deep Neural Networks, teacher-student techniques aim to transfer knowledge from a complex teacher network to a simple student network. We instead propose a novel method called the teacher-class network consisting of a single teacher and multiple student networks (class of students). Instead of transferring knowledge to one student only, the proposed method divides learned space into sub-spaces, and each sub-space is learned by a student. Our students are not trained for problem-specific logits; they are trained to mimic knowledge (dense representation) learned by the teacher network; thus, the combined knowledge learned by the class of students can be used to solve other problems. The proposed teacher-class architecture is evaluated on several benchmark datasets such as MNIST, Fashion MNIST, IMDB Movie Reviews, CIFAR-10, and ImageNet on multiple tasks such as image and sentiment classification. Our approach outperforms the state-of-the-art single student approach in terms of accuracy and computational cost while achieving a 10-30 times reduction in parameters. Code is available at https://github.com/musab-r/TCN."
  - id: 835
    order: 59
    poster_session: 1
    session_id: 2
    title: "UWC: Unit-wise Calibration Towards Rapid Network Compression"
    authors:
      - author: "Chen Lin (Hikvision Research Institute)"
      - author: "Zheyang Li (Hikvision Research Institute)"
      - author: "Bo Peng (Hikvision Research Institute)"
      - author: "Wenming Tan (Hikvision Research Institute)"
      - author: "Ye Ren (Hikvision Research Institute)"
      - author: "Shiliang Pu (Hikvision Research Institute)"
    all_authors: "Chen Lin, Zheyang Li, Bo Peng, Wenming Tan, Ye Ren and Shiliang Pu"
    code: ""
    keywords:
      - word: "post training quantization"
    paper: "papers/0835.pdf"
    supp: ""
    abstract: "This paper introduces a post-training quantization~(PTQ) method achieving highly efficient Convolutional Neural Network~ (CNN) quantization with high performance.
Previous PTQ methods usually reduce compression error via performing layer-by-layer parameters calibration. However, with lower representational ability of extremely compressed parameters (e.g., the bit-width goes less than 4), it is hard to eliminate all the layer-wise errors. This work addresses this issue via proposing a unit-wise feature reconstruction algorithm based on an observation of second order Taylor series expansion of the unit-wise error. It indicates that leveraging the interaction between adjacent layers' parameters could compensate layer-wise errors better. In this paper, we define several adjacent layers as a Basic-Unit, and present a unit-wise post-training algorithm which  can minimize quantization error. This method achieves near-original accuracy on ImageNet and COCO when quantizing FP32 models to INT4 and INT3."
  - id: 836
    order: 389
    poster_session: 4
    session_id: 11
    title: "Self-Supervised Learning of Image Scale and Orientation"
    authors:
      - author: "Jongmin Lee (POSTECH)"
      - author: "Yoonwoo Jeong (POSTECH)"
      - author: "Minsu Cho (POSTECH)"
    all_authors: "Jongmin Lee, Yoonwoo Jeong and Minsu Cho"
    code: "https://github.com/bluedream1121/self-sca-ori"
    keywords:
      - word: "self-supervised learning"
      - word: "scale and orientation estimation"
      - word: "histogram alignment"
      - word: "patch pose estimation"
      - word: "visual correspondence"
      - word: "wide-baseline matching"
      - word: "multi-modal distribution"
    paper: "papers/0836.pdf"
    supp: "supp/0836_supp.zip"
    abstract: "We study the problem of learning to assign a characteristic pose, i.e., scale and orientation, for an image region of interest. Despite its apparent simplicity, the problem is non-trivial; it is hard to obtain a large-scale set of image regions with explicit pose annotations that a model directly learns from. To tackle the issue, we propose a self-supervised learning framework with a histogram alignment technique. We generate pairs of image regions by random scaling and rotating and then train a model to predict their distributions so that their relative difference is consistent with scaling and rotating being used. The proposed method learns a non-parametric and multi-modal distribution of scale and orientation without any supervision, achieving a significant improvement over previous methods in experimental evaluation. "
  - id: 838
    order: 180
    poster_session: 2
    session_id: 5
    title: "Multi-Class Novelty Detection with Generated Hard Novel Features"
    authors:
      - author: "Wei-Ta Chu (National Cheng Kung University)"
      - author: "Wei-Ting Cao (National Cheng Kung University)"
    all_authors: "Wei-Ta Chu and Wei-Ting Cao"
    code: ""
    keywords:
      - word: "novelty detection"
      - word: "hard novel features"
      - word: "generative adversarial network"
      - word: "segregation network"
    paper: "papers/0838.pdf"
    supp: ""
    abstract: "It has been shown that convolutional neural networks clearly have the overconfidence problem, i.e., mis-classify a novel sample into one of the known classes with high confidence. The task of multi-class novelty detection is thus important to detect novel samples during inference. In this work, we propose to generate hard novel features via a generative adversarial network to facilitate constructing a powerful novelty detector. The generated features should be around the boundaries between known classes and novel classes. They cause a bigger challenge for the novelty detector, and consequently enforce the novelty detector to be stronger. We verify effectiveness of hard novel features from several perspectives, and show that this idea yields the state-of-the-art performance. "
  - id: 840
    order: 181
    poster_session: 2
    session_id: 5
    title: "Latent-optimization based Disease-aware Image Editing for Medical Image Augmentation"
    authors:
      - author: "Aakash saboo (Delhi Technological University)"
      - author: "Prashnna K Gyawali (Rochester Institute of Technology)"
      - author: "ankit shukla (Bennet university)"
      - author: "Manoj Sharma (Bennett University)"
      - author: "Neeraj Jain (Sir Ganga Ram Hospital)"
      - author: "Linwei Wang (RIT)"
    all_authors: "Aakash saboo, Prashnna K Gyawali, Ankit Shukla, Manoj Sharma, Neeraj Jain and Linwei Wang"
    code: ""
    keywords:
      - word: "Latent optimization"
      - word: "StyleGAN"
      - word: "Image Editing"
      - word: "Chest X-ray"
      - word: "Image manipulation"
      - word: "constrained optimization"
      - word: "Disease progression"
      - word: "Disease quantification"
      - word: "Manifold"
      - word: "Latent space traversal"
    paper: "papers/0840.pdf"
    supp: "supp/0840_supp.zip"
    abstract: "Data augmentation addresses the critical challenge of limited data in medical imaging.  While generative adversarial networks (GANs) have been a popular choice in synthesizing medical images, controlled generation targeting disease-specific semantic has been difficult, partly due to the difficulty to disentangle local disease-specific semantic factors from global disease-irrelevant factors.  In this work, we present a semantic image editing framework for medical image augmentation that is able to generate smooth variations along the desired direction of disease attributes in user-defined regions of interest.  This is achieved by discovering the optimal trajectory on the latent manifold of a pre-trained StyleGAN, guided by a mask of the region of interest and explicitly constrained by desired directions of semantic changes. We test the presented method on the public Chest X-ray dataset. To evaluate the quality of the generated medical images, we leverage both domain experts (pulmonologists) for qualitative assessments and present a novel metric to quantify the ability of the presented method to generate progression of disease severity in the synthesized images.  We also show that data augmentation using the presented method improves downstream classification tasks."
  - id: 842
    order: 112
    oral_session: 3
    poster_session: 2
    session_id: 3
    title: "Unsupervised Human Action Recognition with Skeletal Graph Laplacian and Self-Supervised Viewpoints Invariance"
    authors:
      - author: "Giancarlo Paoletti (Istituto Italiano di Tecnologia)"
      - author: "Jacopo Cavazza (Istituto Italiano di Tecnologia)"
      - author: "Cigdem Beyan (Università degli Studi di Trento)"
      - author: "Alessio Del Bue (Istituto Italiano di Tecnologia (IIT))"
    all_authors: "Giancarlo Paoletti, Jacopo Cavazza, Cigdem Beyan and Alessio Del Bue"
    code: "www.github.com/IIT-PAVIS/UHAR_Skeletal_Laplacian"
    keywords:
      - word: "unsupervised learning"
      - word: "unsupervised action recognition"
      - word: "skeletal action recognition"
      - word: "skeleton action recognition"
      - word: "human action recognition"
      - word: "graph laplacian"
      - word: "viewpoint invariance"
    paper: "papers/0842.pdf"
    supp: "supp/0842_supp.zip"
    abstract: "This paper presents a novel end-to-end method for the problem of skeleton-based unsupervised human action recognition.
We propose a new architecture with a convolutional autoencoder that uses graph Laplacian regularization to model the skeletal geometry across the temporal dynamics of actions.
Our approach is robust towards viewpoint variations by including a self-supervised gradient reverse layer that ensure generalization across camera views.
The proposed method is validated on NTU-60 and NTU-120 large-scale datasets in which it outperforms all prior unsupervised skeleton-based approaches on the cross-subject, cross-view, and cross-setup protocols.
Although unsupervised, our learnable representation allows our method even to surpass a few supervised skeleton-based action recognition methods.
The code is available in: www.github.com/IIT-PAVIS/UHAR_Skeletal_Laplacian"
  - id: 844
    order: 286
    poster_session: 3
    session_id: 8
    title: "Weakly-Supervised Dense Action Anticipation"
    authors:
      - author: "Haotong Zhang (National University of Singapore)"
      - author: "Fuhai Chen (National University of Singapore)"
      - author: "Angela Yao (National University of Singapore)"
    all_authors: "Haotong Zhang, Fuhai Chen and Angela Yao"
    code: "https://github.com/zhanghaotong1/WSLVideoDenseAnticipation"
    keywords:
      - word: "dense anticipation"
      - word: "video understanding"
      - word: "weak supervision"
    paper: "papers/0844.pdf"
    supp: "supp/0844_supp.zip"
    abstract: "Dense anticipation aims to forecast future actions and their durations for long horizons. Existing approaches rely on fully-labelled data, i.e. sequences labelled with all future actions and their durations. We present a (semi-) weakly supervised method using only a small number of fully-labelled sequences and predominantly sequences in which only the (one) upcoming action is labelled. To this end, we propose a framework that generates pseudo-labels for future actions and their durations and adaptively refines them through a refinement module. Given only the upcoming action label as input, these pseudo-labels guide action/duration prediction for the future. We further design an attention mechanism to predict context-aware durations. Experiments on the Breakfast and 50Salads benchmarks verify our method’s effectiveness; we are competitive even when compared to fully supervised state-of-the-art models. We will make our code available at: https://github.com/zhanghaotong1/WSLVideoDenseAnticipation."
  - id: 848
    order: 60
    poster_session: 1
    session_id: 2
    title: "Hardware-Aware Mixed-Precision Neural Networks using In-Train Quantization"
    authors:
      - author: "Manoj Rohit Vemparala (BMW Group)"
      - author: "Nael Fasfous (Technical University of Munich)"
      - author: "Lukas Frickenstein (BMW Group)"
      - author: "Alexander Frickenstein (BMW Group)"
      - author: "Anmol Singh (BMW AG)"
      - author: "Driton Salihu (Technical University Munich)"
      - author: "Christian Unger (BMW)"
      - author: "Naveen Shankar Nagaraja (BMW Group)"
      - author: "WALTER STECHELE (Technical University of Munich (TUM))"
    all_authors: "Manoj Rohit Vemparala, Nael Fasfous, Lukas Frickenstein, Alexander Frickenstein, Anmol Singh, Driton Salihu, Christian Unger, Naveen Shankar Nagaraja and WALTER STECHELE"
    code: ""
    keywords:
      - word: "Quantization"
      - word: "Inference"
      - word: "Neural Network Compression"
      - word: "Mixed Precision"
      - word: "Hardware Aware Networks"
    paper: "papers/0848.pdf"
    supp: "supp/0848_supp.zip"
    abstract: "Fixed-point quantization is an effective method to reduce the model size and computational demand of convolutional neural networks, by lowering the numerical precision of all layers down to a specific bit-width. Recent work shows assigning layer-wise specific bit-widths has an advantage over uniform assignments, although requiring complex, post-training search techniques and many GPU hours to identify the optimal bit-width strategy. To alleviate this, we propose an in-train quantization method that can directly learn the optimal bit-widths for weights and activations during the gradient-based training process. We incorporate hardware-awareness into the gradient-based optimization to directly improve the real hardware execution metrics. We replace the discrete and non-differentiable hardware measurements with a differentiable Gaussian process regressor. This provides accurate hardware predictions as an auxiliary loss to the gradient-descent optimizer, performing hardware-friendly in-train quantization. Our hardware-aware mixed-precision ResNet56 achieves an improvement of 1.3 x in execution latency compared to the uniform 4-bit quantization with no degradation in accuracy. Finally, we highlight the effectiveness of the in-train quantization method in the context of adversarial training, improving the trade-off between prediction accuracy and robustness."
  - id: 868
    order: 330
    oral_session: 7
    poster_session: 4
    session_id: 9
    title: "KonIQ++: Boosting No-Reference Image Quality Assessment in the Wild by Jointly Predicting Image Quality and Defects"
    authors:
      - author: "Shaolin Su (Northwestern Polytechnical University)"
      - author: "Vlad Hosu (University of Konstanz)"
      - author: "Hanhe Lin (Robert Gordon University)"
      - author: "Yanning  Zhang (Northwestern Polytechnical University)"
      - author: "Dietmar Saupe (University of Konstanz)"
    all_authors: "Shaolin Su, Vlad Hosu, Hanhe Lin, Yanning  Zhang and Dietmar Saupe"
    code: "https://github.com/SSL92/koniqplusplus"
    keywords:
      - word: "image quality assessment"
      - word: "database"
      - word: "defects"
      - word: ""
    paper: "papers/0868.pdf"
    supp: "supp/0868_supp.zip"
    abstract: "Although image quality assessment (IQA) in-the-wild has been researched in computer vision, it is still challenging to precisely estimate perceptual image quality in the presence of real-world complex and composite distortions. In order to improve machine learning solutions for IQA, we consider side information denoting the presence of distortions besides the basic quality ratings in IQA datasets. Specifically, we extend one of the largest in-the-wild IQA databases, KonIQ-10k, to KonIQ++, by collecting distortion annotations for each image, aiming to improve quality prediction together with distortion identification. We further explore the interactions between image quality and distortion by proposing a novel IQA model, which jointly predicts image quality and distortion by recurrently refining task-specific features in a multi-stage fusion framework. Our dataset KonIQ++, along with the model, boosts IQA performance and generalization ability, demonstrating its potential for solving the challenging authentic IQA task. The proposed model can also accurately predict distinct image defects, suggesting its application in image processing tasks such as image colorization and deblurring."
  - id: 871
    order: 61
    poster_session: 1
    session_id: 2
    title: "Multi-bit Adaptive Distillation for Binary Neural Networks"
    authors:
      - author: "Ying Nie (Huawei Noah's Ark Lab)"
      - author: "Kai Han (Noah’s Ark Lab, Huawei Technologies)"
      - author: "Yunhe Wang (Huawei Technologies)"
    all_authors: "Ying Nie, Kai Han and Yunhe Wang"
    code: ""
    keywords:
      - word: "binary"
      - word: "distillation"
      - word: "1bit"
      - word: ""
    paper: "papers/0871.pdf"
    supp: "supp/0871_supp.zip"
    abstract: "Binary neural networks (BNNs) represent weights and activations using 1-bit values, which has extremely lower memory costs and computational complexities, but usually suffer from severe accuracy degradation. Knowledge distillation is an effective way to improve the performance of BNN by inheriting the knowledge from higher-bit network. However, faced with the accuracy gap and bit gap between 1-bit network and different higher-bit networks, it is uncertain which higher-bit network is more suitable to be the teacher of a certain BNN. Therefore, we propose a novel multi-bit adaptive distillation(MAD) method for maximally integrating the advantages of various bit-width teacher networks(e.g. 2-bit, 4-bit, 8-bit and 32-bit). In practice, intermediate features and output logits of teachers will be simultaneously utilized for improving the performance of BNN. Moreover, an adaptive knowledge adjusting scheme is explored to dynamically adjust the contribution of different teachers in the distillation process. Comprehensive experiments conducted on CIFAR-10/100 and ImageNet datasets with various network architectures demonstrate the superiorities of MAD over many state-of-the-arts binarization methods. For instance, without introducing any extra inference calculations, our binarized ResNet-18 achieves 1.5% improvement for BirealNet binarization method on ImageNet."
  - id: 872
    order: 220
    oral_session: 5
    poster_session: 3
    session_id: 6
    title: "StyleVideoGAN: A Temporal Generative Model using a Pretrained StyleGAN"
    authors:
      - author: "Gereon Fox (Max Planck Institute for Informatics)"
      - author: "Ayush Tewari (Max Planck Institute for Informatics)"
      - author: "Mohamed Elgharib (Max Planck Institute for Informatics)"
      - author: "Christian Theobalt (MPI Informatik)"
    all_authors: "Gereon Fox, Ayush Tewari, Mohamed Elgharib and Christian Theobalt"
    code: ""
    keywords:
      - word: "video generation"
      - word: "StyleGAN"
      - word: "GAN"
      - word: "embedding"
      - word: "faces"
      - word: "hands"
      - word: "cars"
      - word: "RNN"
    paper: "papers/0872.pdf"
    supp: "supp/0872_supp.zip"
    abstract: "Generative adversarial models (GANs) continue to produce advances in terms of the visual quality of still images, as well as the learning of temporal correlations. However, few works manage to combine these two interesting capabilities for the synthesis of video content: Most methods require an extensive training dataset to learn temporal correlations, while being rather limited in the resolution and visual quality of their output.
We present a novel approach to the video synthesis problem that helps to greatly improve visual quality and drastically reduce the amount of training data and resources necessary for generating videos. Our formulation separates the spatial domain, in which individual frames are synthesized, from the temporal domain, in which motion is generated. 
For the spatial domain we use a pre-trained StyleGAN network, the latent space of which allows control over the appearance of the objects it was trained for. The expressive power of this model allows us to embed our training videos in the StyleGAN latent space. 
Our temporal architecture is then trained not on sequences of RGB frames, but on sequences of StyleGAN latent codes. The advantageous properties of the StyleGAN space simplify the discovery of temporal correlations.
We demonstrate that it suffices to train our temporal architecture on only 10 minutes of footage of  1 subject for about 6 hours. After training, our model can not only generate new portrait videos for the training subject, but also for any random subject which can be embedded in the StyleGAN space.
"
  - id: 874
    order: 182
    poster_session: 2
    session_id: 5
    title: "Meta-learning the Learning Trends Shared Across Tasks"
    authors:
      - author: "Jathushan Rajasegaran ( 	University of Moratuwa)"
      - author: "Salman Khan (MBZUAI/ANU)"
      - author: "Munawar Hayat (Monash University)"
      - author: "Fahad Shahbaz Khan (MBZUAI)"
      - author: "Mubarak Shah (University of Central Florida)"
    all_authors: "Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan and Mubarak Shah"
    code: ""
    keywords:
      - word: "Meta-learning"
      - word: "Few-shot learning"
    paper: "papers/0874.pdf"
    supp: ""
    abstract: "Meta-learning stands for ‘learning to learn’ such that generalization to new tasks is achieved. Among these methods,Gradient-based meta-learning algorithms are a specific sub-class that excel at quick adaptation to new tasks with limited data. This demonstrates their ability to acquire transferable knowledge, a capability that is central to human learning. However, the existing meta-learning approaches only depend on the current task information during the adaptation, and do not share the meta-knowledge of how a similar task has been adapted before. To address this gap, we propose a ‘Path-aware’ model-agnostic meta-learning approach. Specifically, our approach not only learns a good initialization (meta-parameters) for adaptation, it also learns an optimal way to adapt these parameters to a set of task-specific parameters, with learnable update directions, learning rates and, most importantly, the way updates evolve over different time-steps. Our approach is simple to implement and demonstrates faster convergence compared to the competing methods. We report significant performance improvements on a number of datasets for few-shot learning on classification and regression tasks."
  - id: 875
    order: 62
    poster_session: 1
    session_id: 2
    title: "Multi-scale Residual Aggregation Deraining Network with Spatial Context-aware Pooling and Activation"
    authors:
      - author: "Kohei Yamamichi (Yamaguchi University)"
      - author: "Xian-Hua Han (Yamaguchi University)"
    all_authors: "Kohei Yamamichi and Xian-Hua Han"
    code: ""
    keywords:
      - word: "Deraining"
      - word: "Rain removal"
      - word: "Deep residual block"
      - word: "Multi-scale residual aggregation network"
      - word: "Spatial context-aware activation"
      - word: "Spatial context-aware pooling"
      - word: "multi-scale context aggregation module"
      - word: ""
    paper: "papers/0875.pdf"
    supp: "supp/0875_supp.zip"
    abstract: "Single image deraining is a fundamental pre-processing step in many computer vision applications for improving the visual effect and analysis performance of subsequent high-level tasks in adverse weather conditions. This study proposes a novel multi-scale residual aggregation network, to effectively solve the single image deraining problem. Specifically, we exploit a lightweight residual structure subnet with less than 10-layers as the deraining backbone network to extract fine and detailed texture context at the original scale, and leverage a multi-scale context aggregation module (MCAM) to augment the complementary semantic context for enhancing the modeling capability of the overall deraining network. The designed MCAM consists of multiple-resolution feature extraction blocks to capture diverse semantic contexts in different expanded receptive fields, and conducts progressive feature fusion between adjacent scales with residual connections, which is expected to  concurrently disentangle the multi-scale structures of scene content and multiple rain layers in the rainy images, and capture high-level representative feature for reconstructing the clean image. Moreover, motivated by the fact that the adopted pooling operation and activation function in deep learning may considerably affect the prediction performance in high-level vision tasks such as image classification and object detection, we delve into a generalized pooling and activation method taking into consideration of the surrounding spatial context instead of pixel-wise operation and propose the spatial context-aware pooling (SCAP) and activation (SCAA) for incorporating with our deraining network to boost performance. Extensive experiments on the benchmark datasets demonstrate that our proposed method performs favorably against state-of-the-art (SoTA) deraining approaches."
  - id: 876
    order: 183
    poster_session: 2
    session_id: 5
    title: "Grouping Bilinear Pooling"
    authors:
      - author: "Rui Zeng (University of Science and Technology of China)"
      - author: "Jingsong He (University of Science and Technology of China)"
    all_authors: "Rui Zeng and Jingsong He"
    code: "https://github.com/morang22/Grouping-Bilinear-Pooling"
    keywords:
      - word: "Bilinear Pooling"
      - word: "Compact"
      - word: "Fine-grained Classification"
    paper: "papers/0876.pdf"
    supp: ""
    abstract: "Fusion of the extracted high-order features to obtain a better representation by capturing the complex correlation between features has always been a research focus in visual tasks. As a simple and effective high-order feature interaction representation, bilinear representation has achieved remarkable results in many visual tasks: fine-grained image classification, semantic segmentation and so on. However, bilinear pooling has not been widely used due to the bilinear representation up to hundreds of thousands or even millions of dimensions. In this paper, we propose grouping bilinear pooling (GBP) that the representation captured by GBP can achieve the same effect with less than 4% parameters compare with full bilinear representation. This more compact representation largely overcomes the high redundancy of the full bilinear representation, it greatly reduces the computational cost of introducing bilinear pooling in visual tasks. The effectiveness of the proposed GBP is proved by experiments on the widely used fine-grained recognition datasets."
  - id: 877
    order: 390
    poster_session: 4
    session_id: 11
    title: "Continuous Event-Line Constraint for Closed-Form Velocity Initialization"
    authors:
      - author: "Xin Peng (ShanghaiTech University)"
      - author: "Wanting Xu (Shanghaitech University)"
      - author: "Jiaqi Yang (ShanghaiTech University)"
      - author: "Laurent Kneip (ShanghaiTech University)"
    all_authors: "Xin Peng, Wanting Xu, Jiaqi Yang and Laurent Kneip"
    code: ""
    keywords:
      - word: "motion estimation"
      - word: "event camera"
      - word: "geometry constraint"
      - word: "line feature"
    paper: "papers/0877.pdf"
    supp: "supp/0877_supp.zip"
    abstract: "Event cameras trigger events asynchronously and independently upon a sufficient change of the logarithmic brightness level. The neuromorphic sensor has several advantages over standard cameras including low latency, absence of motion blur, and high dynamic range. Event cameras are particularly well suited to sense motion dynamics in agile scenarios. We propose the continuous event-line constraint, which relies on a constant-velocity motion assumption as well as trifocal tensor geometry in order to express a relationship between line observations given by event clusters as well as first-order camera dynamics. Our core result is a closed-form solver for up-to-scale linear camera velocity {with known angular velocity}. Nonlinear optimization is adopted to improve the performance of the algorithm. The feasibility of the approach is demonstrated through a careful analysis on both simulated and real data."
  - id: 878
    order: 184
    poster_session: 2
    session_id: 5
    title: "Temporal Alignment via Event Boundary for Few-shot Action Recongnition"
    authors:
      - author: "Shuyuan Li (Shanghai Jiao Tong University)"
      - author: "Huabin Liu (Shanghai Jiao Tong University)"
      - author: "Mengjuan Fei (Huawei Cloud)"
      - author: "Xiaoyuan Yu (Huawei Cloud)"
      - author: "Weiyao Lin (Shanghai Jiao Tong university)"
    all_authors: "Shuyuan Li, Huabin Liu, Mengjuan Fei, Xiaoyuan Yu and Weiyao Lin"
    code: ""
    keywords:
      - word: "few-shot action recognition"
      - word: "temporal alignment"
      - word: "event boundary"
      - word: ""
    paper: "papers/0878.pdf"
    supp: "supp/0878_supp.zip"
    abstract: "Few-shot action recognition aims to recognize novel action classes using just a few samples as knowledge. Most of the recent approaches learn to compare the similarity between videos. Recently, it has been observed that directly measuring this similarity is not ideal since the action instance cannot well aligned among videos.  In this paper, we leverage the novel event boundary information to guide alignment learning in few-shot action recognition. First, a novel frame sampling strategy based on temporal boundaries is proposed to relieve the intra-class variance. Second, we propose a boundary selection module to locate the start & end time of action and further align videos to their duration. Ablation studies and visualizations demonstrate the effectiveness of the proposed methods. Extensive experiments on benchmark datasets show the potential of the proposed method in achieving state-of-the-art performance for few-shot action recognition."
  - id: 890
    order: 185
    poster_session: 2
    session_id: 5
    title: "Siamese Prototypical Contrastive Learning"
    authors:
      - author: "Shentong Mo (Carnegie Mellon University)"
      - author: "Zhun Sun (Tohoku University)"
      - author: "Chao Li (RIKEN)"
    all_authors: "Shentong Mo, Zhun Sun and Chao Li"
    code: ""
    keywords:
      - word: "self-supervised learning"
      - word: "contrastive learning"
      - word: "representation learning"
    paper: "papers/0890.pdf"
    supp: "supp/0890_supp.zip"
    abstract: "Contrastive Self-supervised Learning (CSL) is a practical solution that learns meaningful visual representations from massive data in an unsupervised approach. The ordinary CSL embeds the features extracted from neural networks onto specific topological structures. During the training progress, the contrastive loss draws the different views of the same input together while pushing the embeddings from different inputs apart. One of the drawbacks of CSL is that the loss term requires a large number of negative samples to provide better mutual information bound ideally. However, increasing the number of negative samples by larger running batch size also enhances the effects of false negatives: semantically similar samples are pushed apart from the anchor, hence downgrading downstream performance. In this paper, we tackle this problem by introducing a new prototypical learning framework. The key insight is to employ siamese-style metric loss to match intra-prototype features, while increasing the distance between inter-prototype features. We conduct extensive experiments on various benchmarks where the results demonstrate the effectiveness of our method on improving the quality of visual representations. Specifically, our unsupervised pre-trained ResNet-50 with a linear probe, out-performs the fully-supervised trained version on the ImageNet-1K dataset."
  - id: 894
    order: 63
    poster_session: 1
    session_id: 2
    title: "Global Context and Geometric Priors for Effective Non-Local Self-Attention"
    authors:
      - author: "Sanghyun Woo (KAIST)"
      - author: "Dahun Kim (KAIST)"
      - author: "Joon-Young Lee (Adobe Research)"
      - author: "In So Kweon (KAIST)"
    all_authors: "Sanghyun Woo, Dahun Kim, Joon-Young Lee and In So Kweon"
    code: ""
    keywords:
      - word: "self-attention"
      - word: "non-local attention"
      - word: "attention"
      - word: "transformer"
      - word: "context"
      - word: "position encoding"
      - word: ""
    paper: "papers/0894.pdf"
    supp: "supp/0894_supp.zip"
    abstract: "Capturing relationships among local and global features in an image is crucial for visual understanding. However, the convolution operation is inherently limited at utilizing long-range information due to its small receptive field. Existing approaches thus heavily rely on the non-local network strategies to make up for the locality of convolutional features. Despite their successful applications in various tasks, we propose there is still considerable room for improvement, by exploring the effectiveness of global image context and position-aware representations. Notably, the concept of relative position is surprisingly under-explored in the vision domain, whereas it has proven to be useful for modeling dependencies in machine translation tasks.  In this paper, we propose a new relational reasoning module, which incorporates a contextualized diagonal matrix and a 2D relative position representations. While being simple and flexible, our module allows the relational representation of a feature point to encode the whole image context and its relative position information. We also explore multi-head and dropout strategies to improve the relation learning further. Extensive experiments show our module consistently improve over the state-of-the-art baselines on different vision tasks, including detection, instance segmentation, semantic segmentation, and panoptic segmentation. The code and models will be released."
  - id: 896
    order: 186
    poster_session: 2
    session_id: 5
    title: "SPARROW: Semantically Coherent Prototypes for Image Classification"
    authors:
      - author: "Stefan Kraft (IT-Designers Gruppe)"
      - author: "Klaus Broelemann (Schufa Holding AG)"
      - author: "Andreas Theissler (Aalen University of Applied Sciences)"
      - author: "Gjergji Kasneci ( 	University of Tuebingen)"
    all_authors: "Stefan Kraft, Klaus Broelemann, Andreas Theissler and Gjergji Kasneci"
    code: ""
    keywords:
      - word: "prototype classification"
      - word: "semantically coherent explanations"
      - word: "interpretability evaluation"
      - word: "image recognition"
      - word: "concept-based explanations"
      - word: "object part semantics"
      - word: "intrinsically interpretable models"
      - word: "representation learning"
      - word: "interpretable machine learning"
      - word: "xai"
    paper: "papers/0896.pdf"
    supp: "supp/0896_supp.zip"
    abstract: "Current prototype-based classification often leads to prototypes with overlapping semantics where several prototypes are similar to the same image parts. Also, single prototypes tend to activate highly on a mixture of semantically different image parts. This impedes interpretability since the nature of the connections between the parts is unknown. We propose a framework that is comprised of two key elements: (i) A novel method which leads to semantically coherent prototypes and (ii) an evaluation protocol which is based on part annotations and allows to quantitatively compare the explanatory capacity of prototypes from different methods. We demonstrate the viability of our framework by comparing our method to a standard prototype-based classification method and show that our method is capable of producing prototypes of superior interpretability."
  - id: 900
    order: 287
    poster_session: 3
    session_id: 8
    title: "Quantum Unsupervised Domain Adaptation: Does Entanglement Help?"
    authors:
      - author: "Nanqing Dong (University of Oxford)"
      - author: "Michael C. Kampffmeyer (UiT The Arctic University of Norway)"
      - author: "Irina D Voiculescu (University of Oxford)"
    all_authors: "Nanqing Dong, Michael C. Kampffmeyer and Irina D Voiculescu"
    code: ""
    keywords:
      - word: "quantum machine learning"
      - word: "quantum neural network"
      - word: "unsupervised domain adptation"
    paper: "papers/0900.pdf"
    supp: ""
    abstract: "Fueled by the recent breakthroughs in quantum theory and hardware, it has been demonstrated that computer vision (CV) tasks can be solved in quantum computers via the emerging quantum machine learning (QML). In contrast to classical computing, quantum computing relies on the entanglement between qubits to communicate. While many conventional machine learning (ML) tasks have been well-studied, there are still ML problems to be solved on quantum data. One of this challenges is the domain shift on quantum data. In this work, we aim to understand the role of entanglement in mitigating the domain shift in a quantum domain. We formulate quantum unsupervised domain adaptation (QUDA) for the first time and, to address the domain shift existing in quantum data, propose quantum adversarial domain adaptation (QADA). Limited by the capacity of the current quantum devices, we lay the groundwork for a computation-efficient quantum system that implements QADA in a simple manner. QADA integrates two quantum neural networks (QNNs), including a quantum classifier and a quantum discriminator. Two QNNs are trained in a hybrid classical-quantum platform with an adversarial strategy. We evaluate QADA on unsupervised domain adaptation tasks between the MNIST and SVHN image datasets under a quantum setting. Our simulated experiments show that QADA can be used to mitigate the domain shift in a quantum device, which further validates that the entanglement in a quantum circuit model can be used to achieve QUDA."
  - id: 902
    order: 391
    poster_session: 4
    session_id: 11
    title: "IB-MVS: An Iterative Algorithm for Deep Multi-View Stereo based on Binary Decisions"
    authors:
      - author: "Christian Sormann (Graz University of Technology)"
      - author: "Mattia Rossi (SONY Europe B.V.)"
      - author: "Andreas Kuhn (SONY Europe)"
      - author: "Friedrich Fraundorfer (Graz University of Technology)"
    all_authors: "Christian Sormann, Mattia Rossi, Andreas Kuhn and Friedrich Fraundorfer"
    code: ""
    keywords:
      - word: "multi-view stereo"
      - word: "mvs"
      - word: "iterative algorithm"
      - word: "binary decisions"
      - word: "deep multi-view stereo"
      - word: "deep mvs"
      - word: "depth estimation"
      - word: "3d reconstruction"
    paper: "papers/0902.pdf"
    supp: "supp/0902_supp.zip"
    abstract: "We present a novel deep-learning-based method for Multi-View Stereo. Our method estimates high resolution and highly precise depth maps iteratively, by traversing the continuous space of feasible depth values at each pixel in a binary decision fashion. The decision process leverages a deep-network architecture: this computes a pixelwise binary mask that establishes whether each pixel actual depth is in front or behind its current iteration individual depth hypothesis. Moreover, in order to handle occluded regions, at each iteration the results from different source images are fused using pixelwise weights estimated by a second network. Thanks to the adopted binary decision strategy, which permits an efficient exploration of the depth space, our method can handle high resolution images without trading resolution and precision. This sets it apart from most alternative learning-based Multi-View Stereo methods, where the explicit discretization of the depth space requires the processing of large cost volumes. We compare our method with state-of-the-art Multi-View Stereo methods on the DTU, Tanks and Temples and the challenging ETH3D benchmarks and show competitive results."
  - id: 904
    order: 64
    poster_session: 1
    session_id: 2
    title: "LUCES: A Dataset for Near-Field Point Light Source Photometric Stereo"
    authors:
      - author: "Roberto Mecca (cambridge university)"
      - author: "Fotios Logothetis (Toshiba research)"
      - author: "Ignas Budvytis (Department of Engineering, University of Cambridge)"
      - author: "Roberto Cipolla (University of Cambridge)"
    all_authors: "Roberto Mecca, Fotios Logothetis, Ignas Budvytis and Roberto Cipolla"
    code: "http://www.robertomecca.com/psdataset.html"
    keywords:
      - word: "photometric stereo"
      - word: "dataset"
      - word: "near field photometric stereo"
      - word: "point light calibration"
    paper: "papers/0904.pdf"
    supp: "supp/0904_supp.zip"
    abstract: "Three-dimensional reconstruction of objects from shading information is a challenging task in computer vision. As most of the approaches facing the Photometric Stereo problem use simplified far-field assumptions, real-world scenarios have essentially more complex physical effects that need to be handled for accurately reconstructing the 3D shape. An increasing number of methods have been proposed to address the problem when point light sources are assumed to be nearby the target object.

To understand the capability of the approaches dealing with this near-field scenario, the literature till now has used synthetically rendered photometric images or minimal and very customised real-world data. In order to fill the gap in evaluating near-field photometric stereo methods, we introduce LUCES the first real-world 'dataset for near-fieLd point light soUrCe photomEtric Stereo' of 14 objects of different materials. 52 LEDs have been used to lit each object positioned 10 to 30 centimeters away from the camera. Together with the raw images, in order to evaluate the 3D reconstructions, the dataset includes both normal and depth maps for comparing different features of the retrieved 3D geometry. Furthermore, we evaluate the performance of the latest near-field Photometric Stereo algorithms on the proposed dataset to assess the state-of-the-art method with respect to actual close range effects and object materials. "
  - id: 906
    order: 65
    poster_session: 1
    session_id: 2
    title: "Beyond Classification: Knowledge Distillation using Multi-Object Impressions"
    authors:
      - author: "Gaurav Kumar Nayak (Indian Institute of Science, Bangalore)"
      - author: "Monish K Keswani (Indian institute of Science)"
      - author: "Sharan  Seshadri  (Carnegie Mellon University)"
      - author: "Anirban Chakraborty (Indian Institute of Science)"
    all_authors: "Gaurav Kumar Nayak, Monish K Keswani, Sharan  Seshadri and Anirban Chakraborty"
    code: ""
    keywords:
      - word: "Knowledge Distillation (KD)"
      - word: "zero-shot"
      - word: "data-free"
      - word: "object detection"
      - word: "data privacy"
      - word: "multi-object impressions"
      - word: "pseudo-data"
      - word: "pseudo-targets"
      - word: "synthetic data"
      - word: "Faster RCNN"
      - word: ""
    paper: "papers/0906.pdf"
    supp: "supp/0906_supp.zip"
    abstract: "Knowledge Distillation (KD) utilizes training data as a transfer set to transfer knowledge from a complex network (Teacher) to a smaller network (Student).  Several works have recently identified many scenarios where the training data may not be available due to data privacy or sensitivity concerns and have proposed solutions under this restrictive constraint for the classification task.  Unlike existing works, we, for the first time, solve a much more challenging problem, i.e., “KD for object detection with zero knowledge about the training data and its statistics”. Our proposed approach prepares pseudo-targets and synthesizes corresponding samples (termed as “Multi-Object Impressions”), using only the pretrained Faster RCNN Teacher network. We use this pseudo-dataset as a transfer set to conduct zero-shot KD for object detection.  We demonstrate the efficacy of our proposed method through several ablations and extensive experiments on benchmark datasets like KITTI, Pascal and COCO. Our approach with no training samples, achieves a respectable mAP of 64.2% and 55.5% on the student with same and half capacity while performing distillation from a Resnet-18 Teacher of 73.3% mAP on KITTI."
  - id: 908
    order: 187
    poster_session: 2
    session_id: 5
    title: "PropMix: Hard Sample Filtering and Proportional MixUp for Learning with Noisy Labels"
    authors:
      - author: "Filipe Rolim Cordeiro (Universidade Federal Rural de Pernambuco)"
      - author: "Vasileios Belagiannis (Universität Ulm)"
      - author: "Ian Reid (University of Adelaide, Australia)"
      - author: "Gustavo Carneiro (University of Adelaide)"
    all_authors: "Filipe Rolim Cordeiro, Vasileios Belagiannis, Ian Reid and Gustavo Carneiro"
    code: "https://github.com/filipe-research/PropMix"
    keywords:
      - word: "noisy labels"
      - word: "noisy annotation"
      - word: "Mixup"
      - word: "hard samples"
      - word: "noisy samples"
      - word: "noisy training"
      - word: ""
    paper: "papers/0908.pdf"
    supp: "supp/0908_supp.zip"
    abstract: "The most competitive noisy label learning methods rely on an unsupervised classification of clean and noisy samples, where samples classified as noisy are re-labelled and \"MixMatched\" with the clean samples.
These methods have two issues in large noise rate problems: 1) the noisy set is more likely to contain hard samples that are incorrectly re-labelled, and 2) the number of samples produced by MixMatch tends to be reduced because it is constrained by the small clean set size. In this paper, we introduce the learning algorithm PropMix to handle the issues above. PropMix filters out hard noisy samples, with the goal of increasing the likelihood of correctly re-labelling the easy noisy samples. Also, PropMix places clean and re-labelled easy noisy samples in a training set that is augmented with MixUp, removing the clean set size constraint and including a large proportion of correctly re-labelled easy noisy samples. We also include self-supervised pre-training to improve robustness to high noisy label scenarios. Our experiments show that PropMix has state-of-the-art (SOTA) results on CIFAR-10/-100 (with symmetric, asymmetric and semantic label noise), Red Mini-ImageNet (from the Controlled Noisy Web Labels), and  WebVision. In severe label noise benchmarks, our results are substantially better than other methods. The code is available at https://github.com/filipe-research/PropMix."
  - id: 911
    order: 392
    poster_session: 4
    session_id: 11
    title: "On Automatic Data Augmentation for 3D Point Cloud Classification"
    authors:
      - author: "Wanyue Zhang (ASTAR)"
      - author: "Xun Xu (South China University of Technology)"
      - author: "Fayao Liu (ASTAR)"
      - author: "Le Zhang (University of Electronic Science and Technology of China)"
      - author: "Chuan Sheng Foo (Institute for Infocomm Research, ASTAR)"
    all_authors: "Wanyue Zhang, Xun Xu, Fayao Liu, Le Zhang and Chuan Sheng Foo"
    code: ""
    keywords:
      - word: "point cloud"
      - word: "automatic data augmentation"
    paper: "papers/0911.pdf"
    supp: "supp/0911_supp.zip"
    abstract: "Data augmentation is an important technique to reduce overfitting and improve learn- ing performance, but existing works on data augmentation for 3D point cloud data are based on heuristics. In this work, we instead propose to automatically learn a data aug- mentation strategy using bilevel optimization. An augmentor is designed in a similar fashion to a conditional generator and is optimized by minimizing a base model’s loss on a validation set when the augmented input is used for training the model. This formula- tion provides a more principled way to learn data augmentation on 3D point clouds. We evaluate our approach on standard point cloud classification tasks and a more challenging setting with pose misalignment between training and validation/test sets. The proposed strategy achieves competitive performance on both tasks and we provide further insight into the augmentor’s ability to learn the validation set distribution."
  - id: 912
    order: 188
    poster_session: 2
    session_id: 5
    title: "Incremental Learning for Animal Pose Estimation using RBF k-DPP"
    authors:
      - author: "Gaurav Kumar Nayak (Indian Institute of Science, Bangalore)"
      - author: "Het Shah (Birla Institute of Technology and Science Pilani, K. K. Birla Goa Campus)"
      - author: "Anirban Chakraborty (Indian Institute of Science)"
    all_authors: "Gaurav Kumar Nayak, Het Shah and Anirban Chakraborty"
    code: ""
    keywords:
      - word: "animal pose estimation"
      - word: "incremental learning"
      - word: "Determinantal Point Processes"
      - word: "k-DPP"
      - word: "RBF k-DPP"
      - word: "image warping"
      - word: "exemplar memory"
      - word: ""
    paper: "papers/0912.pdf"
    supp: "supp/0912_supp.zip"
    abstract: "Pose estimation is the task of locating keypoints for an object of interest in an image. Animal Pose estimation is more challenging than estimating human pose due to high inter and intra class variability in animals. Existing works solve this problem for a fixed set of predefined animal categories. Models trained on such sets usually do not work well with new animal categories. Retraining the model on new categories makes the model overfit and leads to catastrophic forgetting. Thus, in this work, we propose a novel problem of ''Incremental Learning for Animal Pose Estimation''. Our method uses an exemplar memory, sampled using Determinantal Point Processes (DPP) to continually adapt to new animal categories without forgetting the old ones. We further propose a new variant of k-DPP that uses RBF kernel (termed as ''RBF k-DPP'') which gives more gain in performance over traditional k-DPP. Due to memory constraints, the limited number of exemplars along with new class data can lead to class imbalance. We mitigate it by performing image warping as an augmentation technique. This helps in crafting diverse poses, which reduces overfitting and yields further improvement in performance. The efficacy of our proposed approach is demonstrated via extensive experiments and ablations where we obtain significant improvements over state-of-the-art baseline methods."
  - id: 921
    order: 189
    poster_session: 2
    session_id: 5
    title: "TridentAdapt: Learning Domain-invariance via Source-Target Confrontation and Self-induced Cross-domain Augmentation"
    authors:
      - author: "Fengyi Shen (Technical University of Munich)"
      - author: "Akhil Gurram (HUAWEI Technologies Co. Ltd)"
      - author: "Ahmet Faruk Tuna (Huawei)"
      - author: "Onay Urfalioglu (HUAWEI Technologies Co. Ltd)"
      - author: "Alois C. Knoll (Robotics and Embedded Systems)"
    all_authors: "Fengyi Shen, Akhil Gurram, Ahmet Faruk Tuna, Onay Urfalioglu and Alois C. Knoll"
    code: "https://github.com/HMRC-AEL/TridentAdapt"
    keywords:
      - word: "domain adaptation"
      - word: "semantic segmentation"
      - word: "autonomous driving"
      - word: "image-to-image translation"
      - word: "unsupervised learning"
      - word: "domain transfer"
      - word: "scene understanding"
      - word: "urban scene segmentation"
      - word: ""
    paper: "papers/0921.pdf"
    supp: "supp/0921_supp.zip"
    abstract: "Due to the difficulty of obtaining ground-truth labels, learning from virtual-world datasets is of great interest for real-world applications like semantic segmentation. From domain adaptation perspective, the key challenge is to learn domain-agnostic representation of the inputs in order to benefit from virtual data. 

In this paper, we propose a novel trident-like architecture that enforces a shared feature encoder to satisfy confrontational source and target constraints simultaneously, thus learning a domain-invariant feature space. Moreover, we also introduce a novel training pipeline enabling self-induced cross-domain data augmentation during the forward pass. This contributes to a further reduction of the domain gap. Combined with a self-training process, we obtain state-of-the-art results on benchmark datasets (e.g. GTA5 or Synthia to Cityscapes adaptation). Code and pre-trained models available online at https://github.com/HMRC-AEL/TridentAdapt "
  - id: 926
    order: 393
    poster_session: 4
    session_id: 11
    title: "Unified 3D Mesh Recovery of Humans and Animals by Learning Animal Exercise"
    authors:
      - author: "Youwang Kim (POSTECH)"
      - author: "Ji-Yeon Kim (POSTECH)"
      - author: "Kyungdon Joo (UNIST)"
      - author: "Tae-Hyun Oh (POSTECH)"
    all_authors: "Youwang Kim, Ji-Yeon Kim, Kyungdon Joo and Tae-Hyun Oh"
    code: ""
    keywords:
      - word: "single view mesh reconstruction"
      - word: "multi-task learning"
      - word: "unified mesh recovery"
      - word: "human pose estimation"
      - word: "animal pose estimation"
      - word: "morphological similarity"
      - word: "parametric mesh model"
      - word: "weakly-supervised learning"
      - word: "multi-class mesh reconstruction"
      - word: "deformable objects"
      - word: ""
    paper: "papers/0926.pdf"
    supp: "supp/0926_supp.zip"
    abstract: "We propose an end-to-end unified 3D mesh recovery of humans and quadruped animals trained in a weakly-supervised way.  Unlike recent work focusing on a single target class only, we aim to recover 3D mesh of broader classes with a single multi-task model. However, there exists no dataset that can directly enable multi-task learning due to the absence of both human and animal annotations for a single object, e.g., a human image does not have animal pose annotations; thus, we have to devise a new way to exploit heterogeneous datasets.  To make the unstable disjoint multi-task learning jointly trainable, we propose to exploit the morphological similarity between humans and animals, motivated by animal exercise where humans imitate animal poses. We realize the morphological similarity by semantic correspondences, called sub-keypoint, which enables joint training of human and animal mesh regression branches. Besides, we propose class-sensitive regularization methods to avoid a mean-shape bias and to improve the distinctiveness across multi-classes. Our method performs favorably against recent uni-modal models on various human and animal datasets while being far more compact."
  - id: 929
    order: 394
    poster_session: 4
    session_id: 11
    title: "Weakly Supervised Training of Monocular 3D Object Detectors Using Wide Baseline Multi-view Traffic Camera Data"
    authors:
      - author: "Matthew R Howe (The University of Adelaide)"
      - author: "Ian Reid (University of Adelaide, Australia)"
      - author: "Jamie Mackenzie (The University of Adelaide)"
    all_authors: "Matthew R Howe, Ian Reid and Jamie Mackenzie"
    code: "https://github.com/MatthewHowe/WIBAM_public"
    keywords:
      - word: "3D object detection"
      - word: "weak supervision"
      - word: "weakly supervised"
      - word: "multi-view"
      - word: "monocular 3D object detection"
      - word: "monocular"
      - word: "wide baseline multi-view"
      - word: ""
    paper: "papers/0929.pdf"
    supp: "supp/0929_supp.zip"
    abstract: "Accurate 7DoF prediction of vehicles at an intersection is an important task for assessing potential conflicts between road users. In principle, this could be achieved by a single camera system that is capable of detecting the pose of each vehicle but this would require a large, accurately labelled dataset from which to train the detector. Although large vehicle pose datasets exist (ostensibly developed for autonomous vehicles), we find training on these datasets inadequate. These datasets contain images from a ground level viewpoint, whereas an ideal view for intersection observation would be elevated higher above the road surface. 
We develop an alternative approach using a weakly supervised method of fine tuning 3D object detectors for traffic observation cameras; showing in the process that large existing autonomous vehicle datasets can be leveraged for pre-training. 
To fine-tune the monocular 3D object detector, our method utilises multiple 2D detections from overlapping, wide-baseline views and a loss that encodes the subjacent geometric consistency.
Our method achieves vehicle 7DoF pose prediction accuracy on our dataset comparable to the top performing monocular 3D object detectors on autonomous vehicle datasets. We present our training methodology, multi-view reprojection loss, and dataset."
  - id: 930
    order: 0
    oral_session: 1
    poster_session: 1
    session_id: 0
    title: "Audio-Visual Speech Super-Resolution"
    authors:
      - author: "Rudrabha Mukhopadhyay (IIIT Hyderabad)"
      - author: "Sindhu B Hegde (International Institute of Information Technology (IIIT) Hyderabad)"
      - author: "Vinay Namboodiri (University of Bath)"
      - author: "C.V. Jawahar (IIIT-Hyderabad)"
    all_authors: "Rudrabha Mukhopadhyay, Sindhu B Hegde, Vinay Namboodiri and C.V. Jawahar"
    code: "http://cvit.iiit.ac.in/research/projects/cvit-projects/audio-visual-speech-super-resolution"
    keywords:
      - word: "speech super-resolution"
      - word: "audio-visual data"
      - word: "audio-visual learning"
      - word: "pseudo-visual stream"
      - word: "multi-modal learning"
    paper: "papers/0930.pdf"
    supp: "supp/0930_supp.zip"
    abstract: "In this paper, we present an audio-visual model to perform speech super-resolution at large scale-factors (8x and 16x). Previous works attempted to solve this problem using only the audio modality as input, and thus were limited to low scale-factors of 2x and 4x. In contrast, we propose to incorporate both visual and auditory signals to super-resolve speech of sampling rates as low as 1kHz. In such challenging situations, the visual features assist in learning the content, and improves the quality of the generated speech. Further, we demonstrate the applicability of our approach to arbitrary speech signals where the visual stream is not accessible. Our \"pseudo-visual network\" precisely synthesizes the visual stream solely from the low-resolution speech input. Extensive experiments illustrate our method's remarkable results and benefits over state-of-the-art audio-only speech super-resolution approaches. Our project website can be found at http://cvit.iiit.ac.in/research/projects/cvit-projects/audio-visual-speech-super-resolution."
  - id: 931
    order: 395
    poster_session: 4
    session_id: 11
    title: "Discriminative Clue Alignment Network for Both Image- and Video-Based Person Re-Identification"
    authors:
      - author: "Panwen Hu (The Chinese University of Hong Kong,Shenzhen)"
      - author: "Xinyu Zhou (The Chinese University of Hong Kong, Shenzhen)"
      - author: "Rui Huang (The Chinese University of Hong Kong, Shenzhen)"
    all_authors: "Panwen Hu, Xinyu Zhou and Rui Huang"
    code: ""
    keywords:
      - word: "person reidentification"
      - word: "feature alignment"
      - word: "multiple attention"
    paper: "papers/0931.pdf"
    supp: ""
    abstract: "Although great progress has been made in both Image-based person Re-IDentification (IReID) and Video-based person Re-IDentification (VReID), the misalignment problem resulted from some complex factors, e.g., variations of pose, non-rigid deformation of the body, etc., still makes these two tasks very challenging. Some recent IReID studies employ the human parsing model or self-attention mechanisms to capture the human part features or emphasize the Discriminative Clues (DCs), whereas these methods heavily rely on the external annotations, and the DCs are not aligned well. Moreover, these IReID models lack generalization ability and cannot achieve promising performances on the VReID benchmarks yet. To this end, we propose a Discriminative Clue Alignment Network (DCANet), along with a discrimination constraint, to automatically identify various DCs and then align them into a fixed pattern, without requiring additional annotations. The experiments on three popular VReID benchmarks show that even with a simple temporal feature aggregation method, DCANet can still achieve state-of-the-art performances. Moreover, the evaluation on the public IReID datasets also verifies that our method is very competitive."
  - id: 936
    order: 396
    poster_session: 4
    session_id: 11
    title: "3D Flow Field Estimation around a Vehicle Using Convolutional Neural Networks"
    authors:
      - author: "Fangge Chen (Nissan Research Center)"
      - author: "Kei Akasaka (Nissan Motor Co., LTD)"
    all_authors: "Fangge Chen and Kei Akasaka"
    code: ""
    keywords:
      - word: "Vehicle"
      - word: "Aerodynamics"
      - word: "3D Convolutional Neural Networks"
      - word: "Application"
    paper: "papers/0936.pdf"
    supp: "supp/0936_supp.zip"
    abstract: "Flow fields, including velocity and pressure fields, are typically used as references for vehicle shape design in the automotive industry to ensure steering stability and energy conservation. Generally, flow fields are calculated using computational fluid dynamics (CFD) simulations which is time-consuming and expensive. Therefore, a more efficient and interactive method is desired by designers for advanced shape discussion and design. To this end, we propose a fast estimation model using 3D convolutional neural networks. We employ a style extractor to obtain sufficient deep features of each vehicle shape and apply them using adaptive instance normalisation to improve the estimation performance. In addition, a proposed loss function which mainly includes a slice-weighted loss function is used to train the estimation model. The findings show that our proposed method outperforms previous studies, especially on flow field estimation in wake regions and regions near the vehicle surface. Therefore, the proposed method allows designing vehicle shapes while ensuring desirable aerodynamic performance within a much shorter period than extended CFD simulations."
  - id: 945
    order: 66
    poster_session: 1
    session_id: 2
    title: "Rethinking local and global feature representation for semantic segmentation"
    authors:
      - author: "Mohan Chen (Fudan University)"
      - author: "Xinxuan Zhao (Fudan University)"
      - author: "Bingfei Fu (Fudan university)"
      - author: "Li Zhang (Fudan University)"
      - author: "Xiangyang Xue (Fudan University)"
    all_authors: "Mohan Chen, Xinxuan Zhao, Bingfei Fu, Li Zhang and Xiangyang Xue"
    code: ""
    keywords:
      - word: "Semantic Segmentation"
      - word: "Transformer"
    paper: "papers/0945.pdf"
    supp: "supp/0945_supp.zip"
    abstract: "Although fully convolution networks (FCN) have dominated semantic segmentation since the birth of, they are inherently limited in capturing long-range structured relationship with the layers of local kernels. While recent Transformer-based models have proven extremely successful in computer vision tasks by capturing global representation, they would deteriorate semantic segmentation by over-smoothing the regions contain fine details (e.g., boundaries and small objects). To this end, we propose a Dual-Stream Convolution-Transformer segmentation framework, called DSCT, by taking advantage of both the convolution and Transformer to learn a rich feature representation for semantic segmentation. Specifically, DSCT extracts high resolution local feature information from convolution layers and global feature representation across the Transformer layers. Moreover, a feature fusion module is plugged to exchange information between spatial stream and context stream at each stage. With the local and global context modeled explicitly in every layer, the two streams can be combined with a simple decoder to provide a powerful segmentation model. Extensive experiments show that our model builds a new state of the art on Cityscapes dataset (83.31% mIoU) with only 80K training iterations and appealing performance (49.27% mIoU) on ADE20K, outperforming most of the alternatives with a new perspective."
  - id: 946
    order: 190
    poster_session: 2
    session_id: 5
    title: "Coloured Edge Maps for Oil Palm Ripeness Classification"
    authors:
      - author: "Wei Yuen Teh (Priority Dynamics)"
      - author: "Ian Tan (Monash University Malaysia)"
    all_authors: "Wei Yuen Teh and Ian Tan"
    code: ""
    keywords:
      - word: "oil palm ripeness"
      - word: "edge maps"
      - word: "convolutional neural networks"
      - word: "image classification"
    paper: "papers/0946.pdf"
    supp: ""
    abstract: "The task of grading oil palm bunches by ripeness poses a number of significant challenges for computer vision. The small difference in hue between ripe and unripe bunches means that colour-based models are susceptible to errors when presented with images shot in novel lighting conditions. In this paper, we investigate the effectiveness and performance characteristics of coloured edge maps when used as an input feature to a Convolutional Neural Network (CNN) by comparing the Laplacian of Gaussian, Sobel, Prewitt, and Kirsch edge extraction techniques. We show that under normal lighting conditions, coloured edge maps are able to match the performance of fully-coloured images. More notably, they significantly outperform fully-coloured images when variance to lighting is applied. When images are darkened or brightened, classification accuracy for fully-coloured images drops by 19.89% vs only 4.97% on average for the coloured edge map methods tested. This is of major benefit in commercial applications, where images are often captured by a multitude of devices under different lighting conditions, leading to potentially unreliable performance when fully-coloured images are used."
  - id: 948
    order: 397
    poster_session: 4
    session_id: 11
    title: "DeepUME: Learning the Universal Manifold Embedding for Robust Point Cloud Registration"
    authors:
      - author: "Natalie Lang ( Ben-Gurion University of the Negev)"
      - author: "Joseph M Francos (Ben Gurion University)"
    all_authors: "Natalie Lang and Joseph M Francos"
    code: "https://github.com/langnatalie/DeepUME"
    keywords:
      - word: "Rigid Transformation"
      - word: "Registration"
      - word: "Parameter Estimation"
      - word: "3D Computer Vision"
      - word: "3D point clouds"
      - word: "3D feature learning"
      - word: "Neural networks for 3D data analysis"
      - word: ""
    paper: "papers/0948.pdf"
    supp: "supp/0948_supp.zip"
    abstract: "Registration of point clouds related by rigid transformations is one of the fundamental problems in computer vision. However, a solution to the practical scenario of aligning sparsely and differently sampled observations in the presence of noise is still lacking. We approach registration in this scenario with a fusion of the closed-form Universal Manifold Embedding (UME) method and a deep neural network. The two are combined into a single unified framework, named DeepUME, trained end-to-end and in an unsupervised manner. To successfully provide a global solution in the presence of large transformations, we employ an SO(3)-invariant coordinate system to learn both a joint-resampling strategy of the point clouds and SO(3)-invariant features. These features are then utilized by the geometric UME method for transformation estimation. The parameters of
DeepUME are optimized using a metric designed to overcome an ambiguity problem emerging in the registration of symmetric shapes, when noisy scenarios are considered. We show that our hybrid method outperforms state-of-the-art registration methods in various scenarios, and generalizes well to unseen data sets. Our code is publicly available."
  - id: 959
    order: 67
    poster_session: 1
    session_id: 2
    title: "Faster-FCoViAR: Faster Frequency-Domain Compressed Video Action Recognition"
    authors:
      - author: "Lu Xiong (Beijing University of Posts and Telecommunications)"
      - author: "Xia Jia (ZTE corpration)"
      - author: "Yue Ming (BUPT)"
      - author: "Jiangwan Zhou (Beijing University of Posts and Telecommunications)"
      - author: "Fan Feng (BUPT)"
      - author: "Nan nan Hu (Beijing University of Posts and Telecommunications)"
    all_authors: "Lu Xiong, Xia Jia, Yue Ming, Jiangwan Zhou, Fan Feng and Nan nan Hu"
    code: ""
    keywords:
      - word: "action recognition"
      - word: "frequency-domain"
      - word: "compressed videos"
      - word: "teacher-student network"
    paper: "papers/0959.pdf"
    supp: ""
    abstract: "Human action recognition (HAR) is an essential task in computer vision, which still faces the critical challenge of reducing the data redundancy of decompressed video frames and extracting identification information. To address this challenge, we propose a novel faster frequency-domain compressed video action recognition framework (termed Faster-FCoViAR), which consists of a frequency-domain partial decompression method (FPDec), a frequency-domain channel selection strategy (FCS), and a spatial-to-frequency domain student-teacher network (S2FNet). The FPDec obtains frequency-domain DCT coefficients of compressed videos directly without inverse discrete cosine transform (IDCT) for decompression. The FCS down-samples frequency-domain data to enhance the saliency of input. The S2FNet transfers spatial semantic knowledge from a spatial teacher network to a light-weight student network in the frequency domain, and it thus improves the spatial feature extraction ability of the frequency-domain network. Experiments on datasets UCF-101, HMDB-51, and Kinetics-400 show that our Faster-FCoViAR is 12.3 times faster than the frame-based methods and 6.7 times faster than other compressed domain methods based on competitive recognition accuracy compared with the state-of-the-art action recognition methods."
  - id: 971
    order: 288
    poster_session: 3
    session_id: 8
    title: "A cappella: Audio-visual Singing Voice Separation"
    authors:
      - author: "Juan Felipe Montesinos (Universitat Pompeu Fabra)"
      - author: "Venkatesh Shenoy Kadandale (Universitat Pompeu Fabra)"
      - author: "Gloria Haro (Universitat Pompeu Fabra)"
    all_authors: "Juan Felipe Montesinos, Venkatesh Shenoy Kadandale and Gloria Haro"
    code: "https://github.com/JuanFMontesinos/Acappella-YNet"
    keywords:
      - word: "audiovisual"
      - word: "audio-visual"
      - word: "source separation"
      - word: "singing"
      - word: "speech"
      - word: "graph"
      - word: "acappella"
    paper: "papers/0971.pdf"
    supp: "supp/0971_supp.zip"
    abstract: "The task of isolating a target singing voice in music videos has useful applications. In this work, we explore the single-channel singing voice separation problem from a multimodal perspective, by jointly learning from audio and visual modalities. To do so, we present Acappella, a dataset spanning around 46 hours of a cappella solo singing videos sourced from YouTube. We also propose an audio-visual convolutional network based on graphs which achieves state-of-the-art singing voice separation results on our dataset and compare it against its audio-only counterpart, U-Net, and a state-of-the-art audiovisual speech separation model. We evaluate the models in the following challenging setups: i) presence of overlapping voices in the audio mixtures, ii) the target voice set to lower volume levels in the mix, and iii) combination of i) and ii). The third one being the most challenging evaluation setup. We demonstrate that our model outperforms the baseline models in the singing voice separation task in the most challenging evaluation setup. The code, the pre-trained models, and the dataset are publicly available at https://ipcv.github.io/Acappella/"
  - id: 974
    order: 110
    oral_session: 3
    poster_session: 2
    session_id: 3
    title: "Revisiting spatio-temporal layouts for compositional action recognition"
    authors:
      - author: "Gorjan Radevski (KU Leuven)"
      - author: "Marie-Francine Moens (KU Leuven)"
      - author: "Tinne Tuytelaars (KU Leuven)"
    all_authors: "Gorjan Radevski, Marie-Francine Moens and Tinne Tuytelaars"
    code: "https://github.com/gorjanradevski/revisiting-spatial-temporal-layouts"
    keywords:
      - word: "compositional action recognition"
      - word: "video understanding"
      - word: "something-something"
      - word: "action genome"
      - word: "charades"
      - word: "video transformer"
      - word: "multimodal fusion"
      - word: "spatial reasoning"
      - word: "spatio-temporal action recognition"
      - word: "revisiting spatio-temporal layouts"
      - word: ""
    paper: "papers/0974.pdf"
    supp: "supp/0974_supp.zip"
    abstract: "Recognizing human actions is fundamentally a spatio-temporal reasoning problem, and should be, at least to some extent, invariant to the appearance of the human and the objects involved. Motivated by this hypothesis, in this work, we take an object-centric approach to action recognition. Multiple works have studied this setting before, yet it remains unclear (i) how well a carefully crafted, spatio-temporal layout-based method can recognize human actions, and (ii) how, and when, to fuse the information from layout- and appearance-based models. The main focus of this paper is compositional/few-shot action recognition, where we advocate the usage of multi-head attention (proven to be effective for spatial reasoning) over spatio-temporal layouts, i.e., configurations of object bounding boxes. Additionally, we evaluate different schemes to inject video appearance information to the system, and benchmark our approach on background cluttered action recognition. On the Something-Else and Action Genome datasets, we demonstrate (i) how to extend multi-head attention for spatio-temporal layout-based action recognition, (ii) how to improve the performance of appearance-based models by fusion with layout-based models, (iii) that even on non-compositional background-cluttered video datasets, a fusion between layout- and appearance-based models improves performance."
  - id: 979
    order: 68
    poster_session: 1
    session_id: 2
    title: "Training Object Detectors if Only Large Objects are Labeled"
    authors:
      - author: "Daniel Pototzky (Robert Bosch GmbH)"
      - author: "Matthias MK Kirschner (Robert Bosch GmbH)"
      - author: "Azhar Sultan (Robert Bosch GmbH)"
      - author: "Lars Schmidt-Thieme (Universität Hildesheim)"
    all_authors: "Daniel Pototzky, Matthias MK Kirschner, Azhar Sultan and Lars Schmidt-Thieme"
    code: "https://github.com/boschresearch/PCIS"
    keywords:
      - word: "semi-supervised learning"
      - word: "weakly-supervised learning"
      - word: "missing annotations"
      - word: "pseudo-labels"
      - word: ""
    paper: "papers/0979.pdf"
    supp: "supp/0979_supp.zip"
    abstract: "Conventional methods for object detection typically rely on large, well-annotated datasets, which are in short supply due to the high costs of labeling. In this paper, we propose to label only large, easy-to-spot objects. We argue that these contain more pixels and therefore usually more information about the underlying object class than small ones. At the same time, they are easier to spot and hence cheaper to label. Unfortunately, standard supervised learning algorithms do not learn to detect small objects if only large ones are labeled. Instead, they erroneously take up unlabeled objects as negative examples and their accuracy consequently deteriorates. To address that, we propose PCIS, a novel combination of pseudo-labels, output consistency across scales, and an anchor scale-dependent ignore strategy. In experiments on CityPersons, EuroCityPersons and MS COCO, we show that our approach outperforms existing pseudo-label generation methods as well as an oracle which ensures that anchors overlapping missing annotations are ignored during training. We demonstrate that using our method it is possible to approach the performance of a fully labeled dataset with only a subset of the labels and also to train detectors on extremely sparsely labeled images, e.g. if only 1 out of 200 objects is annotated."
  - id: 993
    order: 289
    poster_session: 3
    session_id: 8
    title: "Dynamic Graph Warping Transformer for Video Alignment"
    authors:
      - author: "Junyan Wang (UNSW Sydney)"
      - author: "Yang Long (Durham University)"
      - author: "Maurice Pagnucco (UNSW)"
      - author: "Yang Song (University of New South Wales)"
    all_authors: "Junyan Wang, Yang Long, Maurice Pagnucco and Yang Song"
    code: ""
    keywords:
      - word: "Video alignment"
      - word: "Transformer"
      - word: "Graph Neural Network"
    paper: "papers/0993.pdf"
    supp: ""
    abstract: "Video alignment aims to match synchronised action information between multiple
video sequences. Existing methods are typically based on supervised learning to align video frames according to annotated action phases. However, such phase-level annotation cannot effectively guide frame-level alignment, since each phase can be completed at different speeds across individuals. In this paper, we introduce dynamic warping to take between-video information into account with a new Dynamic Graph Warping Transformer (DGWT) network model. Our approach is the first Graph Transformer framework designed for video analysis and alignment. In particular, a novel dynamic warping loss function is designed to align videos of arbitrary length using attention-level features. A Temporal Segment Graph (TSG) is proposed to enable the adjacency matrix to cope with temporal information in video data. Our experimental results on two public datasets (Penn Action and Pouring) demonstrate significant improvements over state-of-the-art approaches."
  - id: 997
    order: 398
    poster_session: 4
    session_id: 11
    title: "Self-Supervised Real-time Video Stabilization"
    authors:
      - author: "Jinsoo Choi (KAIST)"
      - author: "Jaesik Park (POSTECH)"
      - author: "In So Kweon (KAIST)"
    all_authors: "Jinsoo Choi, Jaesik Park and In So Kweon"
    code: ""
    keywords:
      - word: "video stabilization"
      - word: "self supervised"
      - word: "real time"
    paper: "papers/0997.pdf"
    supp: "supp/0997_supp.zip"
    abstract: "Videos are a popular media form, where online video streaming has recently gathered much popularity. In this work, we propose a novel method of real-time video stabilization - transforming a shaky video to a stabilized video as if it were stabilized via gimbals in real-time. Our framework is trainable in a self-supervised manner, which does not require data captured with special hardware setups (i.e., two cameras on a stereo rig or additional motion sensors). Our framework consists of a transformation estimator between given frames for global stability adjustments, followed by a scene parallax reduction module via spatially smoothed optical flow for further stability. Then, a margin inpainting module fills in the missing margin regions created during stabilization to reduce the amount of post-cropping. These sequential steps reduce distortion and margin cropping to a minimum while enhancing stability. Hence, our approach outperforms state-of-the-art real-time video stabilization methods as well as offline methods that require camera trajectory optimization. Our method procedure takes approximately 24.3 ms yielding 41 fps regardless of resolution (e.g., 480p or 1080p)."
  - id: 998
    order: 399
    poster_session: 4
    session_id: 11
    title: "Two Heads are Better than One: Geometric-Latent Attention for Point Cloud Classification and Segmentation"
    authors:
      - author: "Hanz Cuevas (The University of Edinburgh)"
      - author: "Antonio-Javier Gallego (Universidad de Alicante)"
      - author: "Robert B Fisher (University of Edinburgh)"
    all_authors: "Hanz Cuevas, Antonio-Javier Gallego and Robert B Fisher"
    code: "https://github.com/cuevhv/gelatto"
    keywords:
      - word: "point cloud"
      - word: "point cloud segmentation"
      - word: "3D"
      - word: "point cloud classification"
      - word: "shape classification"
      - word: "deep 3D segmentation"
      - word: "deep learning"
      - word: "scene understanding"
      - word: "3D scene understanding"
      - word: "light network"
      - word: "local agregation"
      - word: "gnn"
    paper: "papers/0998.pdf"
    supp: "supp/0998_supp.zip"
    abstract: "We present an innovative two-headed attention layer that combines geometric and latent features to segment a 3D scene into semantically meaningful subsets. Each head combines local and global information, using either the geometric or latent features, of a neighborhood of points and uses this information to learn better local relationships. This Geometric-Latent attention layer (Ge-Latto) is combined with a sub-sampling strategy to capture global features. Our method is invariant to permutation thanks to the use of shared-MLP layers, and it can also be used with point clouds with varying densities because the local attention layer does not depend on the neighbor order. Our proposal is simple yet robust, which allows it to achieve competitive results in the ShapeNetPart and ModelNet40 datasets, and the state-of-the-art when segmenting the complex dataset S3DIS, with 69.2% IoU on Area 5, and 89.7% overall accuracy using K-fold cross-validation on the 6 areas."
  - id: 1006
    order: 191
    poster_session: 2
    session_id: 5
    title: "SLURP: Side Learning Uncertainty for Regression Problems"
    authors:
      - author: "Xuanlong Yu (Paris-Saclay University)"
      - author: "Gianni Franchi (ENSTA Paris)"
      - author: "Emanuel Aldea (Paris-Saclay University)"
    all_authors: "Xuanlong Yu, Gianni Franchi and Emanuel Aldea"
    code: ""
    keywords:
      - word: "Uncertainty estimation"
      - word: "Confidence estimation"
      - word: "Auxiliary model"
      - word: "Monocular depth"
      - word: "Optical flow"
    paper: "papers/1006.pdf"
    supp: "supp/1006_supp.zip"
    abstract: "It has become critical for deep learning algorithms to quantify their output uncertainties to satisfy reliability constraints and provide accurate results.
Uncertainty estimation for regression has received less attention than classification due to the more straightforward standardized output of the latter class of tasks and their high importance. However, regression problems are encountered in a wide range of applications in computer vision.  We propose SLURP, a generic approach for regression uncertainty estimation via a side learner that exploits the output and the intermediate representations generated by the main task model. We test SLURP on two critical regression tasks in computer vision:  monocular depth and optical flow estimation. In addition, we conduct exhaustive benchmarks comprising transfer to different datasets and the addition of aleatoric noise. The results show that our proposal is generic and readily applicable to various regression problems and has a low computational cost with respect to existing solutions."
  - id: 1007
    order: 192
    poster_session: 2
    session_id: 5
    title: "Probabilistic Regression with Huber Distributions"
    authors:
      - author: "David A Mohlin (Tobii/KTH)"
      - author: "Gerald Bianchi (Univrses)"
      - author: "Josephine Sullivan (KTH Royal Institute of Technology)"
    all_authors: "David A Mohlin, Gerald Bianchi and Josephine Sullivan"
    code: "https://github.com/Davmo049/Public_prob_regression_with_huber_distributions/blob/main/README.md"
    keywords:
      - word: "regression"
      - word: "probabilities"
      - word: "uncertainty"
      - word: "body pose"
      - word: "facial landmarks"
      - word: "huber loss"
      - word: "robust estimation"
      - word: ""
    paper: "papers/1007.pdf"
    supp: "supp/1007_supp.zip"
    abstract: "In this paper we describe a probabilistic method for estimating the position of an object along with its covariance matrix using neural networks.
Our method is designed to be robust to outliers, have bounded gradients with respect to the network outputs, among other desirable properties. To achieve this we introduce a novel probability distribution inspired by the Huber loss.
We also introduce a new way to parameterize positive definite matrices to ensure invariance to the choice of orientation for the coordinate system we regress over.
We evaluate our method on popular body pose and facial landmark datasets and get performance on par or exceeding the performance of non-heatmap methods."
  - id: 1008
    order: 290
    poster_session: 3
    session_id: 8
    title: "Semi-Supervised Few-Shot Object Detection with a Teacher-Student Network"
    authors:
      - author: "Wuti Xiong (University of Oulu, Finland)"
      - author: "Yawen Cui (University of Oulu)"
      - author: "Li Liu (National University of Defense Technology)"
    all_authors: "Wuti Xiong, Yawen Cui and Li Liu"
    code: ""
    keywords:
      - word: "Few-shot"
      - word: "Object Detection"
      - word: "Semi-supervised"
    paper: "papers/1008.pdf"
    supp: ""
    abstract: "Few-shot object detection, which aims to recognize unseen objects with a few annotated instances, has attracted increasing attention in the computer vision community. Most recent works tackle this problem under the meta-learning framework based on an episodic training strategy. In this work, we advance the few-shot object detection paradigm towards a new scenario called semi-supervised few-shot object detection (SSFSOD), where the unlabeled data are available within each episode. To address this paradigm, we propose a novel method which utilizes a dual model (teacher-student) to leverage available unlabeled data. Specifically, the teacher model provides high-quality pseudo-labels for the student model during the training process, while the student model uses the exponential moving average strategy to update the teacher model online. We also employ a two-fold correlation-guided attention module to guide RPN to generate task-specific region proposals by highlighting potential regions and informative channels.  We conduct extensive experiments on three datasets MS COCO, PASCAL VOC, and FSOD. The experimental results demonstrate the effectiveness of the proposed method."
  - id: 1011
    order: 400
    poster_session: 4
    session_id: 11
    title: "Looking at the whole picture: constrained unsupervised anomaly segmentation"
    authors:
      - author: "Julio Silva-Rodríguez (Universitat Politècncia de València)"
      - author: "Valery Naranjo (Universitat Politècnica de València)"
      - author: "Jose Dolz (ETS Montreal)"
    all_authors: "Julio Silva-Rodríguez, Valery Naranjo and Jose Dolz"
    code: "https://github.com/cvblab/anomaly_localization_vae_gcams"
    keywords:
      - word: "unsueprvised anomaly localization"
      - word: "brain lesion segmentation"
      - word: "constrained segmentation"
      - word: "size-constrained loss"
      - word: "class-activations maps"
      - word: "CAMs"
      - word: "log-barrier extension"
      - word: "BRATS19"
      - word: ""
    paper: "papers/1011.pdf"
    supp: "supp/1011_supp.zip"
    abstract: "Current unsupervised anomaly localization approaches rely on generative models to learn the distribution of normal images, which is later used to identify potential anomalous regions derived from errors on the reconstructed images. However, a main limitation of nearly all prior literature is the need of employing anomalous images to set a class-specific threshold to locate the anomalies. This limits their usability in realistic scenarios, where only normal data is typically accessible. Despite this major drawback, only a handful of works have addressed this limitation, by integrating supervision on attention maps during training. In this work, we propose a novel formulation that does not require accessing images with abnormalities to define the threshold. Furthermore, and in contrast to very recent work, the proposed constraint is formulated in a more principled manner, leveraging well-known knowledge in constrained optimization. In particular, the equality constraint on the attention maps in prior work is replaced by an inequality constraint, which allows more flexibility. In addition, to address the limitations of penalty-based functions we employ an extension of the popular log-barrier methods to handle the constraint. Comprehensive experiments on the popular BRATS'19 dataset demonstrate that the proposed approach substantially outperforms relevant literature, establishing new state-of-the-art results for unsupervised lesion segmentation. "
  - id: 1014
    order: 69
    poster_session: 1
    session_id: 2
    title: "TinyStarGAN v2: Distilling StarGAN v2 for Efficient Diverse Image Synthesis for Multiple Domains"
    authors:
      - author: "PARAS KAPOOR (CONCORDIA UNIVERSITY)"
      - author: "Tien D. Bui (Concordia University)"
    all_authors: "PARAS KAPOOR and Tien D. Bui"
    code: "https://github.com/pkhigh/tinystargan-v2"
    keywords:
      - word: "knowledge distillation"
      - word: "gan compression"
      - word: "multi domain image synthesis"
      - word: "deep neural network compression"
      - word: "efficient gans"
      - word: ""
    paper: "papers/1014.pdf"
    supp: "supp/1014_supp.zip"
    abstract: "Image-to-image translation models, such as StarGAN v2, have enabled the translation of diverse images over multiple domains in a single framework.  However, StarGAN v2 is computationally expensive making it challenging to execute on resource-constrained environments. To reduce the computation requirement of StarGAN v2 while maintaining accuracy, we propose a novel cross distillation method that is specially designed for knowledge distillation (KD) of multiple networks in a single framework.  By leveraging this new KD method, the knowledge of a multi-network large teacher StarGAN v2 can be effectively transferred to a small student TinyStarGAN v2 framework. Without losing the quality and diversity of generated images, we reduce the size of the original framework by more than 20× and the computation requirement by more than 5×.  Experiments on CelebA-HQ and AFHQ datasets show the effectiveness of the proposed method."
  - id: 1020
    order: 70
    poster_session: 1
    session_id: 2
    title: "SAGAN: Adversarial Spatial-asymmetric Attention for Noisy Nona-Bayer Reconstruction"
    authors:
      - author: "S M A Sharif (Rigel-IT)"
      - author: "Rizwan Ali  Naqvi (Sejong University)"
      - author: "Mithun  Biswas (Rigel-IT)"
    all_authors: "S M A Sharif, Rizwan Ali  Naqvi and Mithun  Biswas"
    code: "https://github.com/sharif-apu/SAGAN_BMVC21"
    keywords:
      - word: "Nona-Bayer Reconstruction"
      - word: "Joint Demosicing and Denoising"
      - word: "JDD"
      - word: "Pixel-bin Sensor"
      - word: "Nona-Bayer Demosaicking"
      - word: "Nona-Bayer Denoising"
      - word: "Spatial Asymmetric Attention"
      - word: "SAGAN"
      - word: "Spatial-asymmetric Attention Module"
      - word: "Smartphone Image JDD"
    paper: "papers/1020.pdf"
    supp: "supp/1020_supp.zip"
    abstract: "Nona-Bayer colour filter array (CFA) pattern is considered one of the most viable alternatives to traditional Bayer patterns. Despite the substantial advantages, such non-Bayer CFA patterns are susceptible to produce visual artefacts while reconstructing RGB images from noisy sensor data. This study addresses the challenges of learning RGB image reconstruction from noisy Nona-Bayer CFA comprehensively.  We propose a novel spatial-asymmetric attention module to jointly learn bi-direction transformation and large-kernel global attention to reduce the visual artefacts. We combine our proposed module with adversarial learning to produce plausible images from Nona-Bayer CFA. The feasibility of the proposed method has been verified and compared with the state-of-the-art image reconstruction method. The experiments reveal that the proposed method can reconstruct RGB images from noisy Nona-Bayer CFA without producing any visually disturbing artefacts. Also, it can outperform the state-of-the-art image reconstruction method in both qualitative and quantitative comparison."
  - id: 1022
    order: 71
    poster_session: 1
    session_id: 2
    title: "Robustness Learning via Decision Tree Search Robust Optimisation"
    authors:
      - author: "Yi-Ling Liu (Imperial College London)"
      - author: "Alessio Lomuscio (Imperial College London)"
    all_authors: "Yi-Ling Liu and Alessio Lomuscio"
    code: ""
    keywords:
      - word: "adversarial learning"
      - word: "robustness"
      - word: "decision tree search"
    paper: "papers/1022.pdf"
    supp: "supp/1022_supp.zip"
    abstract: "We present a novel method for robustness training for ReLU-based deep neural networks. The method involves a decision tree search targeting the worst-case data points to generate adversarial examples. We combine the decision tree search method with robust optimisation to train a robust model while maintaining accuracy at comparably lower computational effort than SoA methods. The efficiency is obtained by focusing on small regions centred around the input that have significant potential to generate adversarial samples. We implemented the resulting method in the framework DTSRobust, which was evaluated against state-of-the-art defence methods on MNIST and CIFAR10 datasets. In experiments, DTSRobust achieved a 14.2% gain on efficiency against the state-of-the-art defence methods in MNIST and 10.3% of that in CIFAR10 while maintaining similar accuracy."
  - id: 1028
    order: 193
    poster_session: 2
    session_id: 5
    title: "Training Better Deep Neural Networks with Uncertainty Mining Net"
    authors:
      - author: "Yang Sun (Bytedance)"
      - author: "Abhishek Kolagunda (IBM)"
      - author: "Steven Eliuk (IBM)"
      - author: "Xiaolong Wang (IBM)"
    all_authors: "Yang Sun, Abhishek Kolagunda, Steven Eliuk and Xiaolong Wang"
    code: ""
    keywords:
      - word: "label noise"
      - word: "label uncertainty"
      - word: "learning with noisy labels"
    paper: "papers/1028.pdf"
    supp: "supp/1028_supp.zip"
    abstract: "In this work, we consider the problem of training deep neural networks on partially labeled data with label noise. That is, semi-supervised training of deep neural networks with noisily labeled data. As far as we know, this is a scarcely studied topic. We present a novel end-to-end deep generative framework for improving classifier performance when dealing with such data challenges. We call it Uncertainty Mining Net (UMN). We utilize all the available data (labeled and unlabeled) to train the classifier via a semi-supervised generative framework. During training, UMN estimates the uncertainty of the labels to focus on clean data for learning. More precisely, UMN applies a novel sample-wise label uncertainty estimation scheme. Extensive experiments and comparisons against state-of-the-art methods on several popular benchmark datasets demonstrate that UMN can reduce the impact of label noise and significantly improve classifier performance."
  - id: 1035
    order: 401
    poster_session: 4
    session_id: 11
    title: "GPRAR: Graph Convolutional Network based Pose Reconstruction and Action Recognition for Human Trajectory Prediction"
    authors:
      - author: "Manh Huynh (ucdenver)"
      - author: "Gita Alaghband (University of Colorado Denver)"
    all_authors: "Manh Huynh and Gita Alaghband"
    code: "https://github.com/trungmanhhuynh/GPRAR"
    keywords:
      - word: "trajectory prediction"
      - word: "pose reconstruction"
      - word: "temporal neural network"
      - word: "future location prediction"
      - word: "action recognition for trajectory prediction."
    paper: "papers/1035.pdf"
    supp: "supp/1035_supp.zip"
    abstract: "Prediction with high accuracy is essential for various applications such as autonomous driving. Existing prediction models are easily prone to errors in real-world settings where observations (e.g. human poses and locations) are often noisy. To address this problem, we introduce GPRAR, a graph convolutional network based pose reconstruction and action recognition for human trajectory prediction. The key idea of GPRAR is to generate robust features: human poses and actions, under noisy scenarios. To this end, we design GPRAR using two novel sub-networks: PRAR (Pose Reconstruction and Action Recognition) and FA (Feature Aggregator). PRAR aims to simultaneously reconstruct human poses and action features from the coherent and structural properties of human skeletons. It is a network of an encoder and two decoders, each of which comprises multiple layers of spatiotemporal graph convolutional networks. Moreover, we propose a Feature Aggregator (FA) to channel-wise aggregate the learned features: human poses, actions, locations, and camera motion using encoder-decoder based temporal convolutional neural networks to predict future locations. Extensive experiments on the commonly used datasets: JAAD [13] and TITAN [19] show accuracy improvements of GPRAR over state-of-theart models. Specifically, GPRAR improves the prediction accuracy up to 22% and 50% under noisy observations on JAAD and TITAN datasets, respectively"
  - id: 1038
    order: 291
    poster_session: 3
    session_id: 8
    title: "Guided Flow Field Estimation by Generating Independent Patches"
    authors:
      - author: "Mohsen Tabejamaat (Inria)"
      - author: "Farhood Negin (Inria)"
      - author: "Francois Bremond (Inria Sophia Antipolis, France)"
    all_authors: "Mohsen Tabejamaat, Farhood Negin and Francois Bremond"
    code: ""
    keywords:
      - word: "Image generation"
      - word: "Pose transfer"
      - word: "Patch generation"
    paper: "papers/1038.pdf"
    supp: "supp/1038_supp.zip"
    abstract: "Recent studies have demonstrated the effectiveness of warping in transferring unique textures to the output of the pose transfer networks. However, due to the mutual dependencies of image features and pixel locations, joint estimation of flow map and output image is very likely to get stuck in local minima. Current solution is limited to offline estimation of the maps. However, this way the flow is generated without interaction with the embodiment parts of the generative model, causing it to struggle with the occlusion parts of the samples. To address the issue, we introduce a patch generation module which acts as a mediator between the output and flow map estimations, cutting their mutual dependencies while encouraging the flow maps to merely concentrate on regions that are not correctly generated by the patch estimations, regions like clothing with unique colors or textures that due to the scarcity of data can not be properly learned during the training phase of the network. Our patch generation module benefits from two individual experts on removing the visible parts of the source sample which disappear in the target view and drawing those invisible parts which appears in the novel view of the sample. Experimental results demonstrate that our method outperforms the state-of-the-arts on two well-known databases; Deepfashion and Market1501"
  - id: 1039
    order: 194
    poster_session: 2
    session_id: 5
    title: "Mini-batch Similarity Graphs for Robust Image Classification"
    authors:
      - author: "Arnab Kumar Mondal (Mcgill University)"
      - author: "Vineet Jain (McGill University)"
      - author: "Kaleem Siddiqi (McGill University)"
    all_authors: "Arnab Kumar Mondal, Vineet Jain and Kaleem Siddiqi"
    code: "https://github.com/arnab39/MinibatchSimilarityGraph"
    keywords:
      - word: "Minibatch Graph"
      - word: "Graph Neural Network"
      - word: "Robustness"
      - word: "Image Classification"
      - word: "Adversarial Attacks"
      - word: "GAN"
    paper: "papers/1039.pdf"
    supp: "supp/1039_supp.zip"
    abstract: "Current deep learning models for image-based classification tasks are trained using mini-batches. In the present article, we show that exploiting similarity between samples in each mini-batch can significantly boost robustness to input perturbations, an often neglected consideration in the computer vision community. To accomplish this, we dynamically construct a similarity graph from the mini-batch samples and aggregate information using an attention module. In addition to the added robustness, this approach also improves performance in diverse image-based object and scene classification tasks."
  - id: 1043
    order: 402
    poster_session: 4
    session_id: 11
    title: "Learning to Deblur and Rotate Motion-Blurred Faces"
    authors:
      - author: "Givi Meishvili (Computer Vision Group - Computer Science Department - University of Bern)"
      - author: "Attila Szabo (Huawei Noah's Ark Lab)"
      - author: "Simon Jenni (Adobe Research)"
      - author: "Paolo Favaro (University of Bern)"
    all_authors: "Givi Meishvili, Attila Szabo, Simon Jenni and Paolo Favaro"
    code: "https://gmeishvili.github.io/deblur_and_rotate_motion_blurred_faces/index.html"
    keywords:
      - word: "deblurring"
      - word: "face"
      - word: "multi-view"
      - word: "video"
      - word: "blur"
      - word: "GAN"
      - word: "novel view synthesis"
      - word: "inversion"
      - word: "deep learning"
      - word: "dataset"
      - word: ""
    paper: "papers/1043.pdf"
    supp: "supp/1043_supp.zip"
    abstract: "We propose a solution to the novel task of rendering sharp videos from new viewpoints from a single motion-blurred image of a face. Our method handles the complexity of face blur by implicitly learning the geometry and motion of faces through the joint training on three large datasets: FFHQ and 300VW, which are publicly available, and a new Bern Multi-View Face Dataset (BMFD) that we built. The first two datasets provide a large variety of faces and allow our model to generalize better. BMFD instead allows us to introduce multi-view constraints, which are crucial to synthesizing sharp videos from a new camera view. It consists of high frame rate synchronized videos from multiple views of several subjects displaying a wide range of facial expressions. We use the high frame rate videos to simulate realistic motion blur through averaging. Thanks to this dataset, we train a neural network to reconstruct a 3D video representation from a single image and the corresponding face gaze. We then provide a camera viewpoint relative to the estimated gaze and the blurry image as input to an encoder-decoder network to generate a video of sharp frames with a novel camera viewpoint. We demonstrate our approach on test subjects of our multi-view dataset and VIDTIMIT."
  - id: 1048
    order: 403
    poster_session: 4
    session_id: 11
    title: "Learning Query Expansion over the Nearest Neighbor Graph"
    authors:
      - author: "Benjamin Klein (Tel Aviv University)"
      - author: "Lior Wolf (Tel Aviv University, Israel)"
    all_authors: "Benjamin Klein and Lior Wolf"
    code: ""
    keywords:
      - word: "query expanson"
      - word: "graph neural networks"
      - word: "attention models"
    paper: "papers/1048.pdf"
    supp: ""
    abstract: "Query Expansion (QE) is a well established method for improving retrieval metrics
in image search applications. When using QE, the search is conducted on a new query vector, constructed using an aggregation function over the query and images from the database. Recent works gave rise to QE techniques in which the aggregation function is learned, whereas previous techniques were based on hand-crafted aggregation functions, e.g., taking the mean of the query’s nearest neighbors. However, most QE methods have focused on aggregation functions that work directly over the query and its immediate nearest neighbors. In this work, a hierarchical model, Graph Query Expansion (GQE), is presented, which is learned in a supervised manner and performs aggregation over an extended neighborhood of the query, thus increasing the information used from the database when computing the query expansion, and using the structure of the nearest neighbors graph. The technique achieves state-of-the-art results over known benchmarks."
  - id: 1053
    order: 292
    poster_session: 3
    session_id: 8
    title: "GTA: Global Temporal Attention for Video Action Understanding"
    authors:
      - author: "Bo He (University of Maryland)"
      - author: "Xitong Yang (University of Maryland)"
      - author: "Zuxuan Wu (UMD)"
      - author: "Hao Chen (University of Maryland)"
      - author: "Ser-Nam Lim (Facebook AI)"
      - author: "Abhinav Shrivastava (University of Maryland)"
    all_authors: "Bo He, Xitong Yang, Zuxuan Wu, Hao Chen, Ser-Nam Lim and Abhinav Shrivastava"
    code: ""
    keywords:
      - word: "action recognition"
      - word: "self-attention"
      - word: "temporal modeling"
      - word: ""
    paper: "papers/1053.pdf"
    supp: "supp/1053_supp.zip"
    abstract: "Self-attention learns pairwise interactions to model long-range dependencies, yielding great improvements for video action recognition. In this paper, we seek a deeper understanding of self-attention for temporal modeling in videos. We first demonstrate that the entangled modeling of spatio-temporal information by flattening all pixels is sub-optimal, failing to capture temporal relationships among frames explicitly. To this end, we introduce Global Temporal Attention (GTA), which performs global temporal attention on top of spatial attention in a decoupled manner. We apply GTA on both pixels and semantically similar regions to capture temporal relationships at different levels of spatial granularity. Unlike conventional self-attention that computes an instance-specific attention matrix, GTA directly learns a global attention matrix that is intended to encode temporal structures that generalize across different samples. We further augment GTA with a cross-channel multi-head fashion to exploit channel interactions for better temporal modeling. Extensive experiments on 2D and 3D networks demonstrate that our approach consistently enhances temporal modeling and provides state-of-the-art performance on three video action recognition datasets.
"
  - id: 1054
    order: 72
    poster_session: 1
    session_id: 2
    title: "Deep Knowledge Distillation using Trainable Dense Attention"
    authors:
      - author: "Bharat Sau (IITH)"
      - author: "Soumya Roy (IIT, Kanpur)"
      - author: "Vinay P Namboodiri (IIT Kanpur)"
      - author: "Raghu Sesha Iyengar (IIT Hyderabad)"
    all_authors: "Bharat Sau, Soumya Roy, Vinay P Namboodiri and Raghu Sesha Iyengar"
    code: ""
    keywords:
      - word: "Knowledge Distillation"
      - word: "Network Compression"
      - word: "Visual Attention"
    paper: "papers/1054.pdf"
    supp: "supp/1054_supp.zip"
    abstract: "Knowledge distillation based deep model compression has been actively pursued in order  to  obtain  improved  performance  on  specified  student  architectures  by  distilling knowledge  from  deeper  networks.   Among  various  methods,  attention  based  knowledge  distillation  has  shown  great  promise  on  large  datasets.   However,  this  approach is limited by hand-designed attention functions such as absolute sum.  We address this shortcoming  by  proposing  trainable  attention  methods  that  can  be  used  to  obtain  improved performance while distilling knowledge from teacher to student.  We also show that, using dense connections efficiently between attention modules, we can further improve  the  student’s  performance.   Our  approach,  when  applied  to  ResNet50(teacher)-MobileNetv1(student) pair on ImageNet dataset, has a reduction of 9.6% in Top-1 error rate over the previous state-of-the-art method."
  - id: 1056
    order: 293
    poster_session: 3
    session_id: 8
    title: "A Simple Baseline for Weakly-Supervised Human-centric Relation Detection"
    authors:
      - author: "Raghav Goyal (University of British Columbia)"
      - author: "Leonid Sigal (University of British Columbia)"
    all_authors: "Raghav Goyal and Leonid Sigal"
    code: "https://github.com/ubc-vision/SimpleWeakHOI"
    keywords:
      - word: "weakly-supervised learning"
      - word: "human-object interaction"
    paper: "papers/1056.pdf"
    supp: "supp/1056_supp.zip"
    abstract: "In this paper we address the problem of weakly-supervised Visual Relation Detection (VRD) and human-centric Scene Graph generation. Unlike prior works, we assume weaker, yet more natural, supervisory signals. Specifically, we only assume a pre-trained person detector, a generic region proposal mechanism and a set of image-level object and relation labels per frame. Given this data we formulate a very simple architecture with multi-task weak-supervision at object level (for individual proposed regions) and relation level (for each person-object region pair). We show that despite simplicity, our approach achieves state-of-the-art results as compared to other weakly- and strongly-supervised VRD models that are significantly more complex. In ablations, we also show that proposed multi-task learning improves relation predictions. Our goal in this paper is to propose a strong, yet simple, baseline which will spur further developments in the VRD task."
  - id: 1061
    order: 195
    poster_session: 2
    session_id: 5
    title: "PAL : Pretext-based Active Learning"
    authors:
      - author: "Shubhang Bhatnagar (IIT Bombay)"
      - author: "Sachin Goyal (Microsoft research)"
      - author: "Darshan Tank (IIT Bombay)"
      - author: "Amit Sethi (Indian Institute of Technology Bombay)"
    all_authors: "Shubhang Bhatnagar, Sachin Goyal, Darshan Tank and Amit Sethi"
    code: "https://github.com/shubhangb97/PAL_pretext_based_active_learning"
    keywords:
      - word: "active learning"
      - word: "self-supervision"
      - word: "robustness"
      - word: ""
    paper: "papers/1061.pdf"
    supp: "supp/1061_supp.zip"
    abstract: "The goal of pool-based active learning is to judiciously select a fixed-sized subset of unlabeled samples from a pool to query an oracle for their labels, in order to maximize the accuracy of a supervised learner. However, the unsaid requirement that the oracle should always assign correct labels is unreasonable for most situations. We propose an active learning technique for deep neural networks that is more robust to mislabeling than the previously proposed techniques. Previous techniques rely on the task network itself to estimate the novelty of the unlabeled samples, but learning the task (generalization) and selecting samples (out-of-distribution detection) can be conflicting goals. We use a separate network to score the unlabeled samples for selection. The scoring network relies on self-supervision for modeling the distribution of the labeled samples to reduce the dependency on potentially noisy labels. To counter the paucity of data, we also deploy another head on the scoring network for regularization via multi-task learning and use an unusual self-balancing hybrid scoring function. Furthermore, we divide each query into sub-queries before labeling to ensure that the query has diverse samples. In addition to having a higher tolerance to mislabeling of samples by the oracle, the resultant technique also produces competitive accuracy in the absence of label noise. The technique also handles the introduction of new classes on-the-fly well by temporarily increasing the sampling rate of these classes. We make our code publicly available at  https://github.com/shubhangb97/PAL_pretext_based_active_learning"
  - id: 1075
    order: 73
    poster_session: 1
    session_id: 2
    title: "Contour-guided Image Completion with Perceptual Grouping"
    authors:
      - author: "Morteza Rezanejad (University of Toronto)"
      - author: "Sidharth Gupta (University of Toronto)"
      - author: "Chandra Gummaluru (University of Toronto	)"
      - author: "Ryan Marten (University of Toronto)"
      - author: "John Wilder (University of Toronto)"
      - author: "Michael Gruninger (University of Toronto)"
      - author: "Dirk B. Walther (University of Toronto)"
    all_authors: "Morteza Rezanejad, Sidharth Gupta, Chandra Gummaluru, Ryan Marten, John Wilder, Michael Gruninger and Dirk B. Walther"
    code: "https://github.com/sidguptacode/Stochastic_Completion_Fields"
    keywords:
      - word: "Image Completion"
      - word: "Inpainting"
      - word: "Perceptual Grouping"
      - word: "Stochastic Completion Fields"
      - word: "Contour Completion"
      - word: "Good Continuation"
      - word: "Perceptual Organisation"
    paper: "papers/1075.pdf"
    supp: "supp/1075_supp.zip"
    abstract: "Humans are excellent at perceiving illusory outlines. We are readily able to complete contours, shapes, scenes, and even unseen objects when provided with images that contain broken fragments of a connected appearance. In vision science, this ability is largely explained by perceptual grouping: a foundational set of processes in human vision that describes how separated elements can be grouped. In this paper, we revisit an algorithm called Stochastic Completion Fields (SCFs) that mechanizes a set of such processes -- good continuity, closure, and proximity -- through contour completion. This paper implements a modernized model of the SCF algorithm and uses it in an image editing framework where we propose novel methods to complete fragmented contours. We show how the SCF algorithm plausibly mimics results in human perception. We use the SCF completed contours as guides for inpainting, and show that our guides improve the performance of state-of-the-art models. Additionally, we show that the SCF aids in finding edges in high-noise environments. Overall, our described algorithms resemble an important mechanism in the human visual system and offer a novel framework that modern computer vision models can benefit from."
  - id: 1077
    order: 74
    poster_session: 1
    session_id: 2
    title: "C4Net: Contextual Compression and Complementary Combination Network for Salient Object Detection"
    authors:
      - author: "Hazarapet Tunanyan (PicsArt)"
    all_authors: "Hazarapet Tunanyan"
    code: ""
    keywords:
      - word: "salient object detection"
      - word: "c4net"
      - word: "excessiveness loss"
      - word: "complementary combination"
    paper: "papers/1077.pdf"
    supp: "supp/1077_supp.zip"
    abstract: "Deep learning solutions of the salient object detection
problem have achieved great results in recent years. The
majority of these models are based on encoders and decoders, with a different multi-feature combination. In this
paper, we show that feature concatenation works better
than other combination methods like multiplication or addition. Also, joint feature learning gives better results, because of the information sharing during their processing.
We designed a Complementary Extraction Module (CEM)
to extract necessary features with edge preservation. Our
proposed Excessiveness Loss (EL) function helps to reduce
false-positive predictions and purifies the edges with other
weighted loss functions. Our designed Pyramid-Semantic
Module (PSM) with Global guiding flow (G) makes the prediction more accurate by providing high-level complementary information to shallower layers. Experimental results
show that the proposed model outperforms the state-of-the-art methods on all benchmark datasets under three evaluation metrics."
  - id: 1079
    order: 294
    poster_session: 3
    session_id: 8
    title: "One-Shot Deep Model for End-to-End Multi-Person Activity Recognition"
    authors:
      - author: "Shuhei Tarashima (NTT Communications Corporation)"
    all_authors: "Shuhei Tarashima"
    code: ""
    keywords:
      - word: "Group Activity Recognition"
      - word: "Action Recognition"
      - word: "Multi-Object Tracking"
      - word: "Multi-task Learning"
    paper: "papers/1079.pdf"
    supp: "supp/1079_supp.zip"
    abstract: "In this work we tackle the multi-person activity recognition problem, where actor detection, tracking, individual action recognition and group activity recognition tasks are jointly solved given an input sequence. Since related works in the literature only deal with parts of the whole problem despite sharing similar architectures, trivial combinations of them result in slow and redundant pipelines and miss the opportunity to leverage inter-task mutual dependency. This motivates us to introduce a novel deep learning model, named TrAct-Net, that can jointly solve all the above tasks in a unified architecture. A new multi-branch CNN in TrAct-Net makes inference efficient and simple, and a novel relation encoder successfully takes both positional and identical relation of detections into consideration to boost both individual action and group activity recognition performances. The whole network is trained end-to-end using a multi-task learning framework. To the best of our knowledge, TrAct-Net is the first end-to-end trainable model to solve the whole problem in a one-shot manner. Experiments on public datasets demonstrate that TrAct-Net achieves superior performance to combinations of state-of-the-arts with much fewer model parameters and faster inference speed."
  - id: 1083
    order: 295
    poster_session: 3
    session_id: 8
    title: "Attention to Action: Leveraging Attention for Object Navigation"
    authors:
      - author: "Shi Chen (University of Minnesota)"
      - author: "Qi Zhao (University of Minnesota)"
    all_authors: "Shi Chen and Qi Zhao"
    code: "https://github.com/szzexpoi/ana"
    keywords:
      - word: "Object-goal Navigation"
      - word: "Attention"
      - word: "Visual Navigation"
    paper: "papers/1083.pdf"
    supp: "supp/1083_supp.zip"
    abstract: "Navigation towards different objects is prevalent in daily lives. State-of-the-art embodied vision methods accomplish the task by implicitly learning the relationship between perception and action or optimizing them with separate objectives. While effective in some cases, they have yet developed (1) a tight integration of perception and action, and (2) the capability to address visual variance that is significant in the moving and embodied setting. To close these research gaps, we introduce a new attention mechanism, which represents the pursuit of visual information that highlights the potential directions of final targets and guides agents' action for visual navigation. Instead of working conventionally as a weighted map for aggregating visual features, the new attention is defined as a compact intermediate state connecting visual observations and action. It is explicitly coupled with action to enable a joint optimization through a consistent action space, and also plays an importance role in alleviating the effects of visual variance. Our experiments show significant improvements in navigation across various types of unseen environments with known and unknown semantics. Ablation analyses indicate that the proposed method integrates perception and action by correlating attention patterns with the directions of action, and overcomes visual variance by distilling useful information from visual observations into attention distribution. Our code is publicly available at https://github.com/szzexpoi/ana."
  - id: 1087
    order: 115
    oral_session: 4
    poster_session: 2
    session_id: 4
    title: "Holistic Guidance for Occluded Person Re-Identification"
    authors:
      - author: "Madhu Kiran (ETS Montreal)"
      - author: "Gnana Praveen Rajasekhar (Ecole Technologie Superieure)"
      - author: "Le Thanh Nguyen-Meidine (ETS Montreal)"
      - author: "Soufiane Belharbi (ÉTS Montreal)"
      - author: "Louis-Antoine Blais-Morin (Genetec Inc.)"
      - author: "Eric Granger (ETS Montreal )"
    all_authors: "Madhu Kiran, Gnana Praveen Rajasekhar, Le Thanh Nguyen-Meidine, Soufiane Belharbi, Louis-Antoine Blais-Morin and Eric Granger"
    code: "https://github.com/madhukiranets/HolisitcGuidanceOccReID2"
    keywords:
      - word: "Person ReID"
      - word: "Representation learning"
      - word: "image retreival"
    paper: "papers/1087.pdf"
    supp: "supp/1087_supp.pdf"
    abstract: "In real-world video surveillance applications, person re-identification (ReID) suffers
from the effects of occlusions and detection errors. Despite recent advances, occlusions continue to corrupt the features extracted by state-of-art CNN backbones and thereby deteriorate the accuracy of ReID systems. To address this issue, methods in the literature rely on an additional costly process, such as pose estimation, where pose maps provide supervision to focus on visible parts of occluded regions. In contrast, we introduce a Holistic Guidance (HG) method that relies on holistic (or non-occluded) data and its distribution in the dissimilarity space to train the CNN backbone on an occluded dataset. This method is motivated by our empirical study, where the distribution of pairwise between-class and within-class matching distances (Distribution of Class Distances or DCDs) between images has considerable overlap in occluded datasets compared to holistic datasets. Hence, our HG method employs this discrepancy in DCDs of both datasets for joint learning of a student-teacher model to produce an attention map that focuses primarily on visible regions of the occluded images. In particular, features extracted from both datasets are jointly learned using the student model to produce an attention map that allows dissociating visible regions from occluded ones. Additionally, a joint generative-discriminative CNN backbone is trained using a denoising autoencoder such that the system can self-recover from occlusions. Extensive experiments on several challenging public datasets indicate that the proposed approach can outperform state-of-the-art methods on both occluded and holistic datasets."
  - id: 1093
    order: 75
    poster_session: 1
    session_id: 2
    title: "Noise-Aware Video Saliency Prediction"
    authors:
      - author: "Ekta Prashnani (University of California, Santa Barbara)"
      - author: "Orazio Gallo (NVIDIA Research)"
      - author: "Joohwan Kim (NVIDIA)"
      - author: "Josef Spjut (NVIDIA)"
      - author: "Pradeep Sen (UC Santa Barbara)"
      - author: "Iuri Frosio (NVIDIA)"
    all_authors: "Ekta Prashnani, Orazio Gallo, Joohwan Kim, Josef Spjut, Pradeep Sen and Iuri Frosio"
    code: "https://github.com/NVlabs/NAT-saliency"
    keywords:
      - word: "video saliency prediction"
      - word: "video game saliency"
      - word: "video saliency dataset"
      - word: "noise-aware training"
      - word: "learning from noisy labels"
      - word: "gaze data acquisition"
    paper: "papers/1093.pdf"
    supp: "supp/1093_supp.zip"
    abstract: "We tackle the problem of predicting saliency maps for videos of dynamic scenes.
We note that the accuracy of the maps reconstructed from the gaze data of a fixed number of observers varies with the frame, as it depends on the content of the scene.
This issue is particularly pressing when a limited number of observers are available.
In such cases, directly minimizing the discrepancy between the predicted and measured saliency maps, as traditional deep-learning methods do, results in overfitting to the noisy data.
We propose a noise-aware training (NAT) paradigm that quantifies and accounts for the uncertainty arising from frame-specific gaze data inaccuracy.
We show that NAT is especially advantageous when limited training data is available, with experiments across different models, loss functions, and datasets.
We also introduce a video game-based saliency dataset, with rich temporal semantics, and multiple gaze attractors per frame.
The dataset and source code are available at https://github.com/NVlabs/NAT-saliency."
  - id: 1095
    order: 111
    oral_session: 3
    poster_session: 2
    session_id: 3
    title: "AEI: Actors-Environment Interaction with Adaptive Attention for Temporal Action Proposals Generation"
    authors:
      - author: "Khoa HV Vo (University of Arkansas)"
      - author: "Hyekang Joo (University of Maryland, College Park)"
      - author: "Kashu Yamazaki (University of Arkansas)"
      - author: "Sang Q Truong (VNUHCM - International University)"
      - author: "Kris Kitani (Carnegie Mellon University)"
      - author: "Minh-Triet Tran (University of Science, VNU-HCM)"
      - author: "Ngan Le (University of Arkansas)"
    all_authors: "Khoa HV Vo, Hyekang Joo, Kashu Yamazaki, Sang Q Truong, Kris Kitani, Minh-Triet Tran and Ngan Le"
    code: "https://github.com/vhvkhoa/TAPG-AgentEnvInteration.git"
    keywords:
      - word: "temporal action proposal"
      - word: "temporal action detection"
      - word: "video understanding"
    paper: "papers/1095.pdf"
    supp: "supp/1095_supp.zip"
    abstract: "Humans typically perceive the establishment of an action in a video through the interaction between an actor and the surrounding environment. Despite the great progress in temporal action proposal generation, most existing works ignore the above fact and leave their model learning to propose actions as a black-box. In this paper, we make an attempt to simulate that ability of human by proposing Actor Environment Interaction (AEI) network to learn video visual representation for temporal action proposals generation. AEI contains two modules i.e. perception-based visual representation (PVR) and boundary matching module (BMM). PVR represents each video snippet by taking human-human relations and humans-environment relations into consideration using the proposed adaptive attention mechanism. Then, the video representation is taken by BMM to generate action proposals. AEI is comprehensively evaluated in ActivityNet-1.3 and THUMOS-14 datasets, on temporal action proposal and detection tasks, with two boundary matching architectures (i.e. CNN-based and GCN-based) and two classifiers (i.e. Unet and P-GCN). Our AEI shows significant improvement when regarding human logical thinking to extract spatio-temporal visual representation. Our AEI robustly outperforms SOTA methods with remarkable performance and generalization for both temporal action proposal generation and temporal action detection."
  - id: 1098
    order: 1
    oral_session: 1
    poster_session: 1
    session_id: 0
    title: "Geometry-Aware Multi-Task Learning for Binaural Audio Generation from Video"
    authors:
      - author: "Rishabh Garg (The University of Texas at Austin)"
      - author: "Ruohan Gao (Stanford University)"
      - author: "Kristen Grauman (Facebook AI Research & UT Austin)"
    all_authors: "Rishabh Garg, Ruohan Gao and Kristen Grauman"
    code: ""
    keywords:
      - word: "Binaural Audio"
      - word: "Audio visual learning"
      - word: ""
    paper: "papers/1098.pdf"
    supp: "supp/1098_supp.zip"
    abstract: "Binaural audio provides human listeners with an immersive spatial sound experience, but most existing videos lack binaural audio recordings.  We propose an audio spatialization method that draws on visual information in videos to convert their monaural (single-channel) audio to binaural audio.  Whereas existing approaches leverage visual features extracted directly from video frames, our approach explicitly disentangles the geometric cues present in the visual stream to guide the learning process.  In particular, we develop a multi-task framework that learns geometry-aware features for binaural audio generation by accounting for the  underlying room impulse response, the visual stream's coherence with the sound source(s) positions, and the consistency in geometry of the sounding objects over time.  Furthermore, we introduce a new large video dataset with realistic binaural audio simulated for real-world scanned environments.  On two datasets, we demonstrate the efficacy of our method, which achieves state-of-the-art results."
  - id: 1100
    order: 296
    poster_session: 3
    session_id: 8
    title: "Refining FFT-based Heatmap for the Detection of Cluster Distributed Targets in Satellite Images"
    authors:
      - author: "Huan Zhang (Tsinghua University)"
      - author: "Zhiyi Xu (Tsinghua University)"
      - author: "Xiaolin Han (Tsinghua University)"
      - author: "Weidong Sun (Tsinghua University)"
    all_authors: "Huan Zhang, Zhiyi Xu, Xiaolin Han and Weidong Sun"
    code: ""
    keywords:
      - word: "cluster distributed targets"
      - word: "object detection"
      - word: "FFT"
      - word: "frequency domian"
      - word: "remote sensing"
    paper: "papers/1100.pdf"
    supp: ""
    abstract: "The detection of cluster distributed targets in remotely sensed satellite images is a challenging task, as cluster is a common behavior of targets and adhesions between dense distributed targets often exist, which affect the accuracy of object detection seriously. However, the distinct distribution pattern of such cluster distributed targets in frequency domain has never been studied. In this paper, a refinement of FFT-based heatmap with multi-branches network for the detection of cluster distributed targets in the satellite images (termed as HeatNet) is proposed. More specifically, a refining method of the FFT-based heatmaps for different features in frequency domain and an attention-based feature extractor in frequency channel are proposed, to focus the attention and refine the salient regions for the cluster distributed targets. Additionally, as one complete system, a keypoint-based detection is adopted as the basic workflow to tackle with the adhesion, a scale-aware center area is conducted to tackle with the variation of scale, and an orientation discrimination is also utilized to eliminate the specificity of different targets. The effectiveness of our proposed method is validated on two public datasets, and the comparative experimental results with different state-of-the-arts object detection methods have demonstrated the superiority of this proposed method."
  - id: 1101
    order: 196
    poster_session: 2
    session_id: 5
    title: "Quality Map Fusion for Adversarial Learning"
    authors:
      - author: "Uche Osahor (West Virginia University )"
      - author: "Nasser Nasrabadi (West Virginia University)"
    all_authors: "Uche Osahor and Nasser Nasrabadi"
    code: ""
    keywords:
      - word: "quality maps"
      - word: "salient maps"
      - word: "full reference"
      - word: "image quality assessment"
      - word: "attention"
      - word: "neural network"
    paper: "papers/1101.pdf"
    supp: ""
    abstract: "Generative adversarial models that capture salient low-level features which convey visual information in correlation with the human visual system (HVS) still suffer from perceptible image degradations. The inability to convey such highly informative features can be attributed to mode collapse, convergence failure and vanishing gradients. In this paper, we improve image quality adversarially by introducing a novel quality map fusion technique that harnesses image features similar to the HVS and the perceptual properties of a deep convolutional neural network (DCNN). We extend the widely adopted l2 Wasserstein distance metric  to other  preferable quality norms derived from Banach spaces that capture richer image properties like structure, luminance, contrast and the naturalness of images. We also show that incorporating a perceptual attention mechanism (PAM) that extracts global feature embeddings from the network bottleneck with aggregated perceptual maps derived from standard image quality metrics translate to a better image quality. We also demonstrate impressive performance over other methods."
  - id: 1102
    order: 197
    poster_session: 2
    session_id: 5
    title: "Non-Autoregressive Sign Language Production with Gaussian Space"
    authors:
      - author: "Eui Jun Hwang (KAIST)"
      - author: "Jung-Ho Kim (KAIST)"
      - author: "Jong C. Park (KAIST)"
    all_authors: "Eui Jun Hwang, Jung-Ho Kim and Jong C. Park"
    code: "https://github.com/Eddie-Hwang/NSLP-G"
    keywords:
      - word: "sign language production"
      - word: "self-supervised learning"
      - word: "multi-modal translation"
      - word: "machine translation"
      - word: ""
    paper: "papers/1102.pdf"
    supp: "supp/1102_supp.zip"
    abstract: "Sign Language Production (SLP) aims to translate spoken language expressions into sign language expressions such as a sequence of sign poses or a sign video. Previous SLP works have used an autoregressive approach to learn the relationship between spoken words and sign poses. However, since the approaches work autoregressively, the decoder unintentionally regresses to the mean and even suffers from error propagation. In this work, we propose Non-Autoregressive Sign Language Production with Gaussian space (NSLP-G), a novel SLP model that uses non-autoregressive decoding to generate sign poses. To avoid direct regression, NSLP-G makes use of two phases. The first phase is to build a pose generator capable of generating various sign poses in a continuous sign pose space. At the second phase, we use a non-autoregressive Transformer to map from the source sentence to the target distribution. To validate the results of our model, we assess the quality of produced sign poses using Frechet Gesture Distance, Mean Absolute Error of Joint coordination and back-translation evaluation. Experimental results show that NSLP-G outperforms the state-of-the-art model on the RWTH-PHOENIX-Weather 2014T dataset."
  - id: 1109
    order: 404
    poster_session: 4
    session_id: 11
    title: "Learning to Predict Convolutional Filters with Guidance for Conditional Image Generation"
    authors:
      - author: "Lei Chen (Simon Fraser Univeristy)"
      - author: "Mengyao Zhai (Simon Fraser University)"
      - author: "Greg Mori (Simon Fraser University / Borealis AI)"
    all_authors: "Lei Chen, Mengyao Zhai and Greg Mori"
    code: ""
    keywords:
      - word: "conditional image generation"
      - word: "dynamic network"
    paper: "papers/1109.pdf"
    supp: "supp/1109_supp.zip"
    abstract: "Various styles naturally exist in an image domain. To generate images with certain style, previous works would usually feed a style encoding as an input to the network. However, a fixed network may lack the capability to present different styles in the target domain precisely, and the style input may also lose its impact along the generation process. In this paper, we propose Guided Filter GAN for multi-modal image-to-image translation via guided filter generation, in which filters at convolutional and deconvolutional layers are constructed dynamically from the style representation from either a target domain image or random distribution. Compared to conventional treatment of style representations being network input, the proposed approach amplifies the guidance of the given style meanwhile enhances the capacity of with dynamic parameters to adapt to different styles. We demonstrate the effectiveness of our Guided Filter GAN on various image-to-image translation tasks, where the experimental results show our approach could precisely render a reference style onto the conditional image and generate images with high fidelity and large diversity in terms of FID and LPIPS metric."
  - id: 1112
    order: 405
    poster_session: 4
    session_id: 11
    title: "3D-RETR: End-to-End Single and Multi-View 3D Reconstruction with Transformers"
    authors:
      - author: "Zai Shi (ETHZ)"
      - author: "Zhao Meng (ETHZ)"
      - author: "Yiran Xing (RWTH Aachen)"
      - author: "Yunpu Ma (LMU)"
      - author: "Roger Wattenhofer (ETH Zurich)"
    all_authors: "Zai Shi, Zhao Meng, Yiran Xing, Yunpu Ma and Roger Wattenhofer"
    code: "https://github.com/FomalhautB/3D-RETR"
    keywords:
      - word: "3D-RETR"
      - word: "3D Reconstruction"
      - word: "Transformers"
      - word: "Single and Multi-View"
      - word: "ShapeNet"
      - word: "Pix3D"
    paper: "papers/1112.pdf"
    supp: "supp/1112_supp.pdf"
    abstract: "3D reconstruction aims to reconstruct 3D objects from 2D views. Previous works for 3D reconstruction mainly focus on feature matching between views or using CNNs as backbones. Recently, Transformers have been shown effective in multiple applications of computer vision. However, whether or not Transformers can be used for 3D reconstruction is still unclear. In this paper, we fill this gap by proposing 3D-RETR, which is able to perform end-to-end 3D REconstruction with TRansformers.  3D-RETR first uses a pretrained Transformer to extract visual features from 2D input images. 3D-RETR then uses another Transformer Decoder to obtain the voxel features. A CNN Decoder then takes as input the voxel features to obtain the reconstructed objects. 3D-RETR is capable of 3D reconstruction from a single view or multiple views. Experimental results on two datasets show that 3D-RETR reaches state-of-the-art performance on 3D reconstruction. Additional ablation study also demonstrates that 3D-DETR benefits from using Transformers. 
"
  - id: 1115
    order: 297
    poster_session: 3
    session_id: 8
    title: "Semantic-Guided Radar-Vision Fusion for Depth Estimation and Object Detection"
    authors:
      - author: "Wei-Yu Lee (Ghent University - imec)"
      - author: "Ljubomir Jovanov (UGent)"
      - author: "Wilfried Philips (IPI - Ghent University - imec)"
    all_authors: "Wei-Yu Lee, Ljubomir Jovanov and Wilfried Philips"
    code: ""
    keywords:
      - word: "radar-vision fusion"
      - word: "sensor fusion"
      - word: "depth estimation"
      - word: "object detection"
      - word: "semantic segmentation"
      - word: ""
    paper: "papers/1115.pdf"
    supp: "supp/1115_supp.zip"
    abstract: "In the last decade, radar is gaining its importance in perception modules of cars and infrastructure, due to its robustness against various weather and light conditions. Although radar has numerous advantages, the properties of its output signal also make the development of fusion scheme a challenging task. Most of the prior work does not exploit full potential of fusion due to the abstraction, sparsity and low quality of radar data. 
In this paper, we propose a novel fusion scheme to overcome this limitation by introducing semantic understanding to assist the fusion process. The sparse radar point-cloud and vision data is transformed to robust and reliable depth maps and fused in a multi-scale detection network for further exploiting the complementary information. In our experiments, we evaluate the proposed fusion scheme on both depth estimation and 2D object detection problems. The quantitative and qualitative results compare favourably to the state-of-the-art and demonstrate the effectiveness of the proposed scheme. The ablation studies also show the effectiveness of the proposed components."
  - id: 1117
    order: 328
    oral_session: 7
    poster_session: 4
    session_id: 9
    title: "Learned Regularizers and Geometry for Image Denoising"
    authors:
      - author: "Stacey E Levine (Duquesne University)"
      - author: "Ryan M Cecil (Duquesne University)"
      - author: "Marcelo Bertalmío (CSIC)"
    all_authors: "Stacey E Levine, Ryan M Cecil and Marcelo Bertalmío"
    code: ""
    keywords:
      - word: "model based learning"
      - word: "partial differential equations"
      - word: "image geometry"
      - word: "level line curvature"
      - word: "image denoising"
    paper: "papers/1117.pdf"
    supp: ""
    abstract: "Recent frameworks for image denoising have demonstrated that it can be more productive to recover an image from a smoothed version of some geometric feature of the image rather than denoise an image directly. Improvements can be found both with respect to image quality metrics as well as the preservation of fine details. The challenge in working with this data is that mathematically sound mechanisms developed for handling natural image data do not necessarily apply, and this data itself can be quite ill behaved. In this work we learn both `geometric' or nonlinear higher order features and corresponding regularizers. These approaches show improvement over state-of-the-art deep learning (DL) based image denoising methods. Furthermore, the proposed DL architecture is motivated by and has the potential to feed back into mathematically sound models to solve a variety of image processing problems."
  - id: 1119
    order: 298
    poster_session: 3
    session_id: 8
    title: "Multi-Modality Task Cascade for 3D Object Detection"
    authors:
      - author: "Jinhyung Park (Carnegie Mellon University)"
      - author: "Xinshuo Weng (Carnegie Mellon University)"
      - author: "Yunze Man (Carnegie Mellon University)"
      - author: "Kris Kitani (Carnegie Mellon University)"
    all_authors: "Jinhyung Park, Xinshuo Weng, Yunze Man and Kris Kitani"
    code: "https://github.com/Divadi/MTC_RCNN"
    keywords:
      - word: "Multi Modality Learning"
      - word: "Object Detection"
      - word: "Semantic Segmentation"
    paper: "papers/1119.pdf"
    supp: "supp/1119_supp.zip"
    abstract: "Point clouds and RGB images are naturally complementary modalities for 3D visual understanding - the former provides sparse but accurate locations of points on objects, while the latter contains dense color and texture information. Despite this potential for close sensor fusion, many methods train two models in isolation and use simple feature concatenation to represent 3D sensor data. This separated training scheme results in potentially sub-optimal performance and prevents 3D tasks from being used to benefit 2D tasks that are often useful on their own. To provide a more integrated approach, we propose a novel Multi-Modality Task Cascade network (MTC-RCNN) that leverages 3D box proposals to improve 2D segmentation predictions, which are then used to further refine the 3D boxes. We show that including a 2D network between two stages of 3D modules significantly improves both 2D and 3D task performance. Moreover, to prevent the 3D module from over-relying on the overfitted 2D predictions, we propose a dual-head 2D segmentation training and inference scheme, allowing the 2nd 3D module to learn to interpret imperfect 2D segmentation predictions. Evaluating our model on the challenging SUN RGB-D dataset, we improve upon state-of-the-art results of both single modality and fusion networks by a large margin (+3.8 mAP@0.5). Code will be released at https://github.com/Divadi/MTC_RCNN."
  - id: 1124
    order: 299
    poster_session: 3
    session_id: 8
    title: "Selective Pseudo-Labeling with Reinforcement Learning for Semi-Supervised Domain Adaptation"
    authors:
      - author: "Bingyu Liu (Beijing University of Posts and Telecommunications)"
      - author: "Yuhong Guo (Carleton University)"
      - author: "Jieping Ye (Beike & University of Michigan)"
      - author: "Weihong Deng (Beijing University of Posts and Telecommunications)"
    all_authors: "Bingyu Liu, Yuhong Guo, Jieping Ye and Weihong Deng"
    code: ""
    keywords:
      - word: "semi-supervised domain adaptation"
      - word: "reinforcement learning"
      - word: "pseudo-label"
    paper: "papers/1124.pdf"
    supp: ""
    abstract: "Recent domain adaptation methods have demonstrated impressive improvement on unsupervised domain adaptation problems. However, in the semi-supervised domain adaptation (SSDA) setting where the target domain has a few labeled instances available, these methods can fail to improve performance. Inspired by the effectiveness of pseudo-labels in domain adaptation, we propose a reinforcement learning based selective pseudo-labeling method for SSDA. It is difficult for conventional pseudo-labeling methods to balance the correctness and representativeness of pseudo-labeled data. To address this limitation, we develop a deep Q-learning model to select both accurate and representative pseudo-labeled instances. Moreover, motivated by large margin loss's capacity on learning discriminative features with little data, we further propose a novel target margin loss for our base model training to improve its discriminability. Our proposed method is evaluated on several benchmark datasets for SSDA, and demonstrates superior performance to all the comparison methods."
  - id: 1126
    order: 76
    poster_session: 1
    session_id: 2
    title: "NOD: Taking a Closer Look at Detection under Extreme Low-Light Conditions with Night Object Detection Dataset"
    authors:
      - author: "Igor Morawski (National Taiwan Univeristy)"
      - author: "Yu-An Chen (National Taiwan University)"
      - author: "Yu-Sheng Lin (National Taiwan University)"
      - author: "Winston H. Hsu (National Taiwan University)"
    all_authors: "Igor Morawski, Yu-An Chen, Yu-Sheng Lin and Winston H. Hsu"
    code: "https://github.com/igor-morawski/NOD"
    keywords:
      - word: "low-light"
      - word: "dataset"
      - word: "poor visibility"
      - word: "object detection"
      - word: "image enhancement"
      - word: "adverse"
      - word: "dark"
      - word: "night"
      - word: ""
    paper: "papers/1126.pdf"
    supp: "supp/1126_supp.zip"
    abstract: "Recent work indicates that, besides being a challenge in producing perceptually pleasing images, low light proves more difficult for machine cognition than previously thought. In our work, we take a closer look at object detection in low light. First, to support the development and evaluation of new methods in this domain, we present a high-quality large-scale Night Object Detection (NOD) dataset showing dynamic scenes captured on the streets at night. Next, we directly link the lighting conditions to perceptual difficulty and identify what makes low light problematic for machine cognition. Accordingly, we provide instance-level annotation for a subset of the dataset for an in-depth evaluation of future methods. We also present an analysis of the baseline model performance to highlight opportunities for future research and show that low light is a non-trivial problem that requires special attention from the researchers. Further, to address the issues caused by low light, we propose to incorporate an image enhancement module into the object detection framework and two novel data augmentation techniques. Our image enhancement module is trained under the guidance of the object detector to learn image representation optimal for machine cognition rather than for the human visual system. Finally, experimental results confirm that the proposed method shows consistent improvement of the performance on low-light datasets."
  - id: 1127
    order: 9
    oral_session: 2
    poster_session: 1
    session_id: 1
    title: "Learning Attention Map for 3D Human Recovery from a Single RGB Image"
    authors:
      - author: "Peng Xu (Capital Normal University)"
      - author: "Na Jiang (Information Engineering College, Capital Normal University)"
      - author: "Jun Li (Capital Normal University)"
      - author: "Zhiping Shi (Capital Normal University)"
    all_authors: "Peng Xu, Na Jiang, Jun Li and Zhiping Shi"
    code: ""
    keywords:
      - word: "3D Human Recover"
      - word: "Human Parsing"
      - word: "Depth Estimation"
      - word: ""
    paper: "papers/1127.pdf"
    supp: "supp/1127_supp.zip"
    abstract: "3D human recovery from a single RGB image is a promising topic in virtual reality, augmented reality and computer vision, which focuses on estimating 3D pose and shape of human from a 2D image. Due to the lack of depth and local information, the task remains challenging. Targeting to solve these problems, this work proposes a LAMNet with three branches that learning attention map from depth and parsing features for 3D human recovery. The first branch explicitly leverages the depth and pose cues to learn depth attention map, which alleviates the recovery error between 3D space and 2D plane. The second branch explicitly leverages parsing cues as the local information of human, which supplements the local or edge details of 3D recovery. The last branch is the main branch which is responsible for estimating 3D pose and shape of human. Inspired by attention mechanism, it designs an attention aware fusion to integrating depth, parsing and global image cues, which effectively improves the precision of 3D recovery, especially in details and different perspectives. Extensive experimental results demonstrate that our proposed approach significantly outperforms most state-of-the-art methods on the popular Human3.6m, UP-3D, and 3DPW datasets."
  - id: 1137
    order: 77
    poster_session: 1
    session_id: 2
    title: "Regression as a Simple Yet Effective Tool for Self-supervised Knowledge Distillation"
    authors:
      - author: "K L Navaneet  (University of Maryland Baltimore County)"
      - author: "Soroush Abbasi Koohpayegani (University of Maryland Baltimore County)"
      - author: "Ajinkya B Tejankar (UMBC)"
      - author: "Hamed Pirsiavash (UC Davis)"
    all_authors: "K L Navaneet, Soroush Abbasi Koohpayegani, Ajinkya B Tejankar and Hamed Pirsiavash"
    code: "https://github.com/UCDvision/simreg"
    keywords:
      - word: "knowledge distillation"
      - word: "self supervised distillation"
      - word: "regression"
    paper: "papers/1137.pdf"
    supp: "supp/1137_supp.zip"
    abstract: "Feature regression is a simple way to distill larger neural network models to lighter ones. In this work we show that, with simple changes to network architecture, regression can outperform more complex state-of-the-art approaches for knowledge distillation from self-supervised models. Surprisingly, the addition of multi-layer perceptron head to CNN backbone is beneficial even if used only during distillation and discarded for downstream task. Deeper non-linear projections can thus be used to accurately mimic the teacher without changing inference architecture and time. We utilize independent projection heads to simultaneously distill multiple teacher networks. Additionally, we find that using the same weakly augmented image as input for both teacher and student networks is crucial for distillation. Experiments on large scale ImageNet dataset demonstrate the efficacy of the proposed changes in various self-supervised distillation settings. "
  - id: 1139
    order: 406
    poster_session: 4
    session_id: 11
    title: "Scan-IT: Domain segmentation of spatial transcriptomics images by graph neural network"
    authors:
      - author: "Zixuan Cang (University of California, Irvine)"
      - author: "Xinyi Ning (Tsinghua University)"
      - author: "Annika Nie (University High School)"
      - author: "Min Xu (Carnegie Mellon University)"
      - author: "Jing Zhang (UC Irvine)"
    all_authors: "Zixuan Cang, Xinyi Ning, Annika Nie, Min Xu and Jing Zhang"
    code: "https://github.com/zcang/SCAN-IT"
    keywords:
      - word: "spatial transcriptomics"
      - word: "domain segmentation"
      - word: "deep graph infomax"
      - word: "clustering"
      - word: ""
    paper: "papers/1139.pdf"
    supp: "supp/1139_supp.zip"
    abstract: "Complex biological tissues consist of numerous cells in a highly coordinated manner and carry out various biological functions. Therefore, segmenting a tissue into spatial and functional domains is critically important for understanding and controlling the biological functions. The emerging spatial transcriptomic technologies allow simultaneous measurements of thousands of genes with precise spatial information, providing an unprecedented opportunity for dissecting biological tissues. However, how to utilize such noisy, sparse, and high dimensional data for tissue segmentation remains a major challenge. Here, we develop a deep learning-based method, named SCAN-IT by transforming the spatial domain identification problem into an image segmentation problem, with cells mimicking pixels and expression values of genes within a cell representing the color channels. Specifically, SCAN-IT relies on geometric modeling, graph neural networks, and an informatics approach, DeepGraphInfomax. We demonstrate that SCAN-IT can handle datasets from a wide range of spatial transcriptomics techniques, including the ones with high spatial resolution but low gene coverage as well as those with low spatial resolution but high gene coverage. We show that SCAN-IT outperforms state-of-the-art methods using a benchmark dataset with ground truth domain annotations."
  - id: 1141
    order: 407
    poster_session: 4
    session_id: 11
    title: "Distilling Dynamic Spatial Relation Network for Human Pose Estimation"
    authors:
      - author: "Kewei Wu (Hefei University of Technology)"
      - author: "Tao Wang (Hefei University of Technology)"
      - author: "Zhao Xie (Hefei University of Technology)"
      - author: "Dan Guo (Hefei University of Technology)"
    all_authors: "Kewei Wu, Tao Wang, Zhao Xie and Dan Guo"
    code: ""
    keywords:
      - word: "pose estimation"
      - word: "graph model"
      - word: "dynamic propagation"
      - word: "spatial relation distribution"
      - word: "knowledge distillation"
    paper: "papers/1141.pdf"
    supp: ""
    abstract: "Human pose estimation is a challenging task that requires the comprehension of the pose structure. This work can refer to spatial relation inference in a pose structure model; how to model the dynamic spatial relation against various unreliable joints is critical. To this end, we propose a Distilling Dynamic Spatial Relation network (DDSR), which builds pose-based graph representation by exploiting the feature of spatial relation from the location distribution of joints. We use a dynamic message propagation mechanism to update the spatial relation on edges. Specifically, to filter out the noisy predictions, we select the joints with high confidence; to enhance the spatial relation in a large receptive field, we propagate multi-stage messages among joints. Besides, to reduce the computation cost of the multi-stage message propagation, we design a cross-resolution distillation framework. We use a new spatial distillation loss to verify the spatial relation between the teacher model and the student model. Experimental results on COCO and MPII datasets show that our method is superior to the state-of-the-art methods. The visualization results further verify the interpretability of our spatial relation."
  - id: 1142
    order: 78
    poster_session: 1
    session_id: 2
    title: "In-N-Out: Towards Good Initialization for Inpainting and Outpainting"
    authors:
      - author: "Changho Jo (KAIST)"
      - author: "Woobin Im (KAIST)"
      - author: "Sungeui Yoon (KAIST)"
    all_authors: "Changho Jo, Woobin Im and Sungeui Yoon"
    code: "https://github.com/timegate/In_N_Out"
    keywords:
      - word: "inpainting"
      - word: "outpainting"
      - word: "extrapolation"
      - word: "environment map estimation"
      - word: "self-supervised learning"
      - word: "transfer learning"
      - word: ""
    paper: "papers/1142.pdf"
    supp: "supp/1142_supp.zip"
    abstract: "In computer vision, recovering spatial information by filling in masked regions, e.g., inpainting, has been widely investigated for its usability and wide applicability to other various applications: image inpainting, image extrapolation, and environment map estimation. Most of them are studied separately depending on the applications. Our focus, however, is on accommodating the opposite task, e.g., image outpainting, which would benefit the target applications, e.g., image inpainting. Our self-supervision method, In-N-Out, is summarized as a training approach that leverages the knowledge of the opposite task into the target model. We empirically show that In-N-Out 
-- which explores the complementary information -- effectively takes advantage over the traditional pipelines where
only task-specific learning takes place in training. In experiments, we compare our method to the traditional procedure and analyze the effectiveness of our method on different applications: image inpainting, image extrapolation, and environment map estimation. For these tasks, we demonstrate that In-N-Out consistently improves the performance of the recent works with In-N-Out self-supervision to their training procedure. Also, we show that our approach achieves better results than an existing training approach for outpainting."
  - id: 1157
    order: 408
    poster_session: 4
    session_id: 11
    title: "Diffeomorphism Matching for Fast Unsupervised Pretraining on Radiographs"
    authors:
      - author: "Thanh Minh Huynh (VinBrain)"
      - author: "Chanh D Tr Nguyen (VinBrain)"
      - author: "Ta Duc Huy (Vinbrain)"
      - author: "Huyen Hoang (Vinbrain)"
      - author: "Trung Bui (Individual)"
      - author: "QUOC HUNG TRUONG (VINBRAIN)"
    all_authors: "Thanh Minh Huynh, Chanh D Tr Nguyen, Ta Duc Huy, Huyen Hoang, Trung Bui and QUOC HUNG TRUONG"
    code: "https://github.com/jokingbear/DM.git"
    keywords:
      - word: "pretraining"
      - word: "self-supervised learning"
      - word: "unsupervised learning"
      - word: "knowledge distillation"
    paper: "papers/1157.pdf"
    supp: "supp/1157_supp.zip"
    abstract: "Unsupervised pretraining is an approach that leverages a large unlabeled data pool to learn data features. However, it requires billion-scale datasets and a month-long training time to surpass its supervised counterpart on fine-tuning in many computer vision tasks. In this study, we propose a novel method, Diffeomorphism Matching (DM), to overcome those challenges. The proposed method combines self-supervised learning and knowledge distillation to equivalently map the feature space of a student model to that of a big pretrained teacher model. On the Chest X-ray dataset, our method alleviates the need to acquire billions of radiographs and substantially reduces pretraining time by 95%. In addition, our pretrained model outperforms other pretrained models by at least 4.2% in F1 score on the CheXpert dataset and 0.7% in Dice score on the SIIM Pneumothorax dataset."
  - id: 1158
    order: 198
    poster_session: 2
    session_id: 5
    title: "Source-free Unsupervised Domain Adaptation with Surrogate Data Generation"
    authors:
      - author: "Hao Yan (Carleton Univeristy)"
      - author: "Yuhong Guo (Carleton University)"
      - author: "Chunsheng Yang (NRC)"
    all_authors: "Hao Yan, Yuhong Guo and Chunsheng Yang"
    code: "https://github.com/cnyanhao/SSFTSSD"
    keywords:
      - word: "source-free unsupervised domain adaptation"
      - word: "surrogate data generation"
    paper: "papers/1158.pdf"
    supp: ""
    abstract: "Source-free unsupervised domain adaptation aims to learn a model that generalizes well on a target domain given the pre-trained source model and unlabeled target data. Traditional unsupervised domain adaptation methods are mostly not applicable to this setting since no source data are available. To tackle this problem, we propose to generate labeled surrogate source training data from the source model by fixing the model and optimizing the inputs. To avoid naive local fittings to individual instances and in light of the model optimization process, we further enforce model gradient based global fitting constraints on the whole dataset generation and solve the formulated optimization problem using an ADMM algorithm. The generated labeled source training data can then be used to deploy existing unsupervised domain adaptation methods. Furthermore, we propose to incorporate the unlabeled target data into the domain adaptation process to improve generalization in the target domain with a mutual information loss. Experiments show that our proposed method can achieve the state-of-the-art results on benchmark datasets."
  - id: 1165
    order: 199
    poster_session: 2
    session_id: 5
    title: "Evolving Image Compositions for Feature Representation Learning"
    authors:
      - author: "Paola Cascante-Bonilla (University of Virginia)"
      - author: "Arshdeep Sekhon (University of Virginia)"
      - author: "Yanjun Qi (University of Virginia)"
      - author: "Vicente Ordonez (Rice University)"
    all_authors: "Paola Cascante-Bonilla, Arshdeep Sekhon, Yanjun Qi and Vicente Ordonez"
    code: ""
    keywords:
      - word: "data augmentation"
      - word: "representation learning"
      - word: "genetic search"
      - word: "evolutionary search"
      - word: "optimization"
      - word: "regularization"
      - word: ""
    paper: "papers/1165.pdf"
    supp: "supp/1165_supp.zip"
    abstract: "Convolutional neural networks for visual recognition require large amounts of training samples and usually benefit from data augmentation. This paper proposes PatchMix, a data augmentation method that creates new samples by composing patches from pairs of images in a grid-like pattern. These new samples' ground truth labels are set as proportional to the number of patches from each image. We then add a set of additional losses at the patch-level to regularize and to encourage good representations at both the patch and image levels. A ResNet-50 model trained on ImageNet using PatchMix exhibits superior transfer learning capabilities across a wide array of benchmarks. Although PatchMix can rely on random pairings and random grid-like patterns for mixing, we explore evolutionary search as a guiding strategy to discover optimal grid-like patterns and image pairing jointly. For this purpose, we conceive a fitness function that bypasses the need to re-train a model to evaluate each choice.  In this way, PatchMix outperforms a base model on CIFAR-10 (+1.91), CIFAR-100 (+5.31), Tiny Imagenet (+3.52), and ImageNet (+1.16)."
  - id: 1167
    order: 329
    oral_session: 7
    poster_session: 4
    session_id: 9
    title: "Unsupervised Image Denoising with Frequency Domain Knowledge"
    authors:
      - author: "Nahyun Kim (KAIST)"
      - author: "Donggon Jang (KAIST)"
      - author: "Sunhyeok Lee (KAIST)"
      - author: "Bomi Kim (KAIST)"
      - author: "Daeshik Kim (KAIST)"
    all_authors: "Nahyun Kim, Donggon Jang, Sunhyeok Lee, Bomi Kim and Daeshik Kim"
    code: "https://github.com/jdg900/UID-FDK"
    keywords:
      - word: "unsupervised image denoising"
      - word: "Fourier transform"
      - word: "frequency knowledge"
      - word: "GAN-based denoising"
      - word: ""
    paper: "papers/1167.pdf"
    supp: "supp/1167_supp.zip"
    abstract: "Supervised learning-based methods yield robust denoising results, yet they are inherently limited by the need for large-scale clean/noisy paired datasets. The use of unsupervised denoisers, on the other hand, necessitates a more detailed understanding of the underlying image statistics. In particular, it is well known that apparent differences between clean and noisy images are most prominent on high-frequency bands, justifying the use of low-pass filters as part of conventional image preprocessing steps. However, most learning-based denoising methods utilize only one-sided information from the spatial domain without considering frequency domain information. To address this limitation, in this study we propose a frequency-sensitive unsupervised denoising method. To this end,  a generative adversarial network (GAN) is used as a base structure. Subsequently, we include spectral discriminator and frequency reconstruction loss to transfer frequency knowledge into the generator. Results using natural and synthetic datasets indicate that our unsupervised learning method augmented with frequency information achieves state-of-the-art denoising performance, suggesting that frequency domain information could be a viable factor in improving the overall performance of unsupervised learning-based methods."
  - id: 1171
    order: 300
    poster_session: 3
    session_id: 8
    title: "V3GAN: Decomposing Background, Foreground and Motion for Video Generation"
    authors:
      - author: "Arti Keshari (Indian Institute of Technology, Madras)"
      - author: "Sonam Gupta (IIT Madras)"
      - author: "Sukhendu Das (Indian Institute of Technology, Madras)"
    all_authors: "Arti Keshari, Sonam Gupta and Sukhendu Das"
    code: "https://github.com/sampriti111/V3GAN"
    keywords:
      - word: "video generation"
      - word: "unconditional video generation"
      - word: "shuffling loss"
      - word: "feature level masking"
      - word: "unsupervised learning"
      - word: "GAN"
      - word: "foreground"
      - word: "background"
      - word: "motion decomposition"
      - word: ""
    paper: "papers/1171.pdf"
    supp: "supp/1171_supp.zip"
    abstract: "Video generation is a challenging task that requires modeling plausible spatial and temporal dynamics in a video.  Inspired by how humans perceive a video by grouping a scene into moving and stationary components, we propose a method that decomposes the task of video generation into the synthesis of foreground, background and motion. Foreground and background together describe the appearance, whereas motion specifies how the foreground moves in a video over time.  We propose V3GAN, a novel three-branch generative adversarial network where two branches model foreground and background information, while the third branch models the temporal information without any supervision.  The foreground branch is augmented with our novel feature level masking layer that aids in learning an accurate mask for foreground and background separation.   To encourage motion consistency, we further propose a shuffling loss for the video discriminator.  Extensive quantitative and qualitative analysis on synthetic as well as real-world benchmark datasets demonstrates that V3GAN outperforms the state-of-the-art methods by a significant margin."
  - id: 1187
    order: 301
    poster_session: 3
    session_id: 8
    title: "Transferring Domain-Agnostic Knowledge in Video Question Answering"
    authors:
      - author: "Tianran Wu (Osaka University)"
      - author: "Noa Garcia (Osaka University)"
      - author: "Mayu Otani (CyberAgent, Inc.)"
      - author: "Chenhui Chu (Kyoto University)"
      - author: "Yuta Nakashima (Osaka University)"
      - author: "Haruo Takemura (Osaka University)"
    all_authors: "Tianran Wu, Noa Garcia, Mayu Otani, Chenhui Chu, Yuta Nakashima and Haruo Takemura"
    code: ""
    keywords:
      - word: "video question answering"
      - word: "transfer learning"
      - word: ""
    paper: "papers/1187.pdf"
    supp: "supp/1187_supp.zip"
    abstract: "Video question answering (VideoQA) is designed to answer a given question based on a relevant video clip. The current available large-scale datasets have made it possible to formulate VideoQA as the joint understanding of visual and language information. However, this training procedure is costly and still less competent with human performance, which calls for the efficient transfer across existing large-scale datasets. In this paper, we investigate a transfer learning method by the introduction of domain-agnostic knowledge and domain-specific knowledge. First, we develop a novel transfer learning framework, which finetunes the pre-trained model by applying domain-agnostic knowledge as the medium. Second, we construct a new VideoQA dataset with $21,412$ human-generated question-answer samples for comparable transfer of knowledge. Our experiments show that: (i) domain-agnostic knowledge is transferable and (ii) our proposed transfer learning framework can boost VideoQA performance effectively."
  - id: 1188
    order: 302
    poster_session: 3
    session_id: 8
    title: "Unsupervised Spatio-temporal Latent Feature Clustering for Multiple-object Tracking and Segmentation"
    authors:
      - author: "Abubakar  Siddique (Marquette University)"
      - author: "Reza Jalil Mozhdehi (Marquette University)"
      - author: "Henry Medeiros (Marquette University)"
    all_authors: "Abubakar  Siddique, Reza Jalil Mozhdehi and Henry Medeiros"
    code: " https://Siddiquemu@bitbucket.org/Siddiquemu/usc_mots.git"
    keywords:
      - word: "Unsupervised learning"
      - word: "Subspace clustering"
      - word: "Heterogeneous autoencoder"
      - word: "Constraints k-means"
      - word: "Multi-task learning"
      - word: "Uncertainty learning"
      - word: "MOTS"
    paper: "papers/1188.pdf"
    supp: ""
    abstract: "Assigning consistent temporal identifiers to multiple moving objects in a video sequence is a challenging problem. A solution to that problem would have immediate ramifications in multiple object tracking and segmentation problems. We propose a strategy that treats the temporal identification task as a spatio-temporal clustering problem. We propose an unsupervised learning approach using a convolutional and fully connected autoencoder, which we call deep heterogeneous autoencoder, to learn discriminative features from segmentation masks and detection bounding boxes. We extract masks and their corresponding bounding boxes from a pretrained instance segmentation network and train the autoencoders jointly using task-dependent uncertainty weights to generate common latent features. We then construct constraints graphs that encourage associations among objects that satisfy a set of known temporal conditions. The feature vectors and the constraints graphs are then provided to the kmeans clustering algorithm to separate the corresponding data points in the latent space. We evaluate the performance of our method using challenging synthetic and real-world multiple-object video datasets. Our results show that our technique outperforms several state-of-the-art methods."
  - id: 1190
    order: 303
    poster_session: 3
    session_id: 8
    title: "Label2im: Knowledge Graph Guided Image Generation from Labels"
    authors:
      - author: "Hewen Xiao (Dalian University of Technology)"
      - author: "Yuqiu Kong (Dalian University of Technology)"
      - author: "Hongchen Tan (Beijing University Of Technology)"
      - author: "Xiuping Liu (Dalian University of Technology)"
      - author: "Baocai Yin (Dalian University of Technology)"
    all_authors: "Hewen Xiao, Yuqiu Kong, Hongchen Tan, Xiuping Liu and Baocai Yin"
    code: ""
    keywords:
      - word: "scene generation"
      - word: "image generation"
      - word: ""
    paper: "papers/1190.pdf"
    supp: "supp/1190_supp.zip"
    abstract: "Most recent generation methods synthesize images from either complex textual descriptions or scene graphs. However, users need to elaborate attributes and relationships of objects in the scene, and scene graphs are more difficult to obtain. To simplify the burden of users, in this work, we propose a Label2im model to generate images from object labels directly with the help of a Knowledge Graph (KG), e.g. Visual Genome. To acquire rational interactions between objects, we explore possible relationships from the KG. Considering that there is a large gap between the label domain and image domain, we propose to learn knowledge representations of the scene graph from the KG to ensure the semantic consistency. First, given several object labels, we design a Scene Graph Selection Module (SGSM) to explore interactions between objects in the KG and generate a set of scene graphs. Second, the structure representation and knowledge embedding of the scene graph are learned and integrated in the Scene Graph Representation Module (SGRM), which leads to rational scene layouts. Based on the scene layouts and KG, we employ the Cascaded Refinement Network (CRN) to generate the final image. To encode knowledge information in the generation process, we propose a Triplet Attention Module (TAM) which is embedded in the CRN. We verify the effectiveness of the proposed method on the Visual Genome dataset and demonstrate that our method is able to generate complex images with rich content and fine details."
  - id: 1193
    order: 79
    poster_session: 1
    session_id: 2
    title: "Object Re-identification Using Teacher-Like and Light Students"
    authors:
      - author: "Yi Xie (College of Engineering, Huaqiao University)"
      - author: "Hanxiao Wu (Huaqiao University )"
      - author: "Fei Shen (Nanjing University Of Science And Technology)"
      - author: "Jianqing Zhu ( College of Engineering, Huaqiao University)"
      - author: "Huanqiang Zeng (Huaqiao University)"
    all_authors: "Yi Xie, Hanxiao Wu, Fei Shen, Jianqing Zhu and Huanqiang Zeng"
    code: ""
    keywords:
      - word: "object re-identification"
      - word: "knowledge distillation"
      - word: "pruning"
      - word: "re-parameterization"
    paper: "papers/1193.pdf"
    supp: ""
    abstract: "Recent object re-identification approaches tend to use heavy models (e.g., ResNet-50 or ResNet-101) to guarantee performance, which requires massive computations. Although knowledge distillation (KD) methods can be applied to learn light student models from heavy teacher models, numerous existing KD research has shown that significant architectural differences between students and teachers prevent students from achieving good accuracy. For that, we propose a joint distillation and pruning (JDP) method to learn teacher-like and light (TLL) students for object re-identification. Given a heavy teacher, JDP applies a student that holds the same overall architecture but a tiny local adjustment. Specifically, we design a pruner-convolution-pruner (PCP) block to replace a K ×K convolutional layer of the student network. The pruner is a 1×1 convolutional layer and initialized identity matrices to maintain the original output. During the student training phase, the student is jointly supervised by the KD loss and group LASSO loss functions. The KD loss function promotes the student to learn dark knowledge from the teacher. The group LASSO loss function enforces pruners to realize the channel sparsity for filtering unimportant channels. A PCP block can be simplified into a light convolutional layer during the testing phase since multiple linearly convolutional layers in series can be equivalently merged into one convolutional layer. As a result, a TLL student is acquired. Extensive experiments show that our JDP method has superiority in terms of accuracy and computations , e.g., on the Veri-776 dataset, given the ResNet-101 as a teacher, our TLL student saves 80.00% parameters and 78.52% FLOPs, while the mAP only drops by 0.17%."
  - id: 1200
    order: 304
    poster_session: 3
    session_id: 8
    title: "TNT: Text-Conditioned Network with Transductive Inference for Few-Shot Video Classification"
    authors:
      - author: "Andrés Villa (Pontificia Universidad Católica de Chile)"
      - author: "Juan-Manuel Perez-Rua (Facebook AI)"
      - author: "Vladimir Araujo (Pontificia Universidad Católica de Chile)"
      - author: "Juan Carlos Niebles (Stanford University)"
      - author: "Victor A Escorcia (Samsung AI Center)"
      - author: "Alvaro Soto (Universidad Catolica de Chile)"
    all_authors: "Andrés Villa, Juan-Manuel Perez-Rua, Vladimir Araujo, Juan Carlos Niebles, Victor A Escorcia and Alvaro Soto"
    code: ""
    keywords:
      - word: "Few-Shot Learning"
      - word: "Adaptive Network"
      - word: "Multimodal Information"
      - word: "Action Classification"
      - word: "Transductive Classification"
    paper: "papers/1200.pdf"
    supp: "supp/1200_supp.zip"
    abstract: "Recently, few-shot video classification has received an increasing interest. Current approaches mostly focus on effectively exploiting the temporal dimension in videos to improve learning under low data regimes. However, most works have largely ignored that videos are often accompanied by rich textual descriptions that can also be an essential source of information to handle few-shot recognition cases. In this paper, we propose to leverage these human-provided textual descriptions as privileged information when training a few-shot video classification model. Specifically, we formulate a text-based task conditioner to adapt video features to the few-shot learning task. Furthermore, our model follows a transductive setting to improve the task-adaptation ability of the model by using the support textual descriptions and query instances to update a set of class prototypes. Our model achieves state-of-the-art performance on four challenging benchmarks commonly used to evaluate few-shot video action classification models."
  - id: 1202
    order: 409
    poster_session: 4
    session_id: 11
    title: "Efficient Cross-Modal Retrieval via Deep Binary Hashing and Quantization"
    authors:
      - author: "Yang Shi (Rakuten)"
      - author: "Young Joo Chung (Rakuten)"
    all_authors: "Yang Shi and Young Joo Chung"
    code: ""
    keywords:
      - word: "cross modal retrieval"
      - word: "hashing"
      - word: "quantization"
    paper: "papers/1202.pdf"
    supp: "supp/1202_supp.zip"
    abstract: "Cross-modal retrieval aims to search for data with similar semantic meanings across different content modalities. However, cross-modal retrieval requires huge amounts of storage and retrieval time since it needs to process data in multiple modalities. Existing works focused on learning single-source compact features such as binary hash codes that preserve similarities between different modalities. In this work, we propose a jointly learned deep hashing and quantization network (HQ) for cross-modal retrieval. We simultaneously learn binary hash codes and quantization codes to preserve semantic information in multiple modalities by an end-to-end deep learning architecture. At the retrieval step, binary hashing is used to retrieve a subset of items from the search space, then quantization is used to re-rank the retrieved items. We theoretically and empirically show that this two-stage retrieval approach provides faster retrieval results while preserving accuracy. Experimental results on the NUS-WIDE, MIR-Flickr, and Amazon datasets demonstrate that HQ achieves boosts of more than 7% in precision compared to supervised neural network-based compact coding models."
  - id: 1204
    order: 200
    poster_session: 2
    session_id: 5
    title: "Learnable Discrete Wavelet Pooling (LDW-Pooling) for Convolutional Networks"
    authors:
      - author: "BorShiun Wang (Institute of Intelligent Systems, National Chiao Tung University)"
      - author: "Jun-Wei Hsieh (College of Artificial Intelligence and Green Energy )"
      - author: "Ping-Yang Chen (Department of Computer Science, National Yang Ming Chiao Tung University)"
      - author: "Ming-Ching Chang (University at Albany - SUNY)"
      - author: "Lipeng Ke (University at Buffalo)"
      - author: "Siwei Lyu (University at Buffalo)"
    all_authors: "BorShiun Wang, Jun-Wei Hsieh, Ping-Yang Chen, Ming-Ching Chang, Lipeng Ke and Siwei Lyu"
    code: ""
    keywords:
      - word: "Classification"
      - word: "Pooling Method"
      - word: ""
    paper: "papers/1204.pdf"
    supp: "supp/1204_supp.zip"
    abstract: "Pooling is a simple but important layer in modern deep CNN architectures for feature aggregation and extraction. Typical CNN design focuses on the conv layers and activation functions, while leaving the pooling layers without suitable options. We introduce the Learning Discrete Wavelet Pooling (LDW-Pooling) that can be applied universally to replace standard pooling operations to better extract features with improved accuracy and efficiency. Motivated from the wavelet theory, we adopt the low-pass (L) and high-pass (H) filters horizontally and vertically for pooling on a 2D feature map. Feature signals are decomposed into four (LL, LH, HL, HH) subbands to better retain features and avoid information dropping. The wavelet transform ensures features after pooling can be fully preserved and recovered. We next adopt an energy-based attention learning to fine-select crucial and representative features. LDW-Pooling is effective and efficient when compared with other state-of-the-art pooling techniques such as WaveletPooling and LiftPooling. Extensive experimental validation shows that LDW-Pooling can be applied to a wide range of standard CNN architectures in replacing standard (max, mean, mixed, and stochastic) pooling operations and consistently outperforming them."
  - id: 1206
    order: 80
    poster_session: 1
    session_id: 2
    title: "From Under- to Overexposure: Single Image Contrast Enhancement via Semi-Supervised Learning"
    authors:
      - author: "Ziwen Li (School of Artificial Intelligence and Automation,Huazhong University of Science and Technology)"
      - author: "Yuehuan Wang (School of Artificial Intelligent and Automation, Huazhong University of Science and Technology, Wuhan, P.R. China)"
      - author: "RuoWang Chang (School of Automation, Huazhong University of Science and Technology, Wuhan, P.R. China)"
    all_authors: "Ziwen Li, Yuehuan Wang and RuoWang Chang"
    code: ""
    keywords:
      - word: "contrast enhancement"
      - word: "semi-supervised learning"
      - word: "exposure correction"
      - word: ""
    paper: "papers/1206.pdf"
    supp: ""
    abstract: "Poor lighting conditions result in photographs with low contrast. Most existing methods focus on low-light enhancement and perceptual quality improvement, and have not taken both under- and overexposure into consideration. In this paper, we propose a novel semi-supervised learning method for single image contrast enhancement. The supervised branch is trained using paired data under the constraint of supervised losses. While in the unsupervised branch, we explore content consistency and illumination prior as loss functions to train the network. The advantages of the proposed approach are two folds. First, guided by ground truth images, the supervised branch learns well to preserve image details and suppress noise. Second, the unsupervised branch learns to adapt to more illumination intensities and diverse illumination environments, which bridges the gap between various lighting conditions. With the help of the semi-supervised strategy, our method uses a single model to enhance both underexposed and overexposed images, and generalizes well to various lighting conditions. Experimental results show that the proposed method outperforms the state-of-the-art methods quantitatively and qualitatively."
  - id: 1213
    order: 2
    oral_session: 1
    poster_session: 1
    session_id: 0
    title: "Taming Visually Guided Sound Generation"
    authors:
      - author: "Vladimir Iashin (Tampere University)"
      - author: "Esa Rahtu (Tampere University)"
    all_authors: "Vladimir Iashin and Esa Rahtu"
    code: "https://github.com/v-iashin/SpecVQGAN"
    keywords:
      - word: "multi-modal learning"
      - word: "audio generation"
      - word: "video understanding"
      - word: "transformer"
      - word: "VQVAE"
      - word: "MelGAN"
      - word: "perceptual loss"
      - word: "generation metrics"
      - word: "VGGSound"
      - word: "VAS"
      - word: ""
    paper: "papers/1213.pdf"
    supp: "supp/1213_supp.zip"
    abstract: "Recent advances in visually-induced audio generation are based on sampling short, low-fidelity, and one-class sounds. Moreover, sampling 1 second of audio from the state-of-the-art model takes minutes on a high-end GPU. In this work, we propose a single model capable of generating visually relevant, high-fidelity sounds prompted with a set of frames from open-domain videos in less time than it takes to play it on a single GPU.

We train a transformer to sample a new spectrogram from the pre-trained spectrogram codebook given the set of video features. The codebook is obtained using a variant of VQGAN trained to produce a compact sampling space with a novel spectrogram-based perceptual loss. The generated spectrogram is transformed into a waveform using a window-based GAN that significantly speeds up generation. Considering the lack of metrics for automatic evaluation of generated spectrograms, we also build a family of metrics called FID and MKL. These metrics are based on a novel sound classifier, called Melception, and designed to evaluate the fidelity and relevance of open-domain samples.

Both qualitative and quantitative studies are conducted on small- and large-scale datasets to evaluate the fidelity and relevance of generated samples. We also compare our model to the state-of-the-art and observe a substantial improvement in quality, size, and computation time. Code, demo, and samples: v-iashin.github.io/SpecVQGAN"
  - id: 1216
    order: 410
    poster_session: 4
    session_id: 11
    title: "Planar Shape Based Registration for Multi-modal Geometry"
    authors:
      - author: "Muxingzi Li (Inria)"
      - author: "Florent Lafarge (INRIA)"
    all_authors: "Muxingzi Li and Florent Lafarge"
    code: ""
    keywords:
      - word: "global registration"
      - word: "energy minimization"
      - word: "geometric primitives"
      - word: "point cloud"
      - word: "polygonal mesh"
    paper: "papers/1216.pdf"
    supp: "supp/1216_supp.zip"
    abstract: "We present a global registration algorithm for multi-modal geometric data, typically 3D point clouds and meshes. Existing feature-based methods and recent deep learning based approaches typically rely upon point-to-point matching strategies that often fail to deliver accurate results from defect-laden data. In contrast, we reason at the scale of planar shapes whose detection from input data offers robustness on a range of defects, from noise to outliers through heterogeneous sampling. The detected planar shapes are projected into an accumulation space from which a rotational alignment is operated. A second step then refines the result with a local continuous optimization which also estimates the scale. We demonstrate the robustness and efficacy of our algorithm on challenging real-world data. In particular, we show that our algorithm competes well against state-of-the-art methods, especially on piece-wise planar objects and scenes."
  - id: 1225
    order: 81
    poster_session: 1
    session_id: 2
    title: "Multi-Exit Vision Transformer for Dynamic Inference"
    authors:
      - author: "Arian Bakhtiarnia (Aarhus University)"
      - author: "Qi Zhang (Aarhus University)"
      - author: "Alexandros Iosifidis (Aarhus University)"
    all_authors: "Arian Bakhtiarnia, Qi Zhang and Alexandros Iosifidis"
    code: "https://gitlab.au.dk/maleci/multiexitvit"
    keywords:
      - word: "early exiting"
      - word: "vision transformer"
      - word: "dynamic inference"
      - word: "transformer models"
      - word: "attention models"
      - word: "efficient inference"
      - word: "early exits"
      - word: "multi-exit architectures"
      - word: "edge computing"
      - word: ""
    paper: "papers/1225.pdf"
    supp: ""
    abstract: "Deep neural networks can be converted to multi-exit architectures by inserting early exit branches after some of their intermediate layers. This allows their inference process to become dynamic, which is useful for time critical IoT applications with stringent latency requirements, but with time-variant communication and computation resources. In particular, in edge computing systems and IoT networks where the exact computation time budget is variable and not known beforehand. Vision Transformer is a recently proposed architecture which has since found many applications across various domains of computer vision. In this work, we propose seven different architectures for early exit branches that can be used for dynamic inference in Vision Transformer backbones. Through extensive experiments involving both classification and regression problems, we show that each one of our proposed architectures could prove useful in the trade-off between accuracy and speed."
  - id: 1231
    order: 411
    poster_session: 4
    session_id: 11
    title: "Adaptive Content Feature Enhancement GAN for Multimodal Selfie to Anime Translation"
    authors:
      - author: "Yuanming Li (Korea University)"
      - author: "Jeong-gi Kwak (Korea University)"
      - author: "Dongsik Yoon (Korea University)"
      - author: "Youngsaeng Jin (Korea University)"
      - author: "David K Han (Drexel University)"
      - author: "Hanseok Ko (Korea University)"
    all_authors: "Yuanming Li, Jeong-gi Kwak, Dongsik Yoon, Youngsaeng Jin, David K Han and Hanseok Ko"
    code: ""
    keywords:
      - word: "GAN"
      - word: "Image-to-Image translation"
      - word: "style transfer"
      - word: ""
    paper: "papers/1231.pdf"
    supp: "supp/1231_supp.zip"
    abstract: "With astonishing successes of GAN-based style transfer techniques, real-world photo translation to animation style images recently has attracted some interest. In particular, the transfer of a selfie to a cartoon has become quite popular as it can serve as a cartoon filter in social media. Unlike most of the Image-to-Image translation tasks, the selfie to anime task requires preserving the contour in the selfie image in the transfer process while it transforms other local characteristics into an animation style. Since the gap between the selfie domain and anime domain is quite large, as it can be imagined in the case of transforming a person into a cartoon animal like a mouse, developing an effective method remains a difficult challenge. In this paper, we propose an Adaptive Content Feature Enhancement Generative Adversarial Networks (ACFE-GAN) for a selfie to anime translation. By our model, the preservation of content features of selfie is improved. In addition to facial and hair contours, the shapes of worn items (eg{} hat and glasses) are also better preserved compared to existing methods. Our method also captures local features more accurately and selectively in translating them into the animation domain. Moreover, compared to the previous photo to anime translation models, we implement it with multimodal translation. Experiments on the selfie2anime dataset demonstrate that our method delivers superior performance in terms of selective preservation of content features.  "
  - id: 1238
    order: 82
    poster_session: 1
    session_id: 2
    title: "SILT: Self-supervised Lighting Transfer Using Implicit Image Decomposition"
    authors:
      - author: "Nikolina Kubiak (University of Surrey)"
      - author: "Armin Mustafa (University of Surrey)"
      - author: "Graeme Phillipson (BBC)"
      - author: "Stephen Jolly (BBC)"
      - author: "Simon Hadfield (University of Surrey)"
    all_authors: "Nikolina Kubiak, Armin Mustafa, Graeme Phillipson, Stephen Jolly and Simon Hadfield"
    code: "https://github.com/n-kubiak/SILT"
    keywords:
      - word: "relighting"
      - word: "lighting transfer"
      - word: "style transfer"
      - word: "image decomposition"
      - word: "self-supervised"
      - word: ""
    paper: "papers/1238.pdf"
    supp: "supp/1238_supp.zip"
    abstract: "We present SILT, a Self-supervised Implicit Lighting Transfer method. Unlike previous research on scene relighting, we do not seek to apply arbitrary new lighting configurations to a given scene. Instead, we wish to transfer the lighting style from a database of other scenes, to provide a uniform lighting style regardless of the input. The solution operates as a two-branch network that first aims to map input images of any arbitrary lighting style to a unified domain, with extra guidance achieved through implicit image decomposition. We then remap this unified input domain using a discriminator that is presented with the generated outputs and the style reference, i.e. images of the desired illumination conditions. Our method is shown to outperform supervised relighting solutions across two different datasets without requiring lighting supervision."
  - id: 1240
    order: 83
    poster_session: 1
    session_id: 2
    title: "Efficient Video Super Resolution by Gated Local Self Attention"
    authors:
      - author: "Davide Abati (Qualcomm AI Research)"
      - author: "Amir Ghodrati (Qualcomm AI Research)"
      - author: "Amirhossein Habibian (Qualcomm AI Research)"
    all_authors: "Davide Abati, Amir Ghodrati and Amirhossein Habibian"
    code: ""
    keywords:
      - word: "video super resolution"
      - word: "video efficiency"
      - word: "super resolution"
      - word: ""
    paper: "papers/1240.pdf"
    supp: "supp/1240_supp.zip"
    abstract: "We tackle the task of efficient video super resolution.
Motivated by our study on the quality vs. efficiency trade-off on a wide range of video super resolution architectures, we focus on the design of an efficient temporal alignment module, as it represents the major computational bottleneck in the current solutions.
Our alignment module, coined GLSA, is based on a self-attention formulation and takes advantage of motion priors existing in the video to achieve a high efficiency.
More specifically, we leverage the locality of motion in adjacent frames to aggregate information from a local neighborhood only.
Moreover, we propose a gating module capable of learning binary functions over pixels, to restrict the alignment only to regions that undergo significant motion.
We experimentally show the effectiveness of our proposed alignment on the commonly-used REDS and Vid4 datasets, reducing the overall computational cost by ~13x and ~2.8x respectively compared to state-of-the-art efficient video super-resolution networks."
  - id: 1244
    order: 84
    poster_session: 1
    session_id: 2
    title: "Microdosing: Knowledge Distillation for GAN Based Compression"
    authors:
      - author: "Leonhard Helminger (ETH Zurich)"
      - author: "Roberto Azevedo (ETH Zurich)"
      - author: "Abdelaziz Djelouah (Disney Research)"
      - author: "Markus Gross (ETH Zurich)"
      - author: "Christopher Schroers (DisneyResearch|Studios)"
    all_authors: "Leonhard Helminger, Roberto Azevedo, Abdelaziz Djelouah, Markus Gross and Christopher Schroers"
    code: ""
    keywords:
      - word: "image compression"
      - word: "video compression"
      - word: "knowledge distillation"
      - word: ""
    paper: "papers/1244.pdf"
    supp: "supp/1244_supp.zip"
    abstract: "Recently, significant progress has been made in learned image and video compression. In particular, the usage of Generative Adversarial Networks has led to impressive results in the low bit rate regime. However, the model size remains an important issue in current state-of-the-art proposals and existing solutions require significant computation effort on the decoding side. This limits their usage in realistic scenarios and the extension to video compression. In this paper, we demonstrate how to leverage knowledge distillation to obtain equally capable image decoders at a fraction of the original number of parameters. We investigate several aspects of our solution including sequence specialization with side information for image coding. Finally, we also show how to transfer the obtained benefits into the setting of video compression. Overall, this allows us to reduce the model size by a factor of 20 and to achieve 50% reduction in decoding time."
  - id: 1245
    order: 412
    poster_session: 4
    session_id: 11
    title: "HAT-Net: A Hierarchical Transformer Graph Neural Network for Grading of Colorectal Cancer Histology Images"
    authors:
      - author: "Yihan Su (Beijing University of Posts and Telecommunications)"
      - author: "Yu Bai (Beijing University of Posts and Telecommunications)"
      - author: "Bo Zhang (Beijing University of Posts and Telecommunications)"
      - author: "Zheng Zhang (Beijing University of Posts and Telecommunications)"
      - author: "Wendong Wang (Beijing University of Posts and Telecommunications)"
    all_authors: "Yihan Su, Yu Bai, Bo Zhang, Zheng Zhang and Wendong Wang"
    code: ""
    keywords:
      - word: "Colorectal cancer grading"
      - word: "cell graph"
      - word: "GNN"
      - word: "Transformer"
      - word: "HAT-Net"
    paper: "papers/1245.pdf"
    supp: ""
    abstract: "Graph-based learning methods have gained more attention in colorectal adenocarcinoma cancer (CRA) grading tasks for encoding the tissue structure information, which patch-wise CNN based methods fail to. Graph-based methods usually involve extracting nuclei features in the histology images as cell-graph node features and modeling the connections between nodes to construct cell-graphs. However, it is infeasible to directly train a classification model to extract nuclei features as we normally do in nature images since different types of nuclei often cluster together. We propose a Masked Nuclei Patch (MNP) approach to train a ResNet-50 as a strong feature encoder to extract more representative nuclei feature for enhancing the overall performance. Graph Neural Networks (GNNs) are often used to train cell-graphs for different tasks. But GNN may struggle to capture the long-range dependency due to its underlying recurrent structure. Therefore, we propose a new network architecture named  Hierarchical Transformer GraphNeural Network, which merits both GNN and Transformer, as a strong competitor for CRA grading tasks. We have achieved the state-of-the-art results on two publicly available CRA grading datasets: the colorectal cancer (CRC) dataset  (98.55%) and the extended colorectal cancer (Extended CRC) dataset (95.33%)."
  - id: 1249
    order: 413
    poster_session: 4
    session_id: 11
    title: "DKMA-ULD: Domain Knowledge augmented Multi-head Attention based Robust Universal Lesion Detection"
    authors:
      - author: "Manu Sheoran (TCS Research)"
      - author: "Meghal  Dani (TCS Research)"
      - author: "Monika Sharma (TCS Research, India)"
      - author: "Lovekesh Vig (Innovation Labs, Tata Consultancy Services Limited)"
    all_authors: "Manu Sheoran, Meghal  Dani, Monika Sharma and Lovekesh Vig"
    code: ""
    keywords:
      - word: "Universal lesion detection"
      - word: "Multi-intensity images"
      - word: "Custom anchors"
      - word: "DeepLesion"
      - word: "Self-attention"
      - word: "CT scans"
      - word: "HU windows"
    paper: "papers/1249.pdf"
    supp: "supp/1249_supp.zip"
    abstract: "Incorporating data-specific domain knowledge in deep networks explicitly can provide important cues beneficial for lesion detection and can mitigate the need for diverse heterogeneous datasets for learning robust detectors. In this paper, we exploit the domain information present in computed tomography (CT) scans and propose a robust universal lesion detection (ULD) network that can detect lesions across all organs of the body by training on a single dataset, DeepLesion. We analyze CT-slices of varying intensities, generated using heuristically determined Hounsfield Unit (HU) windows that individually highlight different organs and are given as inputs to the deep network. The features obtained from the multiple intensity images are fused using a novel convolution augmented multi-head self-attention module and subsequently, passed to a Region Proposal Network (RPN) for lesion detection. In addition, we observed that traditional anchor boxes used in RPN for natural images are not suitable for lesion sizes often found in medical images. Therefore, we propose to use lesion-specific anchor sizes and ratios in the RPN for improving the detection performance. We use self-supervision to initialize weights of our network on the DeepLesion dataset to further imbibe domain knowledge. Our proposed Domain Knowledge augmented Multi-head Attention based Universal Lesion Detection Network DMKA-ULD produces refined and precise bounding boxes around lesions across different organs. We evaluate the efficacy of our network on the publicly available DeepLesion dataset which comprises of approximately 32K CT scans with annotated lesions across all organs of the body. Results demonstrate that we outperform existing state-of-the-art methods achieving an overall sensitivity of 87.16%."
  - id: 1254
    order: 305
    poster_session: 3
    session_id: 8
    title: "A Foundation for 3D Human Behavior Detection in Privacy-Sensitive Domains"
    authors:
      - author: "Thomas TH Heitzinger (Technische Universität Wien)"
      - author: "Martin Kampel (Vienna University of Technology, Computer Vision Lab)"
    all_authors: "Thomas TH Heitzinger and Martin Kampel"
    code: ""
    keywords:
      - word: "3d human behavior analysis"
      - word: "privacy preserving machine learning"
      - word: "multimodal data"
      - word: "depth imaging"
      - word: "thermal imaging"
      - word: "3d object detection"
      - word: "tracking"
    paper: "papers/1254.pdf"
    supp: "supp/1254_supp.zip"
    abstract: "Human behavior analysis applications in the fields of ambient assisted living (AAL) and human security monitoring require continuous video analysis of individuals. Although intelligent systems deployed in these areas are intended to have a positive impact on the persons involved, subsequent continuous monitoring naturally raises ethical concerns and questions about privacy implications. To address these issues, we present a foundation for identity-preserving 3D human behavior analysis. Our main contributions are a fast 3D detection system and a public multimodal dataset. The introduced detection system uses an innovative target assignment scheme to significantly improve performance, especially in challenging scenes with a large number of person-person and person-object occlusions. On our dataset, the system shows superior performance compared to the state-of-the-art in 3D object detection while being lightweight enough for configuration and deployment to edge devices. The dataset is large, at a total of ~85k annotated frames, and is based solely on anonymizing sensor technologies with spatio-temporally aligned depth and thermal sequences. Annotation is provided as 3D bounding boxes, along with pose labels and consistent person IDs for use in tracking. The dataset is designed to be flexible. Data representation in either image view or point clouds and the option for projected 2D bounding boxes, allows use in a variety of 2D or 3D tasks. Target applications of our work are privacy-sensitive domains that require continuous supervision, including ambient assisted living tasks (e.g., motion rehabilitation, fall detection, vital sign detection) and human security monitoring applications, such as construction safety, critical care and correctional facility monitoring."
  - id: 1258
    order: 334
    oral_session: 8
    poster_session: 4
    session_id: 10
    title: "DISCO: accurate Discrete Scale Convolutions"
    authors:
      - author: "Ivan Sosnovik (University of Amsterdam)"
      - author: "Artem Moskalev (University of Amsterdam)"
      - author: "Arnold W.M. Smeulders (University of Amsterdam)"
    all_authors: "Ivan Sosnovik, Artem Moskalev and Arnold W.M. Smeulders"
    code: ""
    keywords:
      - word: "equivariance"
      - word: "symmetry"
      - word: "invariance"
      - word: "scale"
      - word: "convolutions"
      - word: "dilation"
      - word: "tracking"
      - word: "image classification"
    paper: "papers/1258.pdf"
    supp: "supp/1258_supp.zip"
    abstract: "Scale is often seen as a given, disturbing factor in many vision tasks. When doing so it is one of the factors why we need more data during learning. In recent work scale equivariance was added to convolutional neural networks. It was shown to be effective for a range of tasks. We aim for accurate scale-equivariant convolutional neural networks (SE-CNNs) applicable for problems where high granularity of scale and small kernel sizes are required. Current SE-CNNs rely on weight sharing and kernel rescaling, the latter of which is accurate for integer scales only. To reach accurate scale equivariance, we derive general constraints under which scale-convolution remains equivariant to discrete rescaling. We find the exact solution for all cases where it exists, and compute the approximation for the rest. The discrete scale-convolution pays off, as demonstrated in a new state-of-the-art classification on MNIST-scale and on STL-10 in the supervised learning setting. With the same SE scheme, we also improve the computational effort of a scale-equivariant Siamese tracker on OTB-13."
  - id: 1267
    order: 85
    poster_session: 1
    session_id: 2
    title: "Reference Guided Image Inpainting using Facial Attributes"
    authors:
      - author: "Dongsik Yoon (Korea University)"
      - author: "Youngsaeng Jin (Korea University)"
      - author: "Jeong-gi Kwak (Korea University)"
      - author: "Yuanming Li (Korea University)"
      - author: "David K Han (Drexel University)"
      - author: "Hanseok Ko (Korea University)"
    all_authors: "Dongsik Yoon, Youngsaeng Jin, Jeong-gi Kwak, Yuanming Li, David K Han and Hanseok Ko"
    code: ""
    keywords:
      - word: "image inpainting"
      - word: "GAN"
      - word: "image completion"
      - word: "image manipulation"
      - word: ""
    paper: "papers/1267.pdf"
    supp: "supp/1267_supp.zip"
    abstract: "Image inpainting is a technique of completing missing pixels such as occluded region restoration, distracting objects removal, and facial completion. Among these inpainting tasks, facial completion algorithm performs face inpainting according to the user direction. Existing approaches require delicate and well controlled input by the user, thus it is difficult for an average user to provide the guidance sufficiently accurate for the algorithm to generate desired results. To overcome this limitation, we propose an alternative user-guided inpainting architecture that manipulates facial attributes using a single reference image as the guide. Our end-to-end model consists of attribute extractors for accurate reference image attribute transfer and an inpainting model to map the attributes realistically and accurately to generated images. We customize MS-SSIM loss and learnable bidirectional attention maps in which importance structures remain intact even with irregular shaped masks.  Based on our evaluation using the publicly available dataset CelebA-HQ, we demonstrate that the proposed method delivers superior performance compared to some state-of-the-art methods specialized in inpainting tasks."
  - id: 1269
    order: 86
    poster_session: 1
    session_id: 2
    title: "Conditional De-Identification of 3D Magnetic Resonance Images"
    authors:
      - author: "Lennart Alexander Van der Goten (KTH Royal Institute of Technology)"
      - author: "Tobias Hepp (Max Planck Institute for Intelligent Systems)"
      - author: "Zeynep   Akata (University of Tübingen)"
      - author: "Kevin Smith (KTH Royal Institute of Technology)"
    all_authors: "Lennart Alexander Van der Goten, Tobias Hepp, Zeynep   Akata and Kevin Smith"
    code: ""
    keywords:
      - word: "generative adversarial network"
      - word: "medical"
      - word: "privacy"
    paper: "papers/1269.pdf"
    supp: "supp/1269_supp.zip"
    abstract: "Privacy protection of magnetic resonance imagery (MRI) is challenging. Even when the metadata is removed, brain scans are vulnerable to attacks that match renderings of the face to facial image databases. As sharing of data is essential to scientific and medical progress, solutions to de-identify MRI scans that obfuscate or remove parts of the face have been developed. However, we show that these existing solutions either fail to reliably hide the patient's identity or are so aggressive that they impair further analyses. We go on to identify a new class of de-identification techniques that, instead of removing facial features, remodels them. Our solution to this task relies on a conditional multi-scale 3D GAN architecture. It takes a patient's MRI scan as input and generates a 3D volume conditioned on the patient's brain, which is preserved exactly, but where the face has been de-identified through remodeling. We demonstrate that our approach preserves privacy far better than existing techniques, without compromising downstream medical analyses on the brain."
  - id: 1273
    order: 306
    poster_session: 3
    session_id: 8
    title: "Student-Teacher Feature Pyramid Matching for Anomaly Detection"
    authors:
      - author: "Guodong Wang (Beihang University)"
      - author: "Shumin Han (Baidu Inc.)"
      - author: "Errui Ding (Baidu Inc.)"
      - author: "Di Huang (Beihang University, China)"
    all_authors: "Guodong Wang, Shumin Han, Errui Ding and Di Huang"
    code: "https://github.com/gdwang08/STFPM"
    keywords:
      - word: "anomaly detection"
      - word: "knowledge distillation"
      - word: "student-teacher framework"
      - word: "feature pyramid matching"
      - word: ""
    paper: "papers/1273.pdf"
    supp: "supp/1273_supp.zip"
    abstract: "   Anomaly detection is a challenging task and usually formulated as an one-class learning problem for the unexpectedness of anomalies. This paper proposes a simple yet powerful approach to this issue, which is implemented in the student-teacher framework for its advantages but substantially extends it in terms of both accuracy and efficiency. Given a strong model pre-trained on image classification as the teacher, we distill the knowledge into a single student network with the identical architecture to learn the distribution of anomaly-free images and this one-step transfer preserves the crucial clues as much as possible. Moreover, we integrate the multi-scale feature matching strategy into the framework, and this hierarchical feature matching enables the student network to receive a mixture of multi-level knowledge from the feature pyramid under better supervision, thus allowing to detect anomalies of various sizes. The difference between feature pyramids generated by the two networks serves as a scoring function indicating the probability of anomaly occurring. Due to such operations, our approach achieves accurate and fast pixel-level anomaly detection. Very competitive results are delivered on the MVTec anomaly detection dataset, superior to the state of the art ones."
  - id: 1275
    order: 307
    poster_session: 3
    session_id: 8
    title: "Image Completion with Adaptive Multi-Temperature Mask-Guided Attention"
    authors:
      - author: "Xiang Zhou (Southern University of Science and Technology)"
      - author: "Yuan Zeng (Southern University of Science and Technology)"
      - author: "Yi Gong (Southern University of Science and Technology)"
    all_authors: "Xiang Zhou, Yuan Zeng and Yi Gong"
    code: ""
    keywords:
      - word: "image inpainting"
      - word: "attention mechanism"
      - word: ""
    paper: "papers/1275.pdf"
    supp: "supp/1275_supp.zip"
    abstract: "Leveraging distant contextual information and self-similarity of natural images in deep learning-based models is important for high-quality image completion with large missing regions. Most of the deep generative adversarial network (GAN)-based image completion methods attempt this via increasing receptive field size of convolutions and integrating an attention module. However, existing attention mechanisms treat the softness of the attention for different types of features with the same scale, which may be inferior since the same softness of the attention may lead attention made on limited spatial locations in feature space. To address this limitation, we design a new two-stage image completion model and propose an attention mechanism called Adaptive multi-Temperature Mask-guided Attention (ATMA). The ATMA performs non-local processing and controls the softness of attention by means of multiple self-adaptive temperatures. The proposed model infers a coarse inpainting result via a gated convolution neural network in the first stage and refines appearance consistency between generated regions and known regions via ATMA in the second stage. Experiments demonstrate superior performance compared to state-of-the-art methods on benchmark datasets including CelebA-HQ and Paris StreetView."
  - id: 1276
    order: 308
    poster_session: 3
    session_id: 8
    title: "TAPL: Dynamic Part-based Visual Tracking via Attention-guided Part Localization"
    authors:
      - author: "Wei Han (Nanyang Technological University)"
      - author: "Hantao Huang (MediaTek)"
      - author: "Xiaoxi Yu (Mediatek)"
    all_authors: "Wei Han, Hantao Huang and Xiaoxi Yu"
    code: ""
    keywords:
      - word: "visual tracking"
      - word: "transformer"
      - word: "part based"
      - word: "attention"
    paper: "papers/1276.pdf"
    supp: "supp/1276_supp.zip"
    abstract: "Holistic object representation-based trackers suffer from performance drop under large appearance change such as deformation and occlusion. In this work, we propose a dynamic part-based tracker and constantly update the target part representation to adapt to object appearance change. Moreover, we design an attention-guided part localization network to directly predict the target part locations, and determine the final bounding box with the distribution of target parts. Our proposed tracker achieves promising results on various benchmarks: VOT2018, OTB100 and GOT-10k. "
  - id: 1277
    order: 414
    poster_session: 4
    session_id: 11
    title: "ASM-Net: Category-level Pose and Shape Estimation Using Parametric Deformation"
    authors:
      - author: "Shuichi Akizuki (Chukyo University)"
      - author: "Manabu Hashimoto (Graduate School of Chukyo University)"
    all_authors: "Shuichi Akizuki and Manabu Hashimoto"
    code: "https://github.com/sakizuki/asm-net"
    keywords:
      - word: "6DoF pose estimation"
      - word: "category-level pose estimation"
      - word: "deformable shape model"
      - word: "3D object recognition"
      - word: "point cloud"
    paper: "papers/1277.pdf"
    supp: ""
    abstract: "We propose a novel deep neural network that estimates the six degrees of freedom pose and complete shape of unseen objects from point cloud data. Our concept is to train the network that can perform well on real images captured by a consumer RGBD camera using only 3D models of the target category. To do so, we have employed two ideas. The first is modeling intra-category shape variations with active shape models that can deform the shape with a few dimensional parameters. The second is applying effective filtering processes to the training data to convert the 3D object model into a point cloud that simulates the sensor measurements. We evaluated our method on NOCS REAL275, a widely used benchmark dataset for category-level pose estimation, and confirmed its superiority over conventional methods in terms of both shape recovery and pose estimation.
Our code is available at https://github.com/sakizuki/asm-net."
  - id: 1278
    order: 415
    poster_session: 4
    session_id: 11
    title: "SDNet: Unconstrained Object Structure Detector Network for In-Field Real-Time Crop Part Location And Phenotyping"
    authors:
      - author: "Louis Lac (University of Bordeaux)"
      - author: "Jean-Pierre Da Costa (University of Bordeaux)"
      - author: "Marc Donias (University of Bordeaux)"
      - author: "Barna Keresztes (IMS Bordeaux)"
      - author: "Marine Louargant (CTIFL)"
    all_authors: "Louis Lac, Jean-Pierre Da Costa, Marc Donias, Barna Keresztes and Marine Louargant"
    code: "https://github.com/laclouis5/StructureDetector"
    keywords:
      - word: "structure detection"
      - word: "pose estimation"
      - word: "crop detection"
      - word: "plant detection"
      - word: "keypoint location"
      - word: ""
    paper: "papers/1278.pdf"
    supp: ""
    abstract: "Most modern multi-instance pose estimation neural networks --either bottom-up or top-down variants-- are built around a highly specialized and constrained architecture. They rely on the detection of a fixed set of keypoints specific to the object in order to regress the pose in images. While efficient, those architectures are not very flexible and cannot be applied to objects with a less stable structure such as plants. In this paper we propose a neural network called SDNet which is suitable for the real-time detection of object poses with an unconstrained number of keypoints. To demonstrate this capability as well as its potential application for precision agriculture we evaluate it on a custom crop structure dataset and we compare its performance to the state-of-the-art neural network for real-time object detection Tiny YOLO v4 on two tasks where both of them can compete: (i) multi-instance crop detection and leaf counting --which can be applied to in-field phenotyping-- and (ii) stem and leaf keypoints detection and location --which can be used for real-time precision hoeing. We show that SDNet achieves excellent performance while still providing additional information via its unique structure detection ability."
  - id: 1282
    order: 201
    poster_session: 2
    session_id: 5
    title: "Are conditional GANs explicitly conditional?"
    authors:
      - author: "Houssem eddine BOULAHBAL ( Renault Software Labs, I3S, University of Cote d'Azure)"
      - author: "Adrian Voicila (Renault SW Labs)"
      - author: "Andrew I Comport (CNRS-I3S/Université Cote d'Azur)"
    all_authors: "Houssem eddine BOULAHBAL, Adrian Voicila and Andrew I Comport"
    code: "https://github.com/Houssem-25/AcontrarioGAN"
    keywords:
      - word: "data augmentation"
      - word: "conditional GAN"
      - word: "generative adversarial network"
      - word: "semantic segmentation"
      - word: "image synthesis"
      - word: "depth estimation"
      - word: ""
    paper: "papers/1282.pdf"
    supp: "supp/1282_supp.zip"
    abstract: "This paper proposes two important contributions for conditional Generative Adversarial Networks (cGANs) to improve the wide variety of applications that exploit this architecture. The first main contribution is an analysis of cGANs to show that they are not explicitly conditional. In particular, it will be shown that the discriminator and subsequently the cGAN does not automatically learn the conditionality between inputs. The second contribution is a new method, called a contrario cGAN, that explicitly models conditionality for both parts of the adversarial architecture via a novel a contrario loss that involves training the discriminator to learn unconditional (adverse) examples. This leads to a novel type of data augmentation approach for GANs which allows to restrict the search space of the generator to conditional outputs using adverse examples. Extensive experimentation is carried out to evaluate the conditionality of the discriminator by proposing a probability distribution analysis. Comparisons with the cGAN architecture for different applications show significant improvements in performance on well known datasets including, semantic image synthesis, image segmentation, monocular depth prediction and \"single label\"-to-image using different metrics including Fréchet Inception Distance (FID), mean Intersection over Union (mIoU), Root Mean Square Error log (RMSE log) and Number of statistically-Different Bins (NDB)."
  - id: 1283
    order: 87
    poster_session: 1
    session_id: 2
    title: "M-CAM: Visual Explanation of Challenging Conditioned Dataset with Bias-reducing Memory"
    authors:
      - author: "Seongyeop Kim (KAIST)"
      - author: "Yong Man Ro (KAIST)"
    all_authors: "Seongyeop Kim and Yong Man Ro"
    code: ""
    keywords:
      - word: "visual explanation"
      - word: "memory network"
      - word: "explainable ai"
      - word: "medical image"
      - word: "class activation map"
      - word: ""
    paper: "papers/1283.pdf"
    supp: "supp/1283_supp.zip"
    abstract: "We introduce a framework that enhances visual explanation of class activation map (CAM) with key-value memory structure for deep networks. We reveal challenging conditions inherently existing in several datasets that degrade the visual explanation quality of existing CAM-based visual explanation methods (e.g. imbalanced data, multi-object co-occurrence) and try to solve it with the proposed framework. The proposed Bias-reducing memory module learns spatial feature representation of different classes from trained networks and stores each different semantic information in separate memory slots, while it does not require any modification to the existing networks. Furthermore, we propose a novel visual explanation method accompanied by a memory slot searching algorithm to retrieve semantically relevant spatial feature representation from the memory module and make visual explanation of network decisions. We evaluate our visual explanation framework with datasets of challenging conditions including several medical image datasets and multi-label classification datasets. We qualitatively and quantitatively compare it with existing CAM-based methods to demonstrate the strength of our framework."
  - id: 1291
    order: 202
    poster_session: 2
    session_id: 5
    title: "Bias Field Robustness Verification of Large Neural Image Classifiers"
    authors:
      - author: "Patrick Henriksen (Imperial College London)"
      - author: "Kerstin Hammernik (Imperial College London)"
      - author: "Daniel Rueckert (Imperial College London)"
      - author: "Alessio Lomuscio (Imperial College London)"
    all_authors: "Patrick Henriksen, Kerstin Hammernik, Daniel Rueckert and Alessio Lomuscio"
    code: ""
    keywords:
      - word: "Formal Verification"
      - word: "Neural Networks"
      - word: "Bias Fields"
      - word: "MRI"
      - word: "Robustness"
      - word: "Adversarial Examples"
      - word: ""
    paper: "papers/1291.pdf"
    supp: "supp/1291_supp.zip"
    abstract: "We present a method for verifying the robustness of neural network-based image classifiers against a large class of intensity perturbations that frequently occur in computer vision. These perturbations, or intensity inhomogeneities, can be modelled by a spatially varying, multiplicative transformation of the intensities by a bias field. We illustrate an encoding of bias field transformations into neural network operations to exploit neural network formal verification toolkits. We extend the toolkit VeriNet with the above encoding, GPU support, input-domain splitting and a symbolic interval propagation pre-processing step. Finally, we show that the resulting implementation, VeriNetBF, can analyse models with up to 11M tuneable parameters and 6.5M ReLU nodes trained on the CIFAR-10 ImageNet and NYU fastMRI datasets."
  - id: 1293
    order: 203
    poster_session: 2
    session_id: 5
    title: "Simple Dialogue System with AUDITED"
    authors:
      - author: "Yusuf Tas (ANU)"
      - author: "Piotr Koniusz (ANU College of Engineering and Computer Science)"
    all_authors: "Yusuf Tas and Piotr Koniusz"
    code: ""
    keywords:
      - word: "multimodal"
      - word: "dialogue"
      - word: "AUDITED"
      - word: "MMD"
      - word: "DeepFashion"
      - word: "M-HRED"
      - word: "SIMMC"
      - word: "SVD"
      - word: "unsupervised"
      - word: "FAISS"
    paper: "papers/1293.pdf"
    supp: "supp/1293_supp.zip"
    abstract: "We devise a multimodal conversation system for dialogue utterances composed of text, image or both modalities. We leverage Auxiliary UnsuperviseD vIsual and TExtual Data (AUDITED). To improve the performance of text-based task, we utilize translations of target sentences from English to French to form the assisted supervision. For the image-based task, we employ the DeepFashion  dataset in which we seek nearest neighbor images of positive and negative target images of the MMD data. These nearest neighbors form the nearest neighbor embedding providing an external context for target images. We form two methods to create neighbor embedding vectors, namely Neighbor Embedding by Hard Assignment (NEHA) and Neighbor Embedding by Soft Assignment (NESA) which generate context subspaces per target image. Subsequently, these subspaces are learnt by our pipeline as a context for the target data. We also propose a discriminator which switches between the image- and text-based tasks. We show improvements over baselines on the large-scale Multimodal Dialogue Dataset (MMD) and SIMMC."
  - id: 1295
    order: 309
    poster_session: 3
    session_id: 8
    title: "SVD-GAN for Real-Time Unsupervised Video Anomaly Detection"
    authors:
      - author: "Dinesh Jackson Samuel (Oxford Brookes University)"
      - author: "Fabio Cuzzolin (Oxford Brookes University)"
    all_authors: "Dinesh Jackson Samuel and Fabio Cuzzolin"
    code: "https://github.com/jackson0988/SVD-GAN-for-Real-Time-Unsupervised-Video-Anomaly-Detection"
    keywords:
      - word: "Unsupervised anomaly detection"
      - word: "SVD-GAN"
      - word: "depth-wise separable convolutions"
      - word: "spatiotemporal features"
      - word: "GAN convergence"
      - word: "Singular Value Decomposition loss"
      - word: "GAN reconstruction"
      - word: "lightweight GAN model"
      - word: "minimized KL divergence"
    paper: "papers/1295.pdf"
    supp: "supp/1295_supp.zip"
    abstract: "Real-time unsupervised anomaly detection from videos is challenging due to the uncertainty in occurrence and definition of abnormal events. To overcome this ambiguity, an unsupervised adversarial learning model is proposed to detect such unusual events. The proposed end-to-end system is based on a Generative Adversarial Network (GAN) architecture with spatiotemporal feature learning and a new Singular Value Decomposition (SVD) loss function for robust reconstruction and video anomaly detection. The loss employs efficient low-rank approximations of the matrices involved to drive the convergence of the model. During training, the model strives to learn the relevant normal data distribution. Anomalies are then detected as frames whose reconstruction error, based on such distribution, shows a significant deviation. The model is efficient and lightweight due to our adoption of depth-wise separable convolution. The complete system is validated upon several benchmark datasets and proven to be robust for complex video anomaly detection, in terms of both AUC and Equal Error Rate (EER)."
  - id: 1299
    order: 416
    poster_session: 4
    session_id: 11
    title: "X Resolution Correspondence Networks"
    authors:
      - author: "Georgi Tinchev (University of Oxford)"
      - author: "Shuda Li (XYZ Reality)"
      - author: "Kai Han (Google)"
      - author: "David Mitchell (XYZ Reality)"
      - author: "Rigas Kouskouridas (XYZ Reality)"
    all_authors: "Georgi Tinchev, Shuda Li, Kai Han, David Mitchell and Rigas Kouskouridas"
    code: "https://github.com/XYZ-R-D/xrcnet"
    keywords:
      - word: "Correspondence Estimation"
      - word: "Structure from Motion"
      - word: "Visual Localization"
      - word: "Place Recognition"
    paper: "papers/1299.pdf"
    supp: "supp/1299_supp.zip"
    abstract: "In this paper, we aim at establishing accurate dense correspondences between a pair of images with overlapping field of view under challenging illumination variation, viewpoint changes, and style differences. Through an extensive ablation study of the state-of-the-art correspondence networks, we surprisingly discovered that the widely adopted 4D correlation tensor and its related learning and processing modules could be de-parameterised and removed from training with merely a minor impact over the final matching accuracy. Disabling these computational expensive modules dramatically speeds up the training procedure and allows to use 4 times bigger batch size, which in turn compensates for the accuracy drop. Together with a multi-GPU inference stage, our method facilitates the systematic investigation of the relationship between matching accuracy and up-sampling resolution of the native testing images from 1280 to 4K. This leads to discovery of the existence of an optimal resolution X that produces accurate matching performance surpassing the state-of-the-art methods particularly over the lower error band on public benchmarks for the proposed network."
  - id: 1305
    order: 88
    poster_session: 1
    session_id: 2
    title: "A Design of Contractive Appearance Flow for Photometric Stereo"
    authors:
      - author: "Lixiong Chen (Oxford University)"
      - author: "Victor Adrian Prisacariu (University of Oxford)"
    all_authors: "Lixiong Chen and Victor Adrian Prisacariu"
    code: ""
    keywords:
      - word: "photometric stereo"
      - word: "reflectance analysis"
      - word: ""
    paper: "papers/1305.pdf"
    supp: "supp/1305_supp.pdf"
    abstract: "We introduce the concept of contractive appearance flow to address photometric stereo with general reflectance. Our solution is motivated by the fact that the shape intrinsics of an object are encoded by its Lambertian reflectance, based on which we design a neural network that maps a set of per-pixel general appearances to their Lambertian counterparts as if this process is carried by a flow in a field of vectors of pixel values. Our design has two features: (1) by introducing a transfer operator in the encoded latent space, we replace the typical workflow of a Variational AutoEncoder (VAE) with a more generic encode-transfer-decode procedure. For photometric stereo, we apply this procedure to produce consistent representations of the incident light fields and to eliminate the signal variation caused by material properties; (2) during training each sample of general reflectance is associated with its Lambertian-related template samples, and by minimizing the distance between these two types of signals in the latent space, we enforce the flow to contract in the subspace spanned by Lambertian appearances only. The proposed method learns reflectance measurements directly and does not need to parameterize material properties. Our design is simple, lightweight, and automatic, yet, experiments show that it is effective and yields accurate estimations."
  - id: 1320
    order: 204
    poster_session: 2
    session_id: 5
    title: "FFNB: Forgetting-Free Neural Blocks for Deep Continual Learning"
    authors:
      - author: "Hichem Sahbi (Sorbonne University)"
      - author: "Haoming Zhan (Sorbonne University, CNRS, LIP6)"
    all_authors: "Hichem Sahbi and Haoming Zhan"
    code: ""
    keywords:
      - word: "Continual and incremental learning"
      - word: "lifelong learning"
      - word: "catastrophic interference"
      - word: "catastrophic forgetting"
      - word: "dynamic neural networks"
      - word: "visual recognition"
    paper: "papers/1320.pdf"
    supp: "supp/1320_supp.zip"
    abstract: "Deep neural networks (DNNs) have recently achieved a great success in computer vision and several related fields. Despite such progress, current neural architectures still suffer from catastrophic interference (a.k.a. forgetting)  which obstructs DNNs to learn continually.  While several state-of-the-art methods have been proposed to mitigate forgetting, these existing solutions are either highly rigid (as regularization) or time/memory demanding (as replay). An intermediate class of methods, based on dynamic networks, has been proposed in the literature and provides a reasonable balance between task memorization and computational footprint.  In this paper, we devise a dynamic network architecture for continual learning based on a novel forgetting-free neural block (FFNB). Training FFNB features on new tasks is achieved using a novel procedure that constrains the underlying parameters in the null-space of the previous tasks, while training classifier parameters equates to Fisher discriminant analysis. The latter provides an effective incremental process which is also optimal from a Bayesian perspective. The trained features and classifiers are further enhanced using an incremental \"end-to-end\" fine-tuning. Extensive experiments, conducted on different challenging classification problems, show the high effectiveness of the proposed method."
  - id: 1321
    order: 417
    poster_session: 4
    session_id: 11
    title: "Stabilized Semi-Supervised Training for  COVID Lesion Segmentation"
    authors:
      - author: "Pranjal Sahu (Stony Brook University)"
      - author: "saikiran kumar vunnava (IIT BHU)"
      - author: "HONG QIN (Stony Brook University)"
    all_authors: "Pranjal Sahu, Saikiran Kumar Vunnava and HONG QIN"
    code: ""
    keywords:
      - word: "covid"
      - word: "ct"
      - word: "segmentation"
      - word: "semi-supervised"
      - word: "noisy training"
      - word: "lesion segmentation"
      - word: "label propagation"
      - word: ""
    paper: "papers/1321.pdf"
    supp: "supp/1321_supp.zip"
    abstract: "We propose a novel stabilized semi-supervised training method to solve the chal-lenging problem of covid lesion segmentation in CT scans.  We first study the limita-tions of current models and based on our findings we introduce a lightweight SU-Net(Small U-Net) architecture.   During training we feed the CT scans in sorted order oflesion occupancy and calculate a reliability score at each epoch to determine the stop-ping criteria.  We test the proposed method on the largest publicly available COVID CTdataset  called  MOSMED  dataset.   By  harnessing  around  800  un-labelled  COVID  CTvolumes comprising 25k CT slices, we improve the segmentation accuracy by around2-4 dice percentage points depending upon the availability of labelled training data. We also compare our method with a recently published COVID lesion segmentation methodcalled Semi-InfNet. The proposed method outperforms Semi-InfNet model and achievesstate-of-the-art covid segmentation result on MOSMED dataset."
  - id: 1327
    order: 332
    oral_session: 8
    poster_session: 4
    session_id: 10
    title: "Pseudo-Labeling for Class Incremental Learning"
    authors:
      - author: "Alexis Lechat (ONERA)"
      - author: "stephane herbin (onera)"
      - author: "Frederic Jurie ()"
    all_authors: "Alexis Lechat, Stephane Herbin and Frederic Jurie"
    code: "https://github.com/alechat/PLCiL"
    keywords:
      - word: "incremental learning"
      - word: "catastrophic forgetting"
      - word: "semi-supervised learning"
      - word: "pseudo-labeling"
      - word: "consistency regularization"
    paper: "papers/1327.pdf"
    supp: "supp/1327_supp.zip"
    abstract: "Class Incremental Learning (CIL) consists in training a model iteratively with limited amount of data from few classes that will never be seen again, resulting in catastrophic forgetting and lack of diversity. In this paper, we address these phenomena by assuming that, during incremental learning, additional unlabeled data are continually available, and propose a Pseudo-Labeling approach for class incremental learning (PLCiL) that makes use of a new adapted loss. We demonstrate that our method achieves better performance than supervised or other semi-supervised methods on standard class incremental benchmarks (CIFAR-100 and ImageNet-100) even when a self-supervised pre-training step using a large set of data is used as initialization. We also illustrate the advantages of our method in a more complex context with fewer labels."
  - id: 1334
    order: 418
    poster_session: 4
    session_id: 11
    title: "Towards Holistic Real-time Human 3D Pose Estimation using MocapNETs"
    authors:
      - author: "Ammar Qammaz (CSD-UOC and ICS-FORTH)"
      - author: "Antonis A Argyros (CSD-UOC and ICS-FORTH)"
    all_authors: "Ammar Qammaz and Antonis A Argyros"
    code: "https://github.com/FORTH-ModelBasedTracker/MocapNET"
    keywords:
      - word: "3d hand pose estimation"
      - word: "3d body pose estimation"
      - word: "3d pose estimation"
      - word: "human perception"
      - word: "ensemble"
      - word: "BVH"
      - word: "hierarchical coordinate descent"
      - word: "eNSRM"
      - word: "RGB"
      - word: "mocap"
      - word: ""
    paper: "papers/1334.pdf"
    supp: "supp/1334_supp.zip"
    abstract: "In this work, we extend a method originally devised for 3D body pose estimation to tackle the 3D hand pose estimation task. Due to its compositionality and compact Bio Vision Hierarchy (BVH) output, the resulting method can be combined with the original body 3D pose estimation method. This is achieved based on a novel neural network architecture combining key design characteristics of DenseNets, ResNets and MocapNETs trainable to accommodate both bodies and hands. The resulting method is assessed quantitatively in well-established hand and body pose estimation datasets. The obtained results show that the proposed enhancements result in competitive performance for hands, as well as on accuracy and performance benefits for the original body estimation task. Moreover, we show qualitatively  that due to its real-time performance and easy deployment using off-the-shelf webcam equipped PCs, the proposed solution can become a valuable perceptual building block supporting a variety of applications."
  - id: 1339
    order: 310
    poster_session: 3
    session_id: 8
    title: "Localizing Objects with Self-supervised Transformers and no Labels"
    authors:
      - author: "Oriane Siméoni (valeo.ai)"
      - author: "Gilles Puy (valeo.ai)"
      - author: "Huy V. Vo (Ecole Normale Supérieure - INRIA - Valeo.ai)"
      - author: "Simon  W Roburin (valeo.ai / imagine ENPC)"
      - author: "Spyros Gidaris (valeo.ai)"
      - author: "Andrei Bursuc (valeo.ai)"
      - author: "Patrick Pérez (Valeo.ai)"
      - author: "Renaud Marlet (Ecole des Ponts ParisTech)"
      - author: "Jean Ponce (Inria)"
    all_authors: "Oriane Siméoni, Gilles Puy, Huy V. Vo, Simon  W Roburin, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Renaud Marlet and Jean Ponce"
    code: "https://github.com/valeoai/LOST"
    keywords:
      - word: "unsupervised"
      - word: "object discovery"
      - word: "object detection"
      - word: "transformers"
      - word: "self-supervised"
      - word: "object localization"
      - word: ""
    paper: "papers/1339.pdf"
    supp: "supp/1339_supp.zip"
    abstract: "Localizing objects in image collections  without supervision can help to avoid expensive annotation campaigns. We propose a simple approach to this problem, that leverages the activation features of a vision transformer pre-trained in a self-supervised manner. Our method, LOST, does not require any external object proposal nor any exploration of the image collection; it operates on a single image. Yet, we outperform state-of-the-art object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. We also show that training a class-agnostic detector on the discovered objects boosts results by another 7 points. Moreover, we show promising results on the unsupervised object discovery task."
  - id: 1340
    order: 419
    poster_session: 4
    session_id: 11
    title: "Probabilistic Estimation of 3D Human Shape and Pose with a Semantic Local Parametric Model"
    authors:
      - author: "Akash Sengupta (University of Cambridge)"
      - author: "Ignas Budvytis (Department of Engineering, University of Cambridge)"
      - author: "Roberto Cipolla (University of Cambridge)"
    all_authors: "Akash Sengupta, Ignas Budvytis and Roberto Cipolla"
    code: ""
    keywords:
      - word: "human shape"
      - word: "probabilistic shape"
      - word: ""
    paper: "papers/1340.pdf"
    supp: "supp/1340_supp.zip"
    abstract: "This paper addresses the problem of 3D human body shape and pose estimation from RGB images. Some recent approaches to this task predict probability distributions over human body model parameters conditioned on the input images. This is motivated by the ill-posed nature of the problem wherein multiple 3D reconstructions may match the image evidence, particularly when some parts of the body are locally occluded. However, identity-dependent shape parameters in widely-used body models (e.g. SMPL) control global deformations over the whole body surface. Distributions over these global shape parameters are unable to meaningfully capture uncertainty in shape estimates associated with locally-occluded body parts. In contrast, we present a method that (i) predicts distributions over local body shape in the form of semantic body measurements and (ii) uses a linear mapping to transform a local distribution over body measurements to a global distribution over SMPL shape parameters. We show that our method outperforms the current state-of-the-art in terms of identity-dependent body shape estimation accuracy on the SSP-3D dataset, and a private dataset of tape-measured humans, by probabilistically-combining local body measurement distributions predicted from multiple images of a subject."
  - id: 1342
    order: 89
    poster_session: 1
    session_id: 2
    title: "One Model to Reconstruct Them All: A Novel Way to Use the Stochastic Noise in StyleGAN"
    authors:
      - author: "Christian Bartz (Hasso Plattner Institute)"
      - author: "Joseph Bethge (Hasso Plattner Institute)"
      - author: "Haojin Yang (Hasso-Plattner-Institut für Digital Engineering gGmbH)"
      - author: "Meinel Christoph (Hasso Plattner Institut, Potsdam Germany)"
    all_authors: "Christian Bartz, Joseph Bethge, Haojin Yang and Meinel Christoph"
    code: "https://github.com/Bartzi/one-model-to-reconstruct-them-all"
    keywords:
      - word: "stylegan"
      - word: "generative adversarial networks"
      - word: "stochastic noise"
      - word: "encoder"
      - word: "reconstruction"
      - word: "denoising"
    paper: "papers/1342.pdf"
    supp: "supp/1342_supp.zip"
    abstract: "Generative Adversarial Networks (GANs) have achieved state-of-the-art performance for several image generation and manipulation tasks. Different works have improved the limited understanding of the latent space of GANs by embedding images into specific GAN architectures to reconstruct the original images. In this paper, we investigate the capabilities of the stochastic noise inputs of StyleGAN. We show that the stochastic noise inputs of a StyleGAN model can be used to transfer content and encode color information bypresenting an encoder architecture that, together with a pretrained and fixed StyleGAN model, is able to faithfully reconstruct images from virtually any domain. Thus, we demonstrate a previously unknown grade of generalizablility by training the encoder anddecoder independently and on different datasets. Our proposed architecture processes up to 45 images per second on a single GPU, which is approximately 32× faster than previous approaches. Finally, as one example application, our approach also shows promising results compared to the state of the art on image denoising tasks."
  - id: 1346
    order: 221
    oral_session: 5
    poster_session: 3
    session_id: 6
    title: "Temporal Meta-Adaptor for Video Object Detection"
    authors:
      - author: "Chi Wang (Queen's University Belfast)"
      - author: "Yang Hua (Queen's University Belfast)"
      - author: "ZHENG LU (Anyvision)"
      - author: "Jian Gao (Queen's University Belfast)"
      - author: "Neil Robertson (Queen's University Belfast)"
    all_authors: "Chi Wang, Yang Hua, ZHENG LU, Jian Gao and Neil Robertson"
    code: ""
    keywords:
      - word: "video object detection"
      - word: "temporal aggregation"
      - word: "meta-learning"
      - word: "ImageNet VID"
      - word: ""
    paper: "papers/1346.pdf"
    supp: "supp/1346_supp.zip"
    abstract: "Detecting objects in a video can be difficult due to occlusions and motion blur, where the output features are easily deteriorated. Recent state-of-the-art methods propose to enhance the features of the key frame with reference frames using attention modules. However, the feature enhancement uses the features extracted from a fixed backbone. It is fundamentally hard for a fixed backbone to generate discriminative features for the frames of both low and high quality. To mitigate this challenge, in this paper, we present a meta-learning scheme that learns to adapt the backbone using temporal features. Specifically, we propose to summarise the temporal feature into a fixed size representation, which is then used to make the backbone generate adaptively discriminative features for low and high quality frames. We demonstrate that the proposed approach can be easily incorporated into latest temporal aggregation approaches with almost no impact on the inference speed. Experiments on ImageNet VID dataset show a consistent gain over state-of-the-art methods."
  - id: 1349
    order: 90
    poster_session: 1
    session_id: 2
    title: "IamAlpha: Instant and Adaptive Mobile Network for Alpha Matting"
    authors:
      - author: "Avinav Goel (Samsung Research Institute Bangalore)"
      - author: "Manoj Kumar (Samsung Research Institute Bangalore)"
      - author: "Pavan Sudheendra (Samsung Research Institute, Bangalore	)"
    all_authors: "Avinav Goel, Manoj Kumar and Pavan Sudheendra"
    code: ""
    keywords:
      - word: "matting"
      - word: "image matting"
      - word: "alpha matting"
      - word: "real time matting"
      - word: "video matting"
      - word: "adaptive trimap"
      - word: "trimap estimation"
      - word: "mobile network"
      - word: "auxiliary task"
      - word: "alphamatting benchmark"
    paper: "papers/1349.pdf"
    supp: ""
    abstract: "Extraction of high quality alpha mattes from natural images has been a crucial problem with wide range of applications in the real world. Currently, most of the image matting techniques require a marked unknown region known as “Trimap”, as input for estimating alpha. But due to lack of trimap, majority of the techniques tend to generate the trimap by eroding and dilating ground-truth alpha maps. This in turn makes the prior-art inflexible towards minor inaccuracies introduced while making use of segmentation-based trimaps. In this paper, we introduce a novel, state of the art alpha matting model, “IamAlpha”, which uses trimap adaptation as an auxiliary task to adapt and fix the input trimap errors so that our alpha network focuses primarily on estimating transparency of high-level features (fine structures like hair, furs etc.) crucial to image matting. This in-turn helps us to enable high quality matting applications in real time at 60fps on GPU and 30fps on mobile hardware."
  - id: 1352
    order: 311
    poster_session: 3
    session_id: 8
    title: "FAST3D: Flow-Aware Self-Training for 3D Object Detectors"
    authors:
      - author: "Christian Fruhwirth-Reisinger (Graz University of Technology)"
      - author: "Michael Opitz (Graz University of Technology)"
      - author: "Horst Possegger (Graz University of Technology)"
      - author: "Horst Bischof (Graz University of Technology)"
    all_authors: "Christian Fruhwirth-Reisinger, Michael Opitz, Horst Possegger and Horst Bischof"
    code: ""
    keywords:
      - word: "unsupervised domain adaptation"
      - word: "self-training"
      - word: "3D object detection"
      - word: "scene flow"
      - word: "LiDAR point cloud"
      - word: "source-free domain adaptation"
    paper: "papers/1352.pdf"
    supp: "supp/1352_supp.zip"
    abstract: "In the field of autonomous driving, self-training is widely applied to mitigate distribution shifts in LiDAR-based 3D object detectors. This eliminates the need for expensive, high-quality labels whenever the environment changes (e.g. geographic location, sensor setup, weather condition). State-of-the-art self-training approaches, however, mostly ignore the temporal nature of autonomous driving data.
To address this issue, we propose a flow-aware self-training method that enables unsupervised domain adaptation for 3D object detectors on continuous LiDAR point clouds. In order to get reliable pseudo-labels, we leverage scene flow to propagate detections through time. In particular, we introduce a flow-based multi-target tracker that exploits flow consistency to filter and refine resulting tracks. The emerged precise pseudo-labels then serve as a basis for model re-training. Starting with a pre-trained KITTI model, we conduct experiments on the challenging Waymo Open Dataset to demonstrate the effectiveness of our approach. Without any prior target domain knowledge, our results show a significant improvement over the state-of-the-art."
  - id: 1355
    order: 205
    poster_session: 2
    session_id: 5
    title: "Hidden-Fold Networks: Random Recurrent Residuals Using Sparse Supermasks"
    authors:
      - author: "Ángel López García-Arias (Tokyo Institute of Technology)"
      - author: "Masanori Hashimoto (Kyoto University)"
      - author: "Masato Motomura (Tokyo Institute of Technology)"
      - author: "Jaehoon Yu (Tokyo Institute of Technology)"
    all_authors: "Ángel López García-Arias, Masanori Hashimoto, Masato Motomura and Jaehoon Yu"
    code: "https://github.com/Lopez-Angel/hidden-fold-networks"
    keywords:
      - word: "computer vision"
      - word: "neural networks"
      - word: "deep learning"
      - word: "residual networks"
      - word: "recurrent networks"
      - word: "supermask training"
      - word: "pruning"
      - word: "network compression"
      - word: "small scale networks"
      - word: "sparse networks"
      - word: ""
    paper: "papers/1355.pdf"
    supp: ""
    abstract: "Deep neural networks (DNNs) are so over-parametrized that recent research has found them to already contain a subnetwork with high accuracy at their randomly initialized state. Finding these subnetworks is a viable alternative training method to weight learning. In parallel, another line of work has hypothesized that deep residual networks (ResNets) are trying to approximate the behaviour of shallow recurrent neural networks (RNNs) and has proposed a way for compressing them into recurrent models. This paper proposes blending these lines of research into a highly compressed yet accurate model: Hidden-Fold Networks (HFNs). By first folding ResNet into a recurrent structure and then searching for an accurate subnetwork hidden within the randomly initialized model, a high-performing yet tiny HFN is obtained without ever updating the weights. As a result, HFN achieves equivalent performance to ResNet50 on CIFAR100 while occupying 38.5x less memory, and similar performance to ResNet34 on ImageNet with a memory size 26.8x smaller. The HFN will become even more attractive by minimizing data transfers while staying accurate when it runs on highly-quantized and randomly-weighted DNN inference accelerators. Code available at: https://github.com/Lopez-Angel/hidden-fold-networks"
  - id: 1360
    order: 227
    oral_session: 6
    poster_session: 3
    session_id: 7
    title: "A 2-D Wrist Motion Based Sign Language Video Summarization"
    authors:
      - author: "Evangelos Sartinas (University of Patras)"
      - author: "Emmanouil Psarakis (University of Patras)"
      - author: "Klimis Antzakas (University of Patras)"
      - author: "Dimitrios Kosmopoulos (University of Patras)"
    all_authors: "Evangelos Sartinas, Emmanouil Psarakis, Klimis Antzakas and Dimitrios Kosmopoulos"
    code: ""
    keywords:
      - word: "video summarization"
      - word: "curvature"
      - word: "sign language"
    paper: "papers/1360.pdf"
    supp: "supp/1360_supp.zip"
    abstract: "In this paper we present a keyframe extraction scheme based on the wrist motion using differential geometry. More specifically, the time (t)-parameterized Frennet-Serret frame for tracking the signer's wrist is used and the curvature of the trajectory, is proposed for the identification of the Sign Language (SL) video keyframes. Specifically, a video frame is characterized as keyframe if on that time instance the t-parameterized curvature function attains a maximum value. 
Finally, in order to properly define the wrist 2-D motion model, a skeleton tracker is used.
The proposed scheme is adaptable, i.e., the number of extracted keyframes varies according to the complexity of the signs, while preserving the semantic content.
This in turn makes it attractive for applications like video-calling. Its performance in terms of the achieved compression and intelligibility ratios was evaluated on a ground-truth sequence and outperformed its s-parameterized counterpart (s is the arc length); it also outperformed a moment-based SL summarization technique. Furthermore, the proposed scheme was experimentally evaluated on a dataset containing 5500 signs by SL specialists with very promising results.  Finally, the proposed keyframe extraction was evaluated against the aforementioned techniques  on the same dataset via the use of a GRU neural network on the gloss classification problem; its superior accuracy in identifying the gloss meaning was confirmed."
  - id: 1364
    order: 206
    poster_session: 2
    session_id: 5
    title: "A Simple Approach to Image Tilt Correction with Self-Attention MobileNet for Smartphones"
    authors:
      - author: "Siddhant Garg (University of Massachusetts Amherst)"
      - author: "DEBI PRASANNA MOHANTY (SAMSUNG R&D INSTITUTE-BANGALORE)"
      - author: "Siva Thota (Samsung)"
      - author: "Sukumar Moharana (Samsung Research & Developement Institute Bangalore)"
    all_authors: "Siddhant Garg, DEBI PRASANNA MOHANTY, Siva Thota and Sukumar Moharana"
    code: ""
    keywords:
      - word: "Self Attention MobileNet"
      - word: "Image Tilt Correction"
      - word: "Image Tilt Prediction"
      - word: "Image Orientation"
    paper: "papers/1364.pdf"
    supp: ""
    abstract: "We present a Self-Attention MobileNet, called SA-MobileNet Network for tackling the fine-grained image tilt correction problem. SA-MobileNet contains self-attention modules integrated with the inverted bottleneck blocks of the MobileNetV3 model which results in modeling of both channel-wise attention and spatial attention of the image features and at the same time introduce a novel self-attention architecture for low-resource devices. We treat the problem of image tilt correction in a multi-label scenario where we predict multiple angles for a tilted input image in a narrow interval of range 1 or 2 degrees, depending on the dataset used. With the combination of our novel approach and the architecture, we present state-of-the-art results on detecting the image tilt angle on mobile devices as compared to the MobileNetV3 model. SA-MobileNet is more accurate than MobileNetV3 on SUN397, NYU-V1, and ADE20k datasets by 6.42%, 10.51%, and 9.09% points respectively. Furthermore, the proposed neural network architecture is faster by approximately 4ms from the MobileNetV3 model on Snapdragon 750 Octa-core, despite a slight overhead in the number of parameters."
  - id: 1371
    order: 91
    poster_session: 1
    session_id: 2
    title: "An Adaptive Rectification Model for Arbitrary-Shaped Scene Text Recognition"
    authors:
      - author: "Ye Qian (Nanjing University)"
      - author: "Long Chen (Nanjing University)"
      - author: "Feng Su (Nanjing University)"
    all_authors: "Ye Qian, Long Chen and Feng Su"
    code: ""
    keywords:
      - word: "scene text recognition"
      - word: "rectification"
      - word: "projective transformation"
      - word: ""
    paper: "papers/1371.pdf"
    supp: "supp/1371_supp.zip"
    abstract: "Recognizing scene text in natural images is challenging due to the irregular or distorted shapes of many text instances. In this paper, we propose a novel adaptive rectification model for robust recognition of arbitrary-shaped scene text. The rectification model approximates the complex non-uniform deformation required for rectifying the text with a group of localized linear projective transformations, which better preserve text's shape characteristics than non-linear deformations like TPS during the rectification. By end-to-end training with a text recognition network, the rectification model can effectively learn to transform the input text image to a more regular form that simplifies subsequent recognition. Experiment results on benchmarks demonstrate the effectiveness of the proposed rectification model for scene text recognition."
  - id: 1376
    order: 312
    poster_session: 3
    session_id: 8
    title: "Conditional Model Selection for Efficient Video Understanding"
    authors:
      - author: "Mihir Jain (Qualcomm AI Research)"
      - author: "Haitam Ben Yahia (Qualcomm AI Research)"
      - author: "Amir Ghodrati (Qualcomm AI Research)"
      - author: "Amirhossein Habibian (Qualcomm AI Research)"
      - author: "Fatih Porikli (Qualcomm AI Research)"
    all_authors: "Mihir Jain, Haitam Ben Yahia, Amir Ghodrati, Amirhossein Habibian and Fatih Porikli"
    code: ""
    keywords:
      - word: "action recognition"
      - word: "efficient classification"
      - word: "efficient localization"
      - word: "conditional compute"
      - word: ""
    paper: "papers/1376.pdf"
    supp: "supp/1376_supp.zip"
    abstract: "Video action classification and temporal localization are two key components of video understanding where we witnessed significant progress leveraging neural network architectures. Recently, the research focus in this area shifted towards computationally efficient solutions to support real-world applications. Existing methods mainly aim to pick salient frames or video clips with fixed architectures. As an alternative, here, we propose to learn policies to select the most efficient neural model conditioned on the given input video. Specifically, we train offline a novel model-selector with model-affinity annotations that consolidate recognition quality and computational load. Further, we incorporate the disparity between appearance and motion to estimate action background priors that enable efficient action localization without temporal annotations. To the best of our knowledge, this is the first attempt at computationally efficient action localization. We report classification results on two video benchmarks, Kinetics and multi-label HVU, and show that our method achieves state-of-the-art results while allowing a trade-off between accuracy and efficiency. For localization, we present evaluations on Thumos'14 and MultiThumos, where our approach improves or maintains the state-of-the-art performance while using only a fraction of the computation. "
  - id: 1377
    order: 207
    poster_session: 2
    session_id: 5
    title: "Structured Latent Embeddings for Recognizing Unseen Classes in Unseen Domains"
    authors:
      - author: "Shivam Chandhok (Indian Institute of Technology, Hyderabad)"
      - author: "Sanath Narayan (Inception Institute of Artificial Intelligence)"
      - author: "Hisham Cholakkal (MBZUAI)"
      - author: "Rao Muhammad Anwer  (MBZUAI/AALTO)"
      - author: "Vineeth N Balasubramanian (Indian Institute of Technology, Hyderabad)"
      - author: "Fahad Shahbaz Khan (MBZUAI)"
      - author: "Ling Shao (Inception Institute of Artificial Intelligence)"
    all_authors: "Shivam Chandhok, Sanath Narayan, Hisham Cholakkal, Rao Muhammad Anwer, Vineeth N Balasubramanian, Fahad Shahbaz Khan and Ling Shao"
    code: ""
    keywords:
      - word: "Zero-Shot"
      - word: "Domain Generalization"
      - word: "multimodal-alignment"
      - word: "domain-invariant"
      - word: "conceptual partition"
      - word: "semantics"
    paper: "papers/1377.pdf"
    supp: "supp/1377_supp.zip"
    abstract: "Zero-shot learning and domain generalization strive to overcome the scarcity of task-specific annotated data by individually addressing the issues of semantic and domain shifts, respectively. However, real-world applications often are unconstrained and require handling unseen classes in unseen domains, a setting called zero-shot domain generalization, which presents the issues of domain and semantic shifts simultaneously. Here, we propose a novel approach that learns domain-agnostic structured latent embeddings by projecting images from different domains and their class-specific semantic representations to a common latent space. Our method  jointly strives for the following objectives: 
(i) aligning the multimodal cues from visual and text-based semantic concepts; (ii) partitioning the common latent space according to the domain-agnostic class-level semantic concepts; and (iii) learning a domain invariance w.r.t the visual-semantic joint distribution for generalizing to unseen classes in unseen domains. Our experiments on challenging benchmarks such as DomainNet show the superiority of our approach over existing methods with significant gains on difficult domains like quickdraw and sketch."
  - id: 1378
    order: 313
    poster_session: 3
    session_id: 8
    title: "Order-independent Matching with Shape Similarity for Parking Slot Detection"
    authors:
      - author: "Yin Ziyi (Xi'an Jiaotong University)"
      - author: "Ruijin Liu (Xi`an Jiaotong Unversity)"
      - author: "Zejian Yuan (Xi‘an Jiaotong University)"
      - author: "Zhiliang Xiong (Shenzhen Forward Innovation Digital Technology Co. Ltd)"
    all_authors: "Yin Ziyi, Ruijin Liu, Zejian Yuan and Zhiliang Xiong"
    code: ""
    keywords:
      - word: "parking slot detection"
      - word: "matching strategy"
      - word: "shape cost"
    paper: "papers/1378.pdf"
    supp: "supp/1378_supp.zip"
    abstract: "Current mainstream methods adopt point regression and prior sizes to detect parking slots. Although these methods are approved to be useful in majority of circumstances, they have many limitations when they adapt to size and shape variations in more general slots because of pre-defined fixed order, lack of holistic constraints and adequately diverse data. To address them, we propose an order-independent matching strategy with shape similarity to handle the more general slot sizes and shapes. The matching strategy adopts a two-level procedure: the point-level and the parking slot-level, that finds optimal order and association adaptively. More importantly, we adopt shape similarity to represent holistic geometry to rank slots so as to suppress the misshapen ones. Furthermore, we collected a large-scale and remote-view parking slot dataset (LRPS) to improve data diversity. It contains a large number of general parking environments, as well as slots of various shapes and sizes across different cities, daytime and interior-exterior scenes. The proposed approach is evaluated on the LRPS dataset and achieves superior performance to previous methods. "
  - id: 1381
    order: 92
    poster_session: 1
    session_id: 2
    title: "CamLessMonoDepth: Monocular Depth Estimation with Unknown Camera Parameters"
    authors:
      - author: "Sai Shyam Chanduri (DFKI)"
      - author: "Igor Vozniak (DFKI)"
      - author: "Zeeshan Khan Suri (SETES GmbH)"
    all_authors: "Sai Shyam Chanduri, Igor Vozniak and Zeeshan Khan Suri"
    code: ""
    keywords:
      - word: "monocular depth estimation"
      - word: "self-supervised learning"
      - word: "single-camera egomotion"
      - word: "camera intrinsics estimation"
      - word: "sub-pixel convolutions"
      - word: "uncertainty estimation"
      - word: ""
    paper: "papers/1381.pdf"
    supp: "supp/1381_supp.zip"
    abstract: "Perceiving 3D information is of paramount importance in many applications of computer vision. Recent advances in monocular depth estimation have shown that gaining such knowledge from a single camera input is possible by training deep neural networks to predict inverse depth and pose, without the necessity of ground truth data. The majority of such approaches, however, require camera parameters to be fed explicitly during training. As a result, image sequences from the wild cannot be used during training. While there exist methods which also predict camera intrinsics, their performance is not on par with novel methods which take camera parameters as input. In this work, we propose a method for implicit estimation of pinhole camera intrinsics along with depth and pose, by learning from monocular image sequences alone. In addition, by utilizing efficient sub-pixel convolutions, we show that high fidelity depth estimates can be obtained. We also embed pixel-wise uncertainty estimation into the framework, to emphasize the possible applicability of this work in practical domain. Finally, we demonstrate the possibility of accurate prediction of depth information without prior knowledge of camera intrinsics, while outperforming the existing state-of-the-art approaches on KITTI benchmark."
  - id: 1386
    order: 420
    poster_session: 4
    session_id: 11
    title: "WAN: Watermarking Attack Network"
    authors:
      - author: "Seung-Hun Nam (NAVER WEBTOON Corp.)"
      - author: "IN JAE YU (Samsung Electronics)"
      - author: "Seung-Min Mun (Samsung Electronics)"
      - author: "Daesik Kim (Naver webtoon)"
      - author: "Wonhyuk Ahn (NAVER WEBTOON Corp.)"
    all_authors: "Seung-Hun Nam, IN JAE YU, Seung-Min Mun, Daesik Kim and Wonhyuk Ahn"
    code: ""
    keywords:
      - word: "watermarking attack"
      - word: "watermark bit inversion"
      - word: "neural network-based watermarking attack"
      - word: ""
    paper: "papers/1386.pdf"
    supp: "supp/1386_supp.zip"
    abstract: "Multi-bit watermarking (MW) has been developed to improve robustness against signal processing operations and geometric distortions. To this end, benchmark tools that test robustness by applying simulated attacks on watermarked images are available. However, limitations in these  general attacks exist since they cannot exploit specific characteristics of the targeted MW. In addition, these attacks are usually devised without consideration of visual quality, which rarely occurs in the real world. To address these limitations, we propose a watermarking attack network (WAN), a fully trainable watermarking benchmark tool that utilizes the weak points of the target MW and induces an inversion of the watermark bit, thereby considerably reducing the watermark extractability. To hinder the extraction of hidden information while ensuring high visual quality, we utilize a residual dense blocks-based architecture specialized in local and global feature learning. A novel watermarking attack loss is introduced to break the MW systems. We empirically demonstrate that the WAN can successfully fool various block-based MW systems. Moreover, we show that existing MW methods can be improved with the help of the WAN as an add-on module."
  - id: 1387
    order: 421
    poster_session: 4
    session_id: 11
    title: "Robust Crowd Counting via Image Enhancement and Dynamic Feature Selection"
    authors:
      - author: "Nayeong Kim (POSTECH)"
      - author: "Suha Kwak (POSTECH)"
    all_authors: "Nayeong Kim and Suha Kwak"
    code: ""
    keywords:
      - word: "crowd counting"
      - word: "robustness"
      - word: "image enhancement"
      - word: "dynamic feature selection"
      - word: ""
    paper: "papers/1387.pdf"
    supp: "supp/1387_supp.zip"
    abstract: "In spite of their remarkable success in many vision tasks, convolutional neural networks (CNNs) often has trouble counting people in crowded scenes due to the following reasons. First, ordinary CNNs with fixed receptive fields are inadequate to handle diverse sizes and densities of people. Second, CNNs for counting are sensitive to brightness and contrast changes of input image. This paper proposes a new CNN for crowd counting that resolves these two issues. First, we develop a new counting network called pyramid feature selection network (PFSNet) that adapts its receptive fields dynamically to local crowd densities of the input image. Second, we introduce a light-weight and effective image enhancement network, which manipulates input image to normalize its condition and make it more counting-friendly, leading to robust and improved crowd counting. The concatenation of the two networks, dubbed E-PFSNet, achieves the state of the art on three public benchmarks for crowd counting. Also, it outperforms previous arts in terms of robustness against changes in image conditions as well as counting accuracy."
  - id: 1388
    order: 208
    poster_session: 2
    session_id: 5
    title: "WP2-GAN: Wavelet-based Multi-level GAN for Progressive Facial Expression Translation with Parallel Generators"
    authors:
      - author: "Jun Shao (Concordia University)"
      - author: "Tien Bui (Concordia University)"
    all_authors: "Jun Shao and Tien Bui"
    code: ""
    keywords:
      - word: "expression translation"
      - word: "parallel training"
      - word: "progressive training"
      - word: "wavelet packet transform"
      - word: "multi-level GAN"
    paper: "papers/1388.pdf"
    supp: "supp/1388_supp.zip"
    abstract: "Expression translation has received increasing attention from the computer vision community due to its wide applications in the real world. However, expression synthesis is hard because of the non-linear properties of facial skin and muscle caused by different expressions. A recent study showed that the practice of using the same  generator  for both  forward prediction and  backward  reconstruction as in current conditional GANs would force the generator to leave a potential \"noise\" in the generated images, therefore hindering the use of the images for further tasks. To eliminate the interference and break the unwanted link between the first and second translation, we design a parallel training mechanism with two generators that perform the same first translation but work as a reconstruction model for each other. Additionally, inspired by the successful application of wavelet-based multi-level Generative Adversarial Networks(GANs) in face aging and progressive training in geometric conversion, we further design a novel wavelet-based multi-level Generative Adversarial Network (WP2-GAN) for expression translation with a large gap based on a progressive and parallel training strategy. Extensive experiments show the effectiveness of our approach for expression translation compared with the state-of-the-art models by synthesizing photo-realistic images with high fidelity and vivid expression effect. "
  - id: 1391
    order: 209
    poster_session: 2
    session_id: 5
    title: "OODformer: Out-Of-Distribution Detection Transformer"
    authors:
      - author: "Rajat Koner (Ludwig Maximilian University of Munich)"
      - author: "Poulami Sinhamahapatra (Fraunhofer IKS)"
      - author: "Karsten Roscher (Fraunhofer IKS)"
      - author: "Stephan Günnemann (Technical University of Munich)"
      - author: "Volker Tresp (Siemens AG and Ludwig Maximilian University of Munich	)"
    all_authors: "Rajat Koner, Poulami Sinhamahapatra, Karsten Roscher, Stephan Günnemann and Volker Tresp"
    code: "https://github.com/rajatkoner08/oodformer"
    keywords:
      - word: "Out-Of-Distribution Detection"
      - word: "Vision Transfomer"
      - word: "Repsentation Learning"
    paper: "papers/1391.pdf"
    supp: "supp/1391_supp.pdf"
    abstract: "A serious problem in image classification is that a trained model might perform well for input data that originates from the same distribution as the data available for model training, but performs much worse for out-of-distribution (OOD) samples. In real-world safety-critical applications, in particular, it is important to be aware if a new data point is OOD. To date, OOD detection is typically addressed using either confidence scores, auto-encoder-based reconstruction, or contrastive learning. However, the global image context has not yet been explored to discriminate the non-local objectness between in-distribution and OOD samples.  This paper proposes a first-of-its-kind OOD detection architecture named OODformer that leverages the contextualization capabilities of the transformer. Incorporating the transformer as the principle feature extractor allows us to exploit the object concepts and their discriminate attributes along with their co-occurrence via visual attention.  Using the contextualized embedding, we demonstrate OOD detection using both class-conditioned latent space similarity and a  network confidence score.   Our approach shows improved generalizability across various datasets.   We have achieved a new state-of-the-art result on CIFAR-10/-100 and ImageNet30"
  - id: 1394
    order: 93
    poster_session: 1
    session_id: 2
    title: "MAGECally invert images for realistic editing"
    authors:
      - author: "Asya Grechka (meero)"
      - author: "jean Francois Goudou (Meero)"
      - author: "Matthieu Cord (Sorbonne University)"
    all_authors: "Asya Grechka, jean Francois Goudou and Matthieu Cord"
    code: ""
    keywords:
      - word: "gan inversion"
      - word: "gan"
      - word: "stylegan2"
      - word: "gan editing"
      - word: "image editing"
      - word: "gan projection"
      - word: "stylegan"
      - word: "semantic editing"
      - word: "latent space manipulation"
      - word: "latent editing"
      - word: ""
    paper: "papers/1394.pdf"
    supp: "supp/1394_supp.zip"
    abstract: "Generative Adversarial Networks (GANs) are now able to generate astonishingly realistic high-resolution images. Recent work has shown the emergence of meaningful manipulations simply by editing the corresponding latent vector. However, a real image must first be inverted into its GAN latent code before editing. Previous work  usually achieves accurate reconstruction, but poor-quality latent vectors: applying known editing methods onto these latent codes results in artifacts and erroneous edits.
We aim to bridge the gap between reconstruction and editability. We propose a novel instance-optimization based inversion method, which specifically aims to maximize the semantic information of the latent vector, all while producing an accurate reconstruction. We introduce the iMAGe-latEnt Consistency loss (\"MAGEC''), which allows supervision in the latent space, encouraging editability of the resulting latent vector. We provide extensive qualitative and quantitative evaluation to validate our method, using the recent state-of-the-art StyleGAN and show that our method outperforms baseline inversion methods, opening the door to new realms of real-image editing."
  - id: 1395
    order: 94
    poster_session: 1
    session_id: 2
    title: "GhostShiftAddNet: More Features from Energy-Efficient Operations"
    authors:
      - author: "jia bi (University of Southampton)"
      - author: "Jonathon Hare (University of Southampton)"
      - author: "Geoff V Merrett (University of Southampton)"
    all_authors: "Jia Bi, Jonathon Hare and Geoff V Merrett"
    code: "https://github.com/JIABI/GhostShiftAddNet"
    keywords:
      - word: "Efficient convolutional neural network"
      - word: "embedded platform"
      - word: "feature redundancy"
      - word: "image classifier."
    paper: "papers/1395.pdf"
    supp: ""
    abstract: "Deep convolutional neural networks (CNNs) are computationally and memory intensive. In CNNs, intensive multiplication can have resource implications that may challenge the ability for effective deployment of inference on resource-constrained edge devices. This paper proposes GhostShiftAddNet, where the motivation is to implement a hardware-efficient deep network: a multiplication-free CNN with less redundant features. We introduce a new bottleneck block, GhostSA, that converts all multiplications in the block to cheap operations. The bottleneck uses an appropriate number of bit-shift filters to process intrinsic feature maps, then applies a series of transformations that consist of bit-shifts with addition operations to generate more feature maps that fully learn information underlying intrinsic features. We schedule the number of bit-shift and addition operations for different hardware platforms. We conduct extensive experiments and ablation studies with desktop and embedded (Jetson Nano) devices for implementation and measurements. We demonstrate the proposed GhostSA block can replace bottleneck blocks in the backbone of state-of-the-art networks architectures and gives improved performance on image classification benchmarks. Further, our GhostShiftAddNet can achieve higher classification accuracy by using fewer FLOPs and parameters (reduced by up to 3x) than GhostNet. When compared to GhostNet, inference latency on the Jetson Nano is improved by about 1.3x and 2x on GPU and CPU respectively."
  - id: 1400
    order: 314
    poster_session: 3
    session_id: 8
    title: "FETNet: Feature Exchange Transformer Network for RGB-D Object Detection"
    authors:
      - author: "Zhibin Xiao (Tsinghua University)"
      - author: "Jing-Hao Xue (University College London)"
      - author: "Pengwei Xie (Tsinghua University)"
      - author: "Guijin Wang (Tsinghua University)"
    all_authors: "Zhibin Xiao, Jing-Hao Xue, Pengwei Xie and Guijin Wang"
    code: ""
    keywords:
      - word: "RGB-D object detection"
      - word: "Multi-modal Fusion"
      - word: "Vision Transformer"
      - word: "Feature Exchange"
    paper: "papers/1400.pdf"
    supp: ""
    abstract: "In RGB-D object detection, due to the inherent difference between the RGB and Depth modalities, it remains challenging to simultaneously leverage sensed photometric and depth information. In this paper, to address this issue, we propose a Feature Exchange Transformer Network (FETNet), which consists of two well-designed components: the Feature Exchange Module (FEM), and the Multi-modal Vision Transformer (MViT). Specially, we propose the FEM to exchange part of the channels between RGB and depth features at each backbone stage, which facilitates the information flow, and bridges the gap, between the two modalities. Inspired by the success of Vision Transformer (ViT), we develop the variant MViT to effectively fuse multi-modal features and exploit the attention between the RGB and depth features. Different from previous methods developing from specified RGB detection algorithm, our proposal is generic. Extensive experiments prove that, when the proposed modules are integrated into mainstream RGB object detection methods, their RGB-D counterparts can obtain significant performance gains. Moreover, our FETNet surpasses state-of-the-art RGB-D detectors by 7.0% mAP on SUN RGB-D and 1.7% mAP on NYU Depth v2, which also well demonstrates the effectiveness of the proposed method."
  - id: 1405
    order: 422
    poster_session: 4
    session_id: 11
    title: "Subpixel Heatmap Regression for Facial Landmark Localization"
    authors:
      - author: "Adrian Bulat (Samsung AI Center, Cambridge)"
      - author: "Enrique Sanchez (Samsung AI Centre)"
      - author: "Georgios Tzimiropoulos (Queen Mary University of London)"
    all_authors: "Adrian Bulat, Enrique Sanchez and Georgios Tzimiropoulos"
    code: "https://www.adrianbulat.com/face-alignment"
    keywords:
      - word: "face alignment"
      - word: "landmarks estimation"
      - word: "face tracking"
      - word: ""
    paper: "papers/1405.pdf"
    supp: "supp/1405_supp.zip"
    abstract: "Deep Learning models based on heatmap regression have revolutionized the task of facial landmark localization with existing models working robustly under large poses, non-uniform illumination and shadows, occlusions and self-occlusions, low resolution and blur. However, despite their wide adoption, heatmap regression approaches suffer from discretization-induced errors related to both the heatmap encoding and decoding process. In this work we show that these errors have a surprisingly large negative impact on facial alignment accuracy. To alleviate this problem, we propose a new approach for the heatmap encoding and decoding process by leveraging the underlying continuous distribution. To take full advantage of the newly proposed encoding-decoding mechanism, we also introduce a Siamese-based training that enforces heatmap consistency across various geometric image transformations. Our approach offers noticeable gains across multiple datasets setting a new state-of-the-art result in facial landmark localization. Code alongside the pretrained models will be made available."
  - id: 1406
    order: 95
    poster_session: 1
    session_id: 2
    title: "Lightweight HDR Camera ISP for Robust Perception in Dynamic Illumination Conditions via Fourier Adversarial Networks"
    authors:
      - author: "Pranjay Shyam (Korea Advanced Institute of Science and technology)"
      - author: "Sandeep Singh Sengar (Department of Computer Science, University of Copenhagen, Denmark)"
      - author: "Kuk-Jin Yoon (KAIST)"
      - author: "Kyung-Soo Kim (KAIST)"
    all_authors: "Pranjay Shyam, Sandeep Singh Sengar, Kuk-Jin Yoon and Kyung-Soo Kim"
    code: ""
    keywords:
      - word: "Low Light Image Enhancement"
      - word: "High Dynamic Range Image Signal Processing"
      - word: "Fourier Adversarial Networks"
    paper: "papers/1406.pdf"
    supp: "supp/1406_supp.zip"
    abstract: "Limited dynamic range of commercial compact camera sensors results in inaccurate representation of scenes with varying illumination conditions, adversely affecting image quality and subsequently limiting the performance of underlying image processing algorithms. Current state-of-the-art (SoTA) convolutional neural networks (CNN) are developed as post-processing techniques to independently recover under/over-exposed images. However, when applied to images containing real-world degradations such as glare, high-beam, color bleeding with varying noise intensity, these algorithms amplify the degradations, further reducing image quality. To overcome these limitations, we propose a lightweight image enhancement algorithm following a two-stage approach sequentially balancing illumination and removing noise using frequency priors for structural guidance. Furthermore, to ensure realistic image quality, we leverage the relationship between frequency and spatial domain properties of an image and propose a Fourier spectrum based adversarial framework (AFNet) for consistent image enhancement under varying illumination conditions. While current formulations of image enhancement are envisioned as post-processing techniques, we examine if such an algorithm could be extended to integrate the functionality of Image Signal Processing (ISP) pipeline within the camera sensor benefiting from RAW sensor data and Lightweight CNN architecture. Based on quantitative and qualitative evaluations, we extend our evaluation to examine the practicality and effects of image enhancement techniques on the performance of common perception tasks such as object detection and semantic segmentation in varying illumination conditions."
  - id: 1415
    order: 210
    poster_session: 2
    session_id: 5
    title: "Frequency learning for structured CNN filters with Gaussian fractional derivatives"
    authors:
      - author: "Nikhil Saldanha (Delft University of Technology)"
      - author: "Silvia-Laura L Pintea (TU Delft)"
      - author: "Jan C van Gemert (Delft University of Technology)"
      - author: "Nergis  Tomen (Delft University of Technology)"
    all_authors: "Nikhil Saldanha, Silvia-Laura L Pintea, Jan C van Gemert and Nergis  Tomen"
    code: ""
    keywords:
      - word: "structured filters"
      - word: "frequency learning"
      - word: "Gaussian derivative basis"
      - word: "fractional derivatives"
    paper: "papers/1415.pdf"
    supp: "supp/1415_supp.zip"
    abstract: "A structured CNN filter basis allows incorporating priors about natural image statistics and thus require less training examples to learn, saving valuable annotation time. Here, we build on the Gaussian derivative CNN filter basis that learn both the orientation and scale of the filters. However, this Gaussian filter basis definition depends on a predetermined derivative order, which typically results in fixed frequency responses for the basis functions whereas the optimal frequency of the filters should depend on the data and the downstream learning task. 

We show that by learning the order of the basis we can accurately learn the frequency of the filters, and hence adapt to the optimal frequencies for the underlying task. We investigate the well-founded mathematical formulation of fractional derivatives to adapt the filter frequencies during training. Our formulation leads to parameter savings and data efficiency when compared to the standard CNNs and the Gaussian derivative CNN filter networks that we build on. "
  - id: 1417
    order: 423
    poster_session: 4
    session_id: 11
    title: "Gaussian map predictions for 3D surface feature localisation and counting"
    authors:
      - author: "Justin Le Louëdec (University of Lincoln)"
      - author: "Grzegorz Cielniak (University of Lincoln)"
    all_authors: "Justin Le Louëdec and Grzegorz Cielniak"
    code: "https://github.com/lelouedec/PhD_3DPerception"
    keywords:
      - word: "Gaussian map"
      - word: "Strawberries"
      - word: "3D vision"
      - word: "counting"
      - word: "surface features"
      - word: "phenotyping"
      - word: "Machine learning"
      - word: "Agriculture"
      - word: "horticulture"
      - word: ""
    paper: "papers/1417.pdf"
    supp: ""
    abstract: "In this paper, we propose to employ a Gaussian map representation to estimate precise location and count of 3D surface features, addressing the limitations of state-of-the-art methods based on density estimation which struggle in presence of local disturbances. Gaussian maps indicate probable object location and can be generated directly from keypoint annotations avoiding laborious and costly per-pixel annotations. We apply this method to the 3D spheroidal class of objects which can be projected into 2D shape representation enabling efficient processing by a neural network GNet, an improved UNet architecture, which generates the likely locations of surface features and their precise count. We demonstrate a practical use of this technique for counting strawberry achenes which is used as a fruit quality measure in phenotyping applications. The results of training the proposed system on several hundreds of 3D scans of strawberries from a publicly available dataset demonstrate the accuracy and precision of the system which outperforms the state-of-the-art density-based methods for this application."
  - id: 1427
    order: 211
    poster_session: 2
    session_id: 5
    title: "Dynamic Feature Alignment for Semi-supervised Domain Adaptation"
    authors:
      - author: "Yu Zhang (University of Kentucky)"
      - author: "Gongbo Liang (Eastern Kentucky University)"
      - author: "Nathan Jacobs (University of Kentucky)"
    all_authors: "Yu Zhang, Gongbo Liang and Nathan Jacobs"
    code: ""
    keywords:
      - word: "domain adaptation"
      - word: "semi-supervised learning"
      - word: "image classification"
      - word: "memory bank"
      - word: "feature alignment"
    paper: "papers/1427.pdf"
    supp: ""
    abstract: "Most research on domain adaptation has focused on the purely unsupervised setting, where no labeled examples in the target domain are available. However, in many real-world scenarios, a small amount of labeled target data is available and can be used to improve adaptation. We address this semi-supervised setting and propose to use dynamic feature alignment to address both inter- and intra-domain discrepancy. Unlike previous approaches, which attempt to align source and target features within a mini-batch, we propose to align the target features to a set of dynamically updated class prototypes, which we use both for minimizing divergence and pseudo-labeling. By updating based on class prototypes, we avoid problems that arise in previous approaches due to class imbalances. Our approach, which doesn't require extensive tuning or adversarial training, significantly improves the state of the art for semi-supervised domain adaptation. We provide a quantitative evaluation on two standard datasets, DomainNet and Office-Home, and performance analysis. All code will be released via Github."
  - id: 1430
    order: 315
    poster_session: 3
    session_id: 8
    title: "AudViSum: Self-Supervised Deep Reinforcement Learning for Diverse Audio-Visual Summary Generation"
    authors:
      - author: "Sanjoy Chowdhury (ShareChat, Bangalore)"
      - author: "Aditya Patra (IIT Patna)"
      - author: "Subhrajyoti Dasgupta (Indian Statistical Institute, Kolkata)"
      - author: "Ujjwal Bhattacharya (ISI Kolkata)"
    all_authors: "Sanjoy Chowdhury, Aditya Patra, Subhrajyoti Dasgupta and Ujjwal Bhattacharya"
    code: ""
    keywords:
      - word: "video summarization"
      - word: "audio-viusal summarization"
      - word: "multi-modal learning"
      - word: "self-supervised learning"
      - word: "contrastive loss"
    paper: "papers/1430.pdf"
    supp: "supp/1430_supp.zip"
    abstract: "A brief yet comprehensive summary of a lengthy video helps us understand the key insights about it. Video summarization aims to generate a ‘video-thumbnail' from a given input video. Although the field has been widely studied in the literature, to the best of our knowledge, all the existing works in this area have majorly emphasized on visual modality only, although its audio component may carry crucial information for efficient video summarization. To this end, we introduce a novel self-supervised audio-visual summarization network AudViSum that leverages both audio and visual information and employs Deep Reinforcement Learning to reward the model to generate diverse yet semantically meaningful summaries. Our experiments establish the fact that combining audio-visual information helps to generate realistic summaries from relatively lengthy input videos. To ensure diverse summary generation we report the top-3 summaries for each video. Since there is no publicly available annotation to evaluate audio-visual summaries, we annotate the TVSum & OVP datasets comprising 50 videos each. Experimental results indicate that AudViSum achieves promising performance in the audio-visual summary generation task when compared against human annotations."
  - id: 1431
    order: 424
    poster_session: 4
    session_id: 11
    title: "Multi-view Image-based Hand Geometry Refinement using Differentiable Monte Carlo Ray Tracing"
    authors:
      - author: "Giorgos Karvounas (CSD-UOC and ICS-FORTH)"
      - author: "Nikolaos Kyriazis (FORTH)"
      - author: "Iason Oikonomidis (FORTH)"
      - author: "Aggeliki Tsoli (FORTH)"
      - author: "Antonis A Argyros (CSD-UOC and ICS-FORTH)"
    all_authors: "Giorgos Karvounas, Nikolaos Kyriazis, Iason Oikonomidis, Aggeliki Tsoli and Antonis A Argyros"
    code: ""
    keywords:
      - word: "hand geometry refinement"
      - word: "differentiable rendering"
      - word: "multiview"
      - word: "hand texture"
      - word: "ray tracing"
      - word: "rasterization"
    paper: "papers/1431.pdf"
    supp: "supp/1431_supp.zip"
    abstract: "The amount and quality of datasets and tools available in the research field of hand pose and shape estimation act as evidence to the significant progress that has been made. We find that there is still room for improvement in both fronts, and even beyond. Even the datasets of the highest quality, reported to date, have shortcomings in annotation. There are tools in the literature that can assist in that direction and yet they have not been considered, so far. To demonstrate how these gaps can be bridged, we employ such a publicly available, multi-camera dataset of hands (InterHands), and perform effective image-based refinement to improve on the imperfect ground truth annotations, yielding a better dataset. The image-based refinement is achieved through raytracing, a method that has not been employed so far to relevant problems and is hereby shown to be superior to the approximative alternatives that have been employed in the past. To tackle the lack of reliable ground truth, we resort to realistic synthetic data, to show that the improvement we induce is indeed significant, qualitatively, and quantitatively, too."
  - id: 1435
    order: 316
    poster_session: 3
    session_id: 8
    title: "MIGS: Meta Image Generation from Scene Graphs"
    authors:
      - author: "Azade Farshad (Technical University of Munich)"
      - author: "Sabrina Musatian (Technical University of Munich)"
      - author: "Helisa Dhamo (Technical University of Munich)"
      - author: "Nassir Navab (TU Munich, Germany)"
    all_authors: "Azade Farshad, Sabrina Musatian, Helisa Dhamo and Nassir Navab"
    code: ""
    keywords:
      - word: "Scene Graphs"
      - word: "Meta-learning"
      - word: "Image Generation"
    paper: "papers/1435.pdf"
    supp: "supp/1435_supp.zip"
    abstract: "Image generation and manipulation have been attractive topics of research in recent years. Generation of images from scene graphs is a promising direction towards explicit scene generation and manipulation. However, the images generated from the scene graphs lack quality, which in part comes due to high difficulty and diversity in the data. We propose MIGS (Meta Image Generation from Scene Graphs), a meta-learning based approach for few-shot image generation from graphs that enables adapting the model to different scenes and increases the image quality by training on diverse sets of tasks. By sampling the data in a task-driven fashion, we train the generator using meta-learning on different sets of tasks that are categorized based on the scene attributes. Our results show that using this meta-learning approach for the generation of images from scene graphs achieves state-of-the-art performance in terms of image quality and capturing the semantic relationships in the scene."
  - id: 1436
    order: 425
    poster_session: 4
    session_id: 11
    title: "Surround-view Free Space Boundary Detection with Polar Representation"
    authors:
      - author: "Zidong Cao (Xi'an Jiaotong University)"
      - author: "Ang Li (Xi‘an Jiaotong University)"
      - author: "Zhiliang Xiong (Shenzhen Forward Innovation Digital Technology Co. Ltd)"
      - author: "Zejian Yuan (Xi‘an Jiaotong University)"
    all_authors: "Zidong Cao, Ang Li, Zhiliang Xiong and Zejian Yuan"
    code: ""
    keywords:
      - word: "free space detection"
      - word: "polar representation"
      - word: "automatic vehicle"
    paper: "papers/1436.pdf"
    supp: "supp/1436_supp.zip"
    abstract: "Vision-based surround-view free space detection is crucial for automatic parking assist. In this task, precise boundary localization is the most concerned problem. In this paper, we have proposed to reframe the free space as polar representation for the free space boundary, and exploit a transformer framework to regress the representation end-to-end. To restrain the overall shape of the free space, we have introduced a Triangle-IoU loss function, enabling the network to consider the boundary as a whole. Furthermore, we have proposed a challenging newly-built surround-view dataset (SVB) with boundary annotations and supplied a new metric for boundary quality. Experiments on SVB dataset validate the effectiveness of our method, which outperforms existing free space detection methods and runs in real-time with a remarkable reduction in the computational cost. Additionally, our method shows excellent generalization ability to new parking scenes."
  - id: 1438
    order: 212
    poster_session: 2
    session_id: 5
    title: "Dual Graph-Based Context Aggregation for Scene Parsing"
    authors:
      - author: "Mengyu Liu (University of Manchester)"
      - author: "Hujun Yin (University of Manchester )"
    all_authors: "Mengyu Liu and Hujun Yin"
    code: ""
    keywords:
      - word: "Neural network"
      - word: "semantic segmentation"
      - word: "scene parsing"
      - word: "graph reasoning"
    paper: "papers/1438.pdf"
    supp: ""
    abstract: "Exploiting global contextual information has been shown useful for improving performance of scene parsing and hence is widely used. In this paper, unlike previous work that captures long-range dependencies with multi-scale feature fusion or attention mechanism, we address the scene parsing tasks by aggregating rich contextual information based on graph reasoning. Specifically, we propose two graph reasoning modules, in which features are aggregated over the coordinate space and projected to the feature and probabilistic spaces, respectively. The feature graph reasoning module adaptively constructs pyramid graphs as multi-scale feature representations and then performs graph reasoning to model global context. Whilst, in the probabilistic graph reasoning module, graph reasoning is performed over a graph consisting of class-dependent representations generated by aggregating the pixels that belong to the same classes. We have conducted extensive experiments on the popular scene parsing datasets, including Cityscapes, PASCAL Context and ADE20K, and achieved state-of-the-art performances."
  - id: 1443
    order: 96
    poster_session: 1
    session_id: 2
    title: "Adverse Weather Image Translation with Asymmetric and Uncertainty-aware GAN"
    authors:
      - author: "Jeong-gi Kwak (Korea University)"
      - author: "Youngsaeng Jin (Korea University)"
      - author: "Yuanming Li (Korea University)"
      - author: "Dongsik Yoon (Korea University)"
      - author: "Donghyeon Kim (Korea university)"
      - author: "Hanseok Ko (Korea University)"
    all_authors: "Jeong-gi Kwak, Youngsaeng Jin, Yuanming Li, Dongsik Yoon, Donghyeon Kim and Hanseok Ko"
    code: ""
    keywords:
      - word: "GAN"
      - word: "adverse image translation"
      - word: "uncertainty learning"
    paper: "papers/1443.pdf"
    supp: "supp/1443_supp.zip"
    abstract: "Adverse weather image translation belongs to the unsupervised image-to-image (I2I) translation task which aims to transfer adverse condition domain (eg, rainy night) to standard domain (eg, day). It is a challenging task because images from adverse domains have some artifacts and insufficient information. Recently, many studies employing Generative Adversarial Networks (GANs) have achieved notable success in I2I translation but there are still limitations in applying them to adverse weather enhancement. Asymmetric architecture based on bidirectional cycle-consistency loss is adopted as a standard framework for unsupervised domain transfer methods. However, it can lead to inferior translation result if the two domains have {em imbalanced information}. To address this issue, we propose a novel GAN model which has an asymmetric architecture for adverse domain translation. We insert a proposed feature transfer network (${T}$-net) in only a normal domain generator (i.e., rainy night $rightarrow$ day) to enhance encoded features of the adverse domain image. In addition, we introduce asymmetric feature matching for disentanglement of encoded features. Finally, we propose uncertainty-aware cycle-consistency loss to address the regional uncertainty of a cyclic reconstructed image. We demonstrate the effectiveness of our method by qualitative and quantitative comparisons with state-of-the-art models.   "
  - id: 1445
    order: 317
    poster_session: 3
    session_id: 8
    title: "3D Object Tracking with Transformer"
    authors:
      - author: "Yubo Cui (Northeastern University)"
      - author: "Zheng Fang (Northeastern University)"
      - author: "jiayao shan (Northeastern University)"
      - author: "Zuoxu Gu (Northeastern University)"
      - author: "Sifan Zhou (Northeastern University)"
    all_authors: "Yubo Cui, Zheng Fang, Jiayao Shan, Zuoxu Gu and Sifan Zhou"
    code: "https://github.com/3bobo/lttr"
    keywords:
      - word: "3D object tracking"
      - word: "point cloud"
      - word: "transformer"
    paper: "papers/1445.pdf"
    supp: "supp/1445_supp.zip"
    abstract: "Feature fusion and similarity computation are two core problems in 3D object tracking, especially for object tracking using sparse and disordered point clouds. Feature fusion could make similarity computing more efficient by including target object information. However, most existing LiDAR-based approaches directly use the extracted point cloud feature to compute similarity while ignoring the attention changes of object regions during tracking. In this paper, we propose a feature fusion network based on transformer architecture. Benefiting from the self-attention mechanism, the transformer encoder captures the inter- and intra- relations among different regions of the point cloud. By using cross-attention, the transformer decoder fuses features and includes more target cues into the current point cloud feature to compute the region attentions, which makes the similarity computing more efficient. Based on this feature fusion network, we propose an end-to-end point cloud object tracking framework, a simple yet effective method for 3D object tracking using point clouds. Comprehensive experimental results on the KITTI dataset show that our method achieves new state-of-the-art performance. Code is available at: https://github.com/3bobo/lttr."
  - id: 1447
    order: 97
    poster_session: 1
    session_id: 2
    title: "Deep Degradation Prior for Real-World Super-Resolution"
    authors:
      - author: "Kyungdeuk Ko (Korea University)"
      - author: "Bokyeung Lee (Korea University)"
      - author: "Jonghwan Hong (Korea university)"
      - author: "David K Han (Drexel University)"
      - author: "Hanseok Ko (Korea University)"
    all_authors: "Kyungdeuk Ko, Bokyeung Lee, Jonghwan Hong, David K Han and Hanseok Ko"
    code: ""
    keywords:
      - word: "real-world super-resolution"
      - word: "single image super-resolution"
      - word: "image enhancement"
      - word: ""
    paper: "papers/1447.pdf"
    supp: "supp/1447_supp.zip"
    abstract: "Real-world Super-Resolution (SR) is a very challenging task to reconstruct a higher resolution image from a real-world image. Most of the high performance SR methods rely on availability of LR-HR paired datasets, target domain images, or degradation priors.  These information, however, are usually not available in real world use, thus these methods are not often practical for real world obtained images. Recent studies related to real-world SR mainly focus on constructing the LR-HR paired dataset. The methods estimate the noises and the blur kernels from real-world images to generate a new training set. However, these methods use the degradation only for dataset construction. In this paper, we propose a novel real-world SR method called Deep Degradation Prior-based SR (DDP-SR). Upon completion of training, denoising network and kernel estimation network within DDP-SR becomes capable of extracting degradation representation of any given input image. Thus, the model works regardless of whether the input image from the same or the different domain of the training set. As such, DDP-SR generalizes well on images from different domain while it also outperforms the state-of-the-art methods in the SR task."
  - id: 1451
    order: 426
    poster_session: 4
    session_id: 11
    title: "Duplicate Latent Representation Suppression for Multi-object Variational Autoencoders"
    authors:
      - author: "Li Nanbo (University of Edinburgh)"
      - author: "Robert B Fisher (University of Edinburgh)"
    all_authors: "Li Nanbo and Robert B Fisher"
    code: ""
    keywords:
      - word: "object-centric representation learning"
      - word: "variational autoencoders"
      - word: "scene representation"
    paper: "papers/1451.pdf"
    supp: "supp/1451_supp.zip"
    abstract: "Generative object-centric scene representation learning is crucial for structural visual scene understanding. Built upon variational autoencoders (VAEs)~cite{kingma2013auto}, current approaches infer a set of latent object representations to interpret a scene observation (e.g. an image) under the assumption that each part (e.g. a pixel) of a scene observation must be explained by one and only one object of the underlying scene. Despite the impressive performance these models achieved in unsupervised scene factorization and representation learning, we show empirically that they often produce duplicate scene object representations which directly harms the scene factorization performance. In this paper, we address the issue by introducing a differentiable prior that explicitly forces the inference to suppress duplicate latent object representations. The extension is evaluated by adding it to three different unsupervised scene factorization approaches. The results show that the models trained with the proposed method not only outperform the original models in scene factorization and have fewer duplicate representations, but also achieve better variational posterior approximations than the original models."
  - id: 1467
    order: 427
    poster_session: 4
    session_id: 11
    title: "Shape Feature Loss for Kidney Segmentation in 3D Ultrasound Images"
    authors:
      - author: "Haithem Boussaid (Philips)"
      - author: "Rouet Laurence (Philips Research Paris)"
    all_authors: "Haithem Boussaid and Rouet Laurence"
    code: ""
    keywords:
      - word: "Feature Shape Loss 3D medical image segmentation Kidney segmentation Prior Shape Knowledge Spatial Transformer Network"
    paper: "papers/1467.pdf"
    supp: ""
    abstract: "Kidney segmentation from 3D ultrasound images remains a challenging task due to low signal-to-noise ratio and low-contrasted object boundaries.
Most of recently proposed segmentation CNNs rely on loss functions where each voxel is treated independently. However, the desired output mask is a high-dimensional structured object. This fails to produce regularly shaped segmentation masks especially in complex cases. In this work, we design a loss function to compare segmentation masks in a feature space designed to describe explicit global shape attributes.
We use a Spatial Transformer Network to derive the 3D pose of a mask and we project the resulting aligned mask on a linear sub-space describing the variations across objects. The resulting shape-feature vector is a  concatenation of weighted shape rigid pose parameters and non-rigid deformation parameters with respect to a mean shape. We use the L1 function to compare the prediction and the ground truth shape-feature vectors.
We validate our method on a large 3D ultrasound kidney segmentation dataset. Using the same U-net Architecture, our loss function outperforms dice and cross entropy standard loss functions used in the nnU-net state-of-the-art approach."
  - id: 1468
    order: 428
    poster_session: 4
    session_id: 11
    title: "Personalized One-Shot Lipreading for an ALS Patient"
    authors:
      - author: "Bipasha Sen (IIIT Hyderabad)"
      - author: "Aditya Agarwal (IIIT Hyderabad)"
      - author: "Rudrabha Mukhopadhyay (IIIT Hyderabad)"
      - author: "Vinay Namboodiri (University of Bath)"
      - author: "C.V. Jawahar (IIIT-Hyderabad)"
    all_authors: "Bipasha Sen, Aditya Agarwal, Rudrabha Mukhopadhyay, Vinay Namboodiri and C.V. Jawahar"
    code: ""
    keywords:
      - word: "lipreading"
      - word: "variational autoencoders"
      - word: "domain adaptation"
      - word: "synthetic data augmentation"
      - word: "amyotrophic lateral sclerosis"
      - word: "medical"
      - word: "als"
      - word: ""
    paper: "papers/1468.pdf"
    supp: "supp/1468_supp.zip"
    abstract: "Lipreading or visually recognizing speech from the mouth movements of a speaker is a challenging and mentally taxing task. Unfortunately, multiple medical conditions force people to depend on this skill in their day-to-day lives for essential communication. Patients suffering from ‘Amyotrophic Lateral Sclerosis’ (ALS) often lose muscle control, consequently their ability to generate speech and communicate via lip movements. Existing large datasets do not focus on medical patients or curate personalized vocabulary relevant to an individual. Collecting large-scale dataset of a patient, needed to train modern data-hungry deep learning models is however, extremely challenging. In this work, we propose a personalized network to lipread an ALS patient using only one-shot examples. We depend on synthetically generated lip movements to augment the one-shot scenario. A Variational Encoder based domain adaptation technique is used to bridge the real-synthetic domain gap. Our approach significantly improves and achieves high top-5 accuracy with 83.2% accuracy compared to 62.6% achieved by comparable methods for the patient. Apart from evaluating our approach on the ALS patient, we also extend it to people with hearing impairment relying extensively on lip movements to communicate."
  - id: 1480
    order: 318
    poster_session: 3
    session_id: 8
    title: "Multi-attribute Pizza Generator: Cross-domain Attribute Control with Conditional StyleGAN"
    authors:
      - author: "Fangda Han (RUTGERS UNIVERSITY)"
      - author: "Guoyao Hao (Rutgers University)"
      - author: "Ricardo Guerrero (Samsung)"
      - author: "Vladimir Pavlovic (Rutgers University)"
    all_authors: "Fangda Han, Guoyao Hao, Ricardo Guerrero and Vladimir Pavlovic"
    code: "https://github.com/klory/MPG2"
    keywords:
      - word: "generative model"
      - word: "computer vision"
      - word: "generative adverserial nets"
      - word: ""
    paper: "papers/1480.pdf"
    supp: "supp/1480_supp.zip"
    abstract: "Multi-attribute conditional image generation is a challenging problem in computer vision. We propose Multi-attribute Pizza Generator (MPG), a conditional Generative Neural Network (GAN) framework for synthesizing images from a trichotomy of attributes: content, view-geometry, and implicit visual style. We design MPG by extending the state-of-the-art StyleGAN2, using a new conditioning technique that guides the intermediate feature maps to learn multi-scale multi-attribute entangled representations of controlling attributes. Because of the complex nature of the multi-attribute image generation problem, we regularize the image generation by predicting the explicit conditioning attributes (ingredients and view). To synthesize a pizza image with view attributes outside the range of natural training images, we design a CGI pizza dataset pizzaView using 3D pizza models and employ it to train a view attribute regressor to regularize the generation process, bridging the real and CGI training datasets. To verify the efficacy of MPG, we test it on Pizza10, a carefully annotated multi-ingredient pizza image dataset. MPG can successfully generate photorealistic pizza images with desired ingredients and view attributes, beyond the range of those observed in real-world training data. "
  - id: 1481
    order: 98
    poster_session: 1
    session_id: 2
    title: "From Seq2Seq Recognition to Handwritten Word Embeddings"
    authors:
      - author: "George Retsinas (National Technical University of Athens)"
      - author: "Giorgos Sfikas (University of Ioannina)"
      - author: "Christophoros Nikou (University of Ioannina)"
      - author: "Petros Maragos (National Technical University of Athens)"
    all_authors: "George Retsinas, Giorgos Sfikas, Christophoros Nikou and Petros Maragos"
    code: "https://github.com/georgeretsi/Seq2Emb"
    keywords:
      - word: "keyword spotting"
      - word: "handwritten text recognition"
      - word: "sequence-to-sequence"
    paper: "papers/1481.pdf"
    supp: "supp/1481_supp.zip"
    abstract: "In this work, we propose a system for automatically extracting handwritten word embeddings, using the encoding module of a Sequence-to-Sequence (Seq2Seq) recognition network. These embeddings are proven to be very discriminative, since they can be effectively used for Keyword Spotting, while they can also be fully decoded into the target string following the Seq2Seq rationale. Architecture-wise, the proposed system incorporates several novel modules (e.g. auto-encoder path or non-recurrent CTC-branch) that assist the training procedure and boost performance. Additionally, we also show how to further process these embeddings/representations with a binarization scheme to provide compact and highly efficient descriptors, suitable for Keyword Spotting. Numerical results validate the usefulness of the proposed architecture, as our method outperforms the previous state of the art in Keyword Spotting."
  - id: 1482
    order: 99
    poster_session: 1
    session_id: 2
    title: "Skeleton-aware Text Image Super-Resolution"
    authors:
      - author: "Shimon Nakaune (University of Tsukuba)"
      - author: "Satoshi Iizuka (University of Tsukuba)"
      - author: "Kazuhiro Fukui (University of Tsukuba)"
    all_authors: "Shimon Nakaune, Satoshi Iizuka and Kazuhiro Fukui"
    code: ""
    keywords:
      - word: "text super-resolution"
      - word: "convolutional neural network"
      - word: "loss function"
      - word: ""
    paper: "papers/1482.pdf"
    supp: ""
    abstract: "We present a novel structure-aware loss function for text image super-resolution to improve the recognition accuracy of text recognizers in natural scenes. Text image super-resolution is a particular case of general image super-resolution,
where our primary goal is to improve the readability of characters in a low-resolution image by increasing the resolution of the text image. In this scenario, general loss functions usually used in previous super-resolution models are insufficient to learn character shapes precisely and stably as it often leads to blurring and breaking of the shapes. In this paper, we propose a skeleton loss for training text super-resolution networks. Skeleton loss enables the networks to generate more readable characters by considering the detailed structural formation of character skeletons, in the optimization process. The key idea of the skeleton loss is to measure the differences between two types of character skeletons, where one is obtained from a high-resolution image and another is from the super-resolved image generated from a given low-resolution image. To implement this idea in an end-to-end form, we introduce a skeletonization network that can generate skeletons from an input text image. Quantitative analysis shows that our method outperforms existing super-resolution models with modern text recognizers in terms of recognition accuracy. Furthermore, our experiments show that our skeleton loss can boost generating readable text images of existing super-resolution networks without modifying their structures."
  - id: 1488
    order: 429
    poster_session: 4
    session_id: 11
    title: "OSS-Net: Memory Efficient High Resolution Semantic Segmentation of 3D Medical Data"
    authors:
      - author: "Christoph Reich (Technische Universität Darmstadt)"
      - author: "Tim Prangemeier ( Technische Universität Darmstadt)"
      - author: "Ozdemir Cetin (TU Darmstadt)"
      - author: "Heinz   Koeppl (TU Darmstadt)"
    all_authors: "Christoph Reich, Tim Prangemeier, Ozdemir Cetin and Heinz   Koeppl"
    code: "https://github.com/ChristophReich1996/OSS-Net"
    keywords:
      - word: "3d semantic segmentation"
      - word: "segmentation"
      - word: "3d vision"
      - word: "medical imaging"
      - word: "3d imaging"
      - word: "brain segmentation"
      - word: "liver segmentation"
      - word: "implicit representation"
    paper: "papers/1488.pdf"
    supp: ""
    abstract: "Convolutional neural networks (CNNs) are the current state-of-the-art meta-algorithm for volumetric segmentation of medical data, for example, to localize COVID-19 infected tissue on computer tomography scans or the detection of tumour volumes in magnetic resonance imaging. A key limitation of 3D CNNs on voxelised data is that the memory consumption grows cubically with the training data resolution. Occupancy networks (O-Nets) are an alternative for which the data is represented continuously in a function space and 3D shapes are learned as a continuous decision boundary. While O-Nets are significantly more memory efficient than 3D CNNs, they are limited to simple shapes, are relatively slow at inference, and have not yet been adapted for 3D semantic segmentation of medical data. Here, we propose Occupancy Networks for Semantic Segmentation (OSS-Nets) to accurately and memory-efficiently segment 3D medical data. We build upon the original O-Net with modifications for increased expressiveness leading to improved segmentation performance comparable to 3D CNNs, as well as modifications for faster inference. We leverage local observations to represent complex shapes and prior encoder predictions to expedite inference. We showcase OSS-Net's performance on 3D brain tumour and liver segmentation against a function space baseline (O-Net), a performance baseline (3D residual U-Net), and an efficiency baseline (2D residual U-Net). OSS-Net yields segmentation results similar to the performance baseline and superior to the function space and efficiency baselines. In terms of memory efficiency, OSS-Net consumes comparable amounts of memory as the function space baseline, somewhat more memory than the efficiency baseline and significantly less than the performance baseline. As such, OSS-Net enables memory-efficient and accurate 3D semantic segmentation that can scale to high resolutions."
  - id: 1497
    order: 116
    oral_session: 4
    poster_session: 2
    session_id: 4
    title: "MMD-ReID: A Simple but Effective Solution for Visible-Thermal Person ReID"
    authors:
      - author: "Chaitra S Jambigi (Indian Institute of Science)"
      - author: "Ruchit Rawal (Indian Institute of Science)"
      - author: "Anirban Chakraborty (Indian Institute of Science)"
    all_authors: "Chaitra S Jambigi, Ruchit Rawal and Anirban Chakraborty"
    code: "https://github.com/vcl-iisc/MMD-ReID"
    keywords:
      - word: "person reidentification"
      - word: "cross modal reid"
      - word: "visible thermal reid"
      - word: "multimodal"
      - word: "domain adaptation"
      - word: "representation learning"
      - word: "metric learning"
    paper: "papers/1497.pdf"
    supp: "supp/1497_supp.zip"
    abstract: "Learning modality invariant features is central to the problem of Visible-Thermal cross-modal Person Reidentification (VT-ReID), where query and gallery images come from different modalities. Existing works implicitly align the modalities in pixel and feature spaces by either using adversarial learning or carefully designing feature extraction modules that heavily rely on domain knowledge. We propose a simple but effective framework, MMD-ReID, that reduces the modality gap by an explicit discrepancy reduction constraint. MMD-ReID takes inspiration from Maximum Mean Discrepancy (MMD), a widely used statistical tool for hypothesis testing that determines the distance between two distributions. MMD-ReID uses a novel margin-based formulation to match class-conditional feature distributions of visible and thermal samples to minimize intra-class distances while maintaining feature discriminability. MMD-ReID is a simple framework in terms of architecture and loss formulation. We conduct extensive experiments to demonstrate both qualitatively and quantitatively the effectiveness of MMD-ReID in aligning the marginal and class conditional distributions, thus learning both modality-independent and identity-consistent features. The proposed framework significantly outperforms the state-of-the-art methods on SYSU-MM01 and RegDB datasets. Code will be released at https://github.com/vcl-iisc/MMD-ReID."
  - id: 1500
    order: 319
    poster_session: 3
    session_id: 8
    title: "Monocular Arbitrary Moving Object Discovery and Segmentation"
    authors:
      - author: "Michal Neoral (FEE CTU Prague)"
      - author: "Jan Sochman (Czech Technical University in Prague)"
      - author: "Jiri Matas (CMP CTU FEE)"
    all_authors: "Michal Neoral, Jan Sochman and Jiri Matas"
    code: "https://github.com/michalneoral/Raptor"
    keywords:
      - word: "motion segmentation"
      - word: "instance motion segmentation"
      - word: ""
    paper: "papers/1500.pdf"
    supp: ""
    abstract: "We propose a method for discovery and segmentation of objects that are, or their parts are, independently moving in the scene. Given three monocular video frames, the method outputs semantically meaningful regions, i.e. regions corresponding to the whole object,  even when only a part of it moves. 

The architecture of the CNN-based end-to-end method, called Raptor, combines semantic and motion backbones, which pass their outputs to a final region segmentation network. The semantic backbone is trained in a class-agnostic manner in order to generalise to object classes beyond the training data. The core of the motion branch is a geometrical cost volume computed from optical flow, optical expansion, mono-depth and the estimated camera motion.

Evaluation of the proposed architecture on the instance motion segmentation and binary moving-static segmentation problems on KITTI, DAVIS-Moving and YTVOS-Moving datasets shows that the proposed method achieves state-of-the-art results on all the datasets and is able to generalise well to various environments.
For the KITTI dataset, we provide an upgraded instance motion segmentation annotation which covers all moving objects. Dataset, code and models are available on the github project page github.com/michalneoral/Raptor."
  - id: 1502
    order: 430
    poster_session: 4
    session_id: 11
    title: "Pose-Transformation and Radial Distance Clustering for Unsupervised Person Re-identification"
    authors:
      - author: "Siddharth Seth (Indian Institute of Science)"
      - author: "Akash Sonth (Indian Institute of Science)"
      - author: "Anirban Chakraborty (Indian Institute of Science)"
    all_authors: "Siddharth Seth, Akash Sonth and Anirban Chakraborty"
    code: ""
    keywords:
      - word: "person re-id"
      - word: "unsupervised"
      - word: "biometric"
    paper: "papers/1502.pdf"
    supp: "supp/1502_supp.zip"
    abstract: "Person re-identification (re-ID) aims to tackle the problem of matching identities across non-overlapping cameras. Supervised approaches require identity information that may be difficult to obtain and are inherently biased towards the dataset they are trained on, making them unscalable across domains. To overcome these challenges, we propose an unsupervised approach to the person re-ID setup. Having zero knowledge of true labels, our proposed method enhances the discriminating ability of the learned features via a novel two-stage training strategy. The first stage involves training a deep network on an expertly designed pose transformed dataset obtained by generating multiple perturbations for each original image in the pose space. Next, the network learns to map similar features closer in the feature space using the proposed discriminative clustering algorithm. We introduce a novel radial distance loss, that attends to the fundamental aspects of feature learning - compact clusters with low intra-cluster and high inter-cluster variation. Extensive experiments on several large-scale re-ID datasets demonstrate the superiority of our method compared to state-of-the-art approaches."
  - id: 1506
    order: 118
    oral_session: 4
    poster_session: 2
    session_id: 4
    title: "Grid Cell Path Integration For Movement-Based Visual Object Recognition"
    authors:
      - author: "Niels Leadholm (University of Oxford)"
      - author: "Marcus Lewis (Numenta)"
      - author: "Subutai Ahmad (Numenta)"
    all_authors: "Niels Leadholm, Marcus Lewis and Subutai Ahmad"
    code: "https://github.com/numenta/htmpapers"
    keywords:
      - word: "biologically plausible"
      - word: "translation invariance"
      - word: "robustness"
      - word: "sequential vision"
      - word: "transsaccadic vision"
      - word: "grid cells"
      - word: "path integration"
      - word: "continual learning"
      - word: "predictive representations"
      - word: "Hebbian learning"
    paper: "papers/1506.pdf"
    supp: "supp/1506_supp.zip"
    abstract: "Grid cells enable the brain to model the physical space of the world and navigate effectively via path integration, updating self-position using information from self-movement. Recent proposals suggest that the brain might use similar mechanisms to understand the structure of objects in diverse sensory modalities, including vision. In machine vision, object recognition given a sequence of sensory samples of an image, such as saccades, is a challenging problem when the sequence does not follow a consistent, fixed pattern - yet this is something humans do naturally and effortlessly. We explore how grid cell-based path integration in a cortical network can support reliable recognition of objects given an arbitrary sequence of inputs. Our network (GridCellNet) uses grid cell computations to integrate visual information and make predictions based on movements. We use local Hebbian plasticity rules to learn rapidly from a handful of examples (few-shot learning), and consider the task of recognizing MNIST digits given a sequence of image feature patches. Extending beyond the current literature, we show that GridCellNet can reliably perform classification, generalizing to both unseen examples and completely novel sequence trajectories. Furthermore, by utilizing grid cells for an internal reference frame derived from sensory inputs and internal motor information alone, the classification process represents an important step towards enabling translation invariance in sequential classifiers. In addition, we demonstrate that GridCellNet is able to predict unseen regions of the image, that inference can be successful after sampling a fraction of the input space, and that a natural benefit of the proposed architecture is robustness in the context of continual learning. We propose that agents with active sensors can use grid cell representations not only for navigation, but also for robust and efficient visual understanding."
  - id: 1514
    order: 213
    poster_session: 2
    session_id: 5
    title: "Stylistic Multi-Task Analysis of Ukiyo-e Woodblock Prints"
    authors:
      - author: "Selina J. Khan (University of Amsterdam)"
      - author: "Nanne van Noord (University of Amsterdam)"
    all_authors: "Selina J. Khan and Nanne van Noord"
    code: "https://github.com/selinakhan/stylistic-MTL-ukiyoe"
    keywords:
      - word: "multi-task learning"
      - word: "transfer learning"
      - word: "vision transformers"
      - word: "artwork dataset"
      - word: "japanese art"
    paper: "papers/1514.pdf"
    supp: "supp/1514_supp.zip"
    abstract: "In this work we present a large-scale dataset of Ukiyo-e woodblock prints. Unlike previous works and datasets in the artistic domain that primarily focus on western art, this paper explores this pre-modern Japanese art form with the aim of broadening the scope for stylistic analysis and to provide a benchmark to evaluate a variety of art focused Computer Vision approaches. Our dataset consists of over 175.000 prints with corresponding metadata (eg. the artist, era, and creation date) ranging from the 17th century to present day. By approaching stylistic analysis as a Multi-Task learning problem we aim to more efficiently utilize the available metadata, and learn more general representations of style. We show results for a variety of well-known and reliable baselines to enable future comparison, and to encourage stylistic analysis on this artistic domain."
  - id: 1518
    order: 320
    poster_session: 3
    session_id: 8
    title: "An attention-driven hierarchical multi-scale representation for visual recognition"
    authors:
      - author: "Zachary Wharton (Edge Hill University)"
      - author: "Ardhendu Behera (Edge Hill University)"
      - author: "Asish Bera (Edge Hill University)"
    all_authors: "Zachary Wharton, Ardhendu Behera and Asish Bera"
    code: ""
    keywords:
      - word: "Hierarchical multiscale regions/patches"
      - word: "fine-grained visual classification"
      - word: "graph convolutional network"
      - word: "visual-spatial structural relationships"
      - word: "structure-driven message propagation"
      - word: "graph pooling"
      - word: "gated attention"
      - word: "graph-level prediction"
      - word: ""
    paper: "papers/1518.pdf"
    supp: "supp/1518_supp.zip"
    abstract: "Convolutional Neural Networks (CNNs) have revolutionized the understanding of visual content. This is mainly due to their ability to break down an image into smaller pieces, extract multi-scale localized features and compose them to construct highly expressive representations for decision making. However, the convolution operation is unable to capture long-range dependencies such as arbitrary relations between pixels since it operates on a fixed-size window. Therefore, it may not be suitable for discriminating subtle changes (e.g. fine-grained visual recognition). To this end, our proposed method captures the high-level long-range dependencies by exploring Graph Convolutional Networks (GCNs), which aggregate information by establishing relationships among multi-scale hierarchical regions. These regions consist of smaller (closer look) to larger (far look), and the dependency between regions is modeled by an innovative attention-driven message propagation, guided by the graph structure to emphasize the neighborhoods of a given region. Our approach is simple yet extremely effective in solving fine-grained and generic visual classification problems. It outperforms the state-of-the-art with a significant margin on three and is very competitive on another two datasets."
  - id: 1522
    order: 214
    poster_session: 2
    session_id: 5
    title: "Using Synthetic Corruptions to Measure Robustness to Natural Distribution Shifts"
    authors:
      - author: "Alfred LAUGROS (Atos)"
      - author: "Alice Caplier (Université Grenoble Alpes)"
      - author: "Matthieu Ospici (Atos)"
    all_authors: "Alfred LAUGROS, Alice Caplier and Matthieu Ospici"
    code: "https://github.com/bds-ailab/common_corruption_benchmark"
    keywords:
      - word: "Robustness"
      - word: "Corruptions"
      - word: "Benchmark"
      - word: "Object Recognition"
    paper: "papers/1522.pdf"
    supp: "supp/1522_supp.zip"
    abstract: "Synthetic corruptions gathered into a benchmark are frequently used to measure neural network robustness to distribution shifts. However, robustness to synthetic corruption benchmarks is not always predictive of robustness to distribution shifts encountered in real-world applications. In this paper, we propose a methodology to build synthetic corruption benchmarks that make robustness estimations more correlated with robustness to real-world distribution shifts. Using the overlapping criterion, we split synthetic corruptions into categories that help to better understand neural network robustness. Based on these categories, we identify  three relevant parameters to take into account when constructing a corruption benchmark that are the (1) number of represented categories, (2) their relative balance in terms of size and, (3) the size of the considered benchmark. In doing so, we build new synthetic corruption selections that are more predictive of robustness to natural corruptions than existing synthetic corruption benchmarks."
  - id: 1528
    order: 100
    poster_session: 1
    session_id: 2
    title: "Adaptive Distillation: Aggregating Knowledge from Multiple Paths for Efficient Distillation"
    authors:
      - author: "Sumanth Chennupati (Wyze Labs Inc)"
      - author: "Mohammad Mahdi Kamani (Wyze Labs)"
      - author: "Zhongwei Cheng (Wyze Labs)"
      - author: "Lin Chen (Wyze Labs Inc.)"
    all_authors: "Sumanth Chennupati, Mohammad Mahdi Kamani, Zhongwei Cheng and Lin Chen"
    code: ""
    keywords:
      - word: "Knowledge Distillation"
      - word: "Multitask Learning"
      - word: "Model Compression"
      - word: "Adaptive Distillation"
      - word: "Efficient Training"
    paper: "papers/1528.pdf"
    supp: "supp/1528_supp.zip"
    abstract: "Knowledge Distillation is becoming one of the primary trends among neural network compression algorithms to improve the generalization performance of a smaller student model with guidance from a larger teacher model. This momentous rise in applications of knowledge distillation is accompanied by the introduction of numerous algorithms for distilling the knowledge such as soft targets and hint layers. Despite this advancement in different techniques for distilling the knowledge, the aggregation of different paths for distillation has not been studied comprehensively. This is of particular significance, not only because different paths have different importance, but also due to the fact that some paths might have negative effects on the generalization performance of the student model. Hence, we need to adaptively adjust the importance of each path to maximize the impact of distillation on the student model. In this paper, we explore different approaches for aggregating these different paths and introduce our proposed adaptive approach based on multitask learning methods. We empirically demonstrate the effectiveness of our proposed approach over other baselines in the applications of knowledge distillation for classification, semantic segmentation, and object detection tasks."
  - id: 1529
    order: 101
    poster_session: 1
    session_id: 2
    title: "1529Sparse Adversarial Video Attacks with Spatial Transformations"
    authors:
      - author: "Ronghui Mu (Lancaster University)"
      - author: "Wenjie Ruan (University of Exeter)"
      - author: "Leandro Soriano Marcolino (Lancaster University)"
      - author: "Qiang Ni (Lancaster University)"
    all_authors: "Ronghui Mu, Wenjie Ruan, Leandro Soriano Marcolino and Qiang Ni"
    code: "https://github.com/anonymous-221/SAVA.git"
    keywords:
      - word: "videos adversarial attack"
      - word: "Spatial Transformations"
      - word: "SSIM"
      - word: "Perceptually Constrained"
      - word: "video recognition"
      - word: "Bayesian optimizations"
    paper: "papers/1529.pdf"
    supp: "supp/1529_supp.zip"
    abstract: "In recent years, a significant amount of research efforts concentrated on adversarial attacks on images, while adversarial video attacks have seldom been explored. We propose an adversarial attack strategy on videos, called DeepSAVA.  Our model includes both additive perturbation and spatial transformation by a unified optimisation framework, where the structural similarity index measure is adopted to measure the adversarial distance. We design an effective and novel optimisation scheme which alternatively utilizes Bayesian optimisation to identify the most influential frame in a video and Stochastic gradient descent (SGD) based optimisation to produce both additive and spatial-transformed perturbations. Doing so enables DeepSAVA to perform a very sparse attack on videos for maintaining human imperceptibility while still achieving state-of-the-art performance in terms of both attack success rate and adversarial transferability. Our intensive experiments on various types of deep neural networks and video datasets confirm the superiority of DeepSAVA."
  - id: 1536
    order: 321
    poster_session: 3
    session_id: 8
    title: "ERA: Entity–relationship Aware Video Summarization with Wasserstein GAN"
    authors:
      - author: "Guande Wu (New York University)"
      - author: "jianzhe peter lin (University of British Columbia)"
      - author: "Claudio Silva (NYU)"
    all_authors: "Guande Wu, Jianzhe Peter Lin and Claudio Silva"
    code: "https://github.com/jnzs1836/ERA-VSum"
    keywords:
      - word: "video summarization"
      - word: "spatio-temporal graph neural network"
    paper: "papers/1536.pdf"
    supp: "supp/1536_supp.zip"
    abstract: "Video summarization aims to simplify large-scale video browsing by generating concise, short summaries that diver from but well represent the original video. Due to the scarcity of video annotations, recent progress for video summarization concentrates on unsupervised methods, among which the GAN-based methods are most prevalent. This type of methods includes a summarizer and a discriminator. The summarized video from the summarizer will be assumed as the final output, only if the video reconstructed from this summary cannot be discriminated from the original one by the discriminator. The primary problems of this GAN-based methods are two-folds. First, the summarized video in this way is a subset of original video with low redundancy and contains high priority events/entities. This summarization criterion is not enough. Second, the training of the GAN framework is not stable. This paper proposes a novel Entity–relationship Aware video summarization method (ERA) to address the above problems. To be more specific, we introduce a Adversarial Spatio-Temporal network to construct the relationship among entities, which we think should also be given high priority in the summarization. The GAN training problem is solved by introducing the Wasserstein GAN and two newly proposed video-patch/score-sum losses. In addition, the score-sum loss can also relieve the model sensitivity to the varying video lengths, which is an inherent problem for most current video analysis tasks. Our method substantially lifts the performance on the target benchmark datasets and exceeds the current state-of-the-art. We hope our straightforward yet effective approach will shed some light on the future research of unsupervised video summarization. Code is available online."
  - id: 1542
    order: 322
    poster_session: 3
    session_id: 8
    title: "Enhancing Human Motion Assessment by Self-supervised Representation Learning"
    authors:
      - author: "Mahdiar Nekoui (University of Alberta)"
      - author: "Li Cheng (ECE dept., University of Alberta)"
    all_authors: "Mahdiar Nekoui and Li Cheng"
    code: ""
    keywords:
      - word: "action quality assessment"
      - word: "self-supervised representation learning"
      - word: "pose estimation"
      - word: "tele-rehabilitation"
    paper: "papers/1542.pdf"
    supp: ""
    abstract: "The space of human motions is vast, ranging from daily behaviors of healthy adults to the slow and stiff motions of Parkinson's patients, or to infant motions. This poses significant challenges when the task is focused on a relatively niche motion subspace such as physical rehabilitation: often the target datasets are limited and less-annotated; meanwhile, there exist large-scale, well-annotated benchmarks, typically consisting of daily activities from healthy adults. This observation inspires us to propose a two-stage pipeline that takes advantage of the best of both worlds: a non-expert network starts to learn the representation of normal motions from source datasets, by estimating the pace and a set of manually inpainted joints of the pose sequence; this is followed by an expert network that takes as input these representations as well as the appearance features of the dedicated motions from the target dataset, to assess the quality of the specific actions. Empirical experiments on two very different motion assessment applications (physical rehabilitation of Parkinson's & stroke patients, and neuromotor behaviors of infants) demonstrate the superior performance of our approach."
  - id: 1544
    order: 102
    poster_session: 1
    session_id: 2
    title: "Corrosion Image Data Set for Automating Scientific Assessment of Materials"
    authors:
      - author: "Biao Yin (Worcester Polytechnic Institute)"
      - author: "Nicholas  Josselyn (Worcester Polytechnic Institute)"
      - author: "Thomas Considine (US Army Research Laboratory)"
      - author: "John Kelley (US Army Research Laboratory)"
      - author: "Berend Rinderspacher (US Army Research Laboratory)"
      - author: "Robert Jensen (ARL Northeast Regional Extended Site)"
      - author: "James F Snyder (DEVCOM Army Research Laboratory)"
      - author: "Ziming Zhang (Worcester Polytechnic Institute)"
      - author: "Elke A Rundensteiner (WPI)"
    all_authors: "Biao Yin, Nicholas  Josselyn, Thomas Considine, John Kelley, Berend Rinderspacher, Robert Jensen, James F Snyder, Ziming Zhang and Elke A Rundensteiner"
    code: "https://arl.wpi.edu"
    keywords:
      - word: "corrosion assessment"
      - word: "small image dataset"
      - word: "data augmentation"
      - word: "self-supervised learning"
      - word: "material science"
      - word: "image classification"
    paper: "papers/1544.pdf"
    supp: "supp/1544_supp.zip"
    abstract: "The study of material corrosion is an important research area, with corrosion degradation of metallic structures causing expenses up to 4% of the global domestic product annually along with major safety risks worldwide. Unfortunately, large-scale and timely scientific discovery of materials has been hindered by the lack of standardized corrosion experimental data in the public domain for developing machine learning models. Obtaining such data is challenging due to the expert knowledge and time required to conduct these scientific experiments and assess corrosion levels. We curate a novel dataset consisting of 600 images annotated with expert corrosion ratings obtained over 10 years of laboratory corrosion testing by material scientists. Based on this data set, we find that non-experts even when rigorously trained with domain guidelines to rate corrosion fail to match expert ratings. Challenges include limited data, image artifacts, and millimeter-precision corrosion. This motivates us to explore the viability of deep learning approaches to tackle this benchmark classification task. We study (i) convolutional neural networks powered with rich domain-specific image augmentation techniques tuned to our data, and (ii) a recent self-supervised representation learning approach either pretrained on ImageNet or trained on our data. We demonstrate that pretrained ResNet-18 and HR-Net models with tuned augmentations can reach up to 0.83 accuracy. With this corrosion data set, we open the door for the design of more advanced deep learning models to support this real-world task, while driving innovative new research to bridge computer vision and material innovation. Our data and code are available at: https://arl.wpi.edu"
  - id: 1545
    order: 431
    poster_session: 4
    session_id: 11
    title: "AGCN: Adversarial Graph Convolutional Network for 3D Point Cloud Segmentation"
    authors:
      - author: "Seunghoi Kim (University College London)"
      - author: "Daniel Alexander (University College London)"
    all_authors: "Seunghoi Kim and Daniel Alexander"
    code: ""
    keywords:
      - word: "point cloud"
      - word: "semantic segmentation"
      - word: "3D computer vision"
      - word: "generative adversarial networks"
      - word: ""
    paper: "papers/1545.pdf"
    supp: ""
    abstract: "3D point cloud segmentation provides a high-level semantic understanding of object structure that is valuable in applications such as medicine, robotics and self-driving. In this paper, we propose an Adversarial Graph Convolutional Network for 3D point cloud segmentation. Many current networks encounter problems such as low segmentation accuracy and high complexities due to their crude network architectures and local feature aggregation methods. To overcome these problems, we propose a) a graph convolutional network (GCN) in an adversarial learning scheme where a discriminator network provides a segmentation network with informative information to improve segmentation accuracy and b) a graph convolution, GeoEdgeConv, as a means of local feature aggregation to improve segmentation accuracy and space and time complexities. By using an embedding L2 loss as an adversarial loss, the proposed network is learned to reduce noisy labels by enforcing the consistency between neighbouring labels. Preserving geometric structures over convolution layers by using both point and relative position features, GeoEdgeConv helps learn fine details of complex structures, and thus improves segmentation accuracy in boundaries and reduces label noise inside a class without increased computational complexity. Experiments on ShapeNet Part demonstrate that our model outperforms the state-of-the-art (SOTA) with lower complexity and it has strong prospects in applications requiring low power but high segmentation performance. "
  - id: 1546
    order: 215
    poster_session: 2
    session_id: 5
    title: "Introducing the Boundary-Aware loss for deep image segmentation"
    authors:
      - author: "Minh ON VU NGOC (EPITA Research and Development Laboratory (LRDE)	)"
      - author: "Yizi CHEN (Institut national de l'information géographique et forestière (IGN))"
      - author: "Nicolas C. BOUTRY (EPITA Research and Development Laboratory (LRDE))"
      - author: "Joseph Chazalon (EPITA Research and Development Laboratory (LRDE))"
      - author: "Edwin Carlinet (LRDE)"
      - author: "Clement Mallet (IGN, France)"
      - author: "Thierry GERAUD (LRDE)"
    all_authors: "Minh ON VU NGOC, Yizi CHEN, Nicolas C. BOUTRY, Joseph Chazalon, Edwin Carlinet, Clement Mallet and Thierry GERAUD"
    code: "https://github.com/onvungocminh/MBD_BAL"
    keywords:
      - word: "Boundary-Aware loss Deep image segmentation Minimum Barrier Distance Topology preservation Electron Microscopy"
    paper: "papers/1546.pdf"
    supp: "supp/1546_supp.zip"
    abstract: "Most contemporary supervised image segmentations do not preserve the initial topology of the given input (like the closeness of the contours). One can generally remark that edge points have been inserted or removed when the binary prediction and the ground truth are compared. This can be critical when accurate localization of multiple interconnected objects is required. In this paper, we present a new loss function, called, Boundary-Aware loss (BAL), based on the Minimum Barrier Distance (MBD) cut algorithm. It is able to locate what we call the leakage pixels and to encode the boundary information coming from the given ground truth. Thanks to this adapted loss, we are able to significantly refine the quality of the predicted boundaries during the learning procedure. Furthermore, our loss function is differentiable and can be applied to any kind of neural network used in image processing. We apply this loss function on the standard U-Net and DC U-Net on Electron Microscopy datasets. They are well-known to be challenging due to their high noise level and to the close or even connected objects covering the image space. Our segmentation performance, in terms of Variation of Information (VOI) and Adapted Rank Index (ARI), are very promising and lead to ∼15% better scores of VOI and ∼5% better scores of ARI than the state-of-the-art. The code of boundary-awareness loss is freely available at https://github.com/onvungocminh/MBD_BAL"
  - id: 1550
    order: 103
    poster_session: 1
    session_id: 2
    title: "Unsupervised computation of salient motion maps from the interpretation of a frame-based classification network"
    authors:
      - author: "Etienne Meunier (Inria, Centre Rennes)"
      - author: "Patrick Bouthemy (INRIA)"
    all_authors: "Etienne Meunier and Patrick Bouthemy"
    code: ""
    keywords:
      - word: "Motion saliency"
      - word: "motion segmentation"
      - word: "interpretation neural network"
      - word: "LRP"
    paper: "papers/1550.pdf"
    supp: "supp/1550_supp.zip"
    abstract: "We introduce a new paradigm for motion saliency (MS) which is an important issue in dynamic scene analysis. We formulate MS as a meta-task that can be instantiated for different tasks usually handled independently. To support this claim, we have addressed two important computer-vision problems with this MS paradigm: independent motion segmentation and anomalous motion detection in videos. We estimate MS from the interpretation of a frame-based saliency classification network with optical flow (OF) as input. Our paradigm can accommodate a given form of motion saliency by simply training the frame-based classification network on the corresponding task. Moreover, our MS estimation is unsupervised, as it does not require any ground-truth saliency maps for training. In addition, we have designed an original two-step network interpretation method, which supplies the binary salient motion segmentation. Finally, we recover the valued motion saliency map using a parametric flow inpainting method."
  - id: 1551
    order: 432
    poster_session: 4
    session_id: 11
    title: "SwinFGHash: Fine-grained Image Retrieval via Transformer-based Hashing Network"
    authors:
      - author: "Di Lu (Tsinghua University)"
      - author: "Jinpeng Wang (Tsinghua University)"
      - author: "Ziyun Zeng (Tsinghua University)"
      - author: "Bin Chen (Harbin Institute of Technology, Shenzhen)"
      - author: "Shudeng Wu (Tsinghua University)"
      - author: "Shu-Tao Xia (Tsinghua University)"
    all_authors: "Di Lu, Jinpeng Wang, Ziyun Zeng, Bin Chen, Shudeng Wu and Shu-Tao Xia"
    code: ""
    keywords:
      - word: "Image Retrieval"
      - word: "Deep Hashing"
      - word: "Fine-grained"
      - word: "Transformer"
    paper: "papers/1551.pdf"
    supp: "supp/1551_supp.zip"
    abstract: "Fine-grained image retrieval is a fundamental and challenging problem in computer vision due to the intra-class diversities and inter-class confusions. Existing hashing-based approaches employed convolutional neural networks (CNNs) to learn hash codes for fast fine-grained image retrieval, which are limited by the inherent locality constrain of the convolution operations and yield sub-optimal performance. Recently, transformers have shown colossal potential on vision tasks for their excellent capacity to capture long-range visual dependencies. Therefore, in this paper, we take the first step to exploit the vision transformer-based hashing network for fine-grained image retrieval. We propose the SwinFGHash, which takes advantage of transformer-based architecture to model the feature interactions among the spatially distant areas, e.g., the head and the tail of a bird on an image, thus improving the fine-grained discrimination of the generated hash codes. Besides, we enhance the critical region localization ability of SwinFGHash by designing a Global with Local (GwL) feature learning module, which preserves subtle yet discriminative features for fine-grained retrieval. Extensive experiments on benchmark datasets show that our SwinFGHash significantly outperforms existing state-of-the-art baselines in fine-grained image retrieval."
  - id: 1564
    order: 104
    poster_session: 1
    session_id: 2
    title: "Robust channel-wise illumination estimation "
    authors:
      - author: "Firas Laakom (Tampere University)"
      - author: "Jenni Raitoharju (Tampere University)"
      - author: "Jarno Nikkanen (Xiaomi Technology)"
      - author: "Alexandros Iosifidis (Aarhus University)"
      - author: "Moncef Gabbouj (Tampere University)"
    all_authors: "Firas Laakom, Jenni Raitoharju, Jarno Nikkanen, Alexandros Iosifidis and Moncef Gabbouj"
    code: ""
    keywords:
      - word: "color constancy"
      - word: "illumination estimation"
      - word: "deep Learning"
      - word: "uncertrainty estimation"
      - word: "regression"
    paper: "papers/1564.pdf"
    supp: ""
    abstract: "Recently, Convolutional Neural Networks (CNNs) have been widely used to solve the illumination estimation problem and have often led to state-of-the-art results. Standard approaches operate directly on the input image. In this paper, we argue that this problem can be decomposed into three channel-wise independent and symmetric sub-problems and propose a novel CNN-based illumination estimation approach based on this decomposition. The proposed method substantially reduces the number of parameters needed  to solve the task while achieving competitive experimental results compared to state-of-the-art methods. Furthermore, the practical application of illumination estimation techniques typically requires the identification of extreme error cases. This can be achieved using an uncertainty estimation technique. In this work, we propose a novel color constancy uncertainty estimation by augmenting the trained model with an auxiliary branch which learns to predict the error based on the feature representation. Intuitively, the model learns which feature combinations are robust and are thus likely to yield low errors and which combinations result in erroneous estimates. We test this approach on the proposed method and show that it can indeed be used to avoid several extreme error cases and, thus, improves the practicality of the proposed technique."
  - id: 1568
    order: 323
    poster_session: 3
    session_id: 8
    title: "Livestock Monitoring with Transformer"
    authors:
      - author: "Bhavesh Tangirala (IIT (ISM) Dhanbad)"
      - author: "Ishan  Bhandari (Indian Institute of Technology (ISM) Dhanbad )"
      - author: "Daniel Laszlo (Serket)"
      - author: "Deepak K Gupta (University of Amsterdam)"
      - author: "Rajat Thomas (Amsterdam University Medical Center)"
      - author: "Devanshu Arya (University of Amsterdam)"
    all_authors: "Bhavesh Tangirala, Ishan  Bhandari, Daniel Laszlo, Deepak K Gupta, Rajat Thomas and Devanshu Arya"
    code: "https://github.com/serket-tech/starformer"
    keywords:
      - word: "multi-object tracking"
      - word: "livestock monitoring"
      - word: "transformers"
    paper: "papers/1568.pdf"
    supp: "supp/1568_supp.zip"
    abstract: "Tracking the behaviour of livestock enables early detection and thus, prevention of contagious diseases in modern animal farms. Apart from economic gains, this would reduce the amount of antibiotics used in livestock farming which otherwise enters the human diet exasperating the epidemic of antibiotic resistance - a leading cause of death. We could use standard video cameras, available in most modern farms, to monitor livestock. However, most computer vision algorithms perform poorly on this task, primarily because, (i) animals bred in farms look identical, lacking any obvious spatial signature, (ii) none of the existing trackers are robust for long duration, and (iii) real-world conditions such as changing illumination, frequent occlusion, varying camera angles, and sizes of the animals make it hard for models to generalize. Given these challenges, we develop an end-to-end behaviour monitoring system for group-housed pigs to perform simultaneous instance level segmentation, tracking, action recognition and re-identification (STAR) tasks. We present starformer, the first end-to-end multiple-object livestock monitoring framework that learns instance-level embeddings for grouped pigs through the use of transformer architecture. For benchmarking, we present Pigtrace, a carefully curated dataset comprising video sequences with instance level bounding box, segmentation, tracking and activity classification of pigs in real indoor farming environment. Using simultaneous optimization on STAR tasks we show that starformer outperforms popular baseline models trained for individual tasks.
"
  - id: 1571
    order: 324
    poster_session: 3
    session_id: 8
    title: "CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search"
    authors:
      - author: "Seyed Mojtaba Marvasti-Zadeh (University of Alberta)"
      - author: "Javad Khaghani (University of Alberta)"
      - author: "Li Cheng (ECE dept., University of Alberta)"
      - author: "Hossein  Ghanei-Yakhdan (Yazd University)"
      - author: "Shohreh Kasaei (Sharif University of Technology)"
    all_authors: "Seyed Mojtaba Marvasti-Zadeh, Javad Khaghani, Li Cheng, Hossein  Ghanei-Yakhdan and Shohreh Kasaei"
    code: "https://github.com/VisualTrackingVLL"
    keywords:
      - word: "visual object tracking"
      - word: "neural architecture search"
      - word: "Siamese neural networks"
      - word: "tracking-by-detection"
      - word: "early-stopping"
      - word: ""
    paper: "papers/1571.pdf"
    supp: "supp/1571_supp.zip"
    abstract: "A strong visual object tracker nowadays relies on its well-crafted modules, which typically consist of manually-designed network architectures to deliver high-quality tracking results. Not surprisingly, the manual design process becomes a particularly challenging barrier, as it demands sufficient prior experience, enormous effort, intuition, and perhaps some good luck. Meanwhile, neural architecture search has gaining grounds in practical applications as a promising method in tackling the issue of automated search of feasible network structures.  In this work, we propose a novel cell-level differentiable architecture search mechanism with early stopping to automate the network design of the tracking module, aiming to adapt backbone features to the objective of Siamese tracking networks during offline training. Besides, the proposed early stopping strategy avoids over-fitting and performance collapse problems leading to generalization improvement. The proposed approach is simple, efficient, and with no need to stack a series of modules to construct a network. Our approach is easy to be incorporated into existing trackers, which is empirically validated using different differentiable architecture search-based methods and tracking objectives. Extensive experimental evaluations demonstrate the superior performance of our approach over five commonly-used benchmarks."
  - id: 1577
    order: 3
    oral_session: 1
    poster_session: 1
    session_id: 0
    title: "Deep Least Squares Alignment for Unsupervised Domain Adaptation"
    authors:
      - author: "Youshan Zhang (Lehigh University)"
      - author: "Brian D. Davison (Lehigh University)"
    all_authors: "Youshan Zhang and Brian D. Davison"
    code: "https://github.com/YoushanZhang/Transfer-Learning/tree/main/Code/Deep/DLSA"
    keywords:
      - word: "Unsupervised Domain Adaptation"
      - word: "Least Squares"
      - word: "Distribution Alignment"
    paper: "papers/1577.pdf"
    supp: "supp/1577_supp.zip"
    abstract: "Unsupervised domain adaptation leverages rich information from a labeled source domain to model an unlabeled target domain. Existing methods attempt to align the cross-domain distributions. However, the statistical representations of the alignment of the two domains are not well addressed.  In this paper, we propose deep least squares alignment (DLSA) to estimate the distribution of the two domains in a latent space by parameterizing a linear model. We further develop marginal and conditional adaptation loss to reduce the domain discrepancy by minimizing the angle between fitting lines and intercept differences and further learning domain invariant features. Extensive experiments demonstrate that the proposed DLSA model is effective in aligning domain distributions and outperforms state-of-the-art methods."
  - id: 1594
    order: 105
    poster_session: 1
    session_id: 2
    title: "Perception Visualization: Seeing Through The Eyes Of a DNN"
    authors:
      - author: "Loris Giulivi (Politecnico Di Milano)"
      - author: "Mark Carman (Politecnico di Milano)"
      - author: "Giacomo   Boracchi (Politecnico di Milano)"
    all_authors: "Loris Giulivi, Mark Carman and Giacomo   Boracchi"
    code: "https://github.com/loris2222/PerceptionVisualization"
    keywords:
      - word: "explainable artificial intelligence"
      - word: "saliency maps"
      - word: "convolutional neural networks"
      - word: "latent representations"
    paper: "papers/1594.pdf"
    supp: "supp/1594_supp.zip"
    abstract: "Artificial intelligence (AI) systems power the world we live in. Deep neural networks (DNNs) are able to solve tasks in an ever-expanding landscape of scenarios, but our eagerness to apply these powerful models leads us to focus on their performance and deprioritizes our ability to understand them. Current research in the field of explainable AI tries to bridge this gap by developing various perturbation or gradient-based explanation techniques. For images, these techniques fail to fully capture and convey the semantic information needed to elucidate why the model makes the predictions it does.
In this work, we develop a new form of explanation that is radically different in nature from current explanation methods, such as Grad-CAM. Perception visualization provides a visual representation of what the DNN perceives in the input image by depicting what visual patterns the latent representation corresponds to. This visualization is learnt by training a separate explanation model to invert the original model's latent representation without modifying its parameters. Results of our user study demonstrate that humans can better understand and predict the system's decisions when perception visualizations are available, thus easing the debugging and deployment of deep models as trusted systems."
  - id: 1595
    order: 216
    poster_session: 2
    session_id: 5
    title: "Learning to ignore: rethinking attention in CNNs"
    authors:
      - author: "Firas Laakom (Tampere University)"
      - author: "Kateryna Chumachenko (Tampere University)"
      - author: "Jenni Raitoharju (Tampere University)"
      - author: "Alexandros Iosifidis (Aarhus University)"
      - author: "Moncef Gabbouj (Tampere University)"
    all_authors: "Firas Laakom, Kateryna Chumachenko, Jenni Raitoharju, Alexandros Iosifidis and Moncef Gabbouj"
    code: ""
    keywords:
      - word: "attention"
      - word: "cnn"
      - word: "squeeze-excitation"
      - word: "cbam"
    paper: "papers/1595.pdf"
    supp: "supp/1595_supp.zip"
    abstract: "Recently, there has been an increased interest in applying attention mechanisms in Convolutional Neural Networks to solve computer vision tasks. Most of these methods learn to explicitly identify and highlight relevant parts of the scene and pass the attended image to further layers of the network. In this paper, we argue that such an approach might not be optimal. Arguably, explicitly learning which parts of the image are relevant is typically harder than learning which parts of the image are less relevant and should thus be ignored.  In fact, in vision domain, there are many easy-to-identify patterns of irrelevant features. For example, image regions close to the borders are less likely to contain useful information for a classification task, and should thus be ignored. Based on this idea, we propose to reformulate the attention mechanism in CNNs to learn to ignore instead of learning to attend. Specifically, we propose to explicitly learn irrelevant information in the scene and suppress it in the produced representation, keeping only important attributes. This implicit attention scheme can be incorporated into any existing attention mechanism. In this work, we validate this idea using two recent attention methods squeeze and excitation (SE) block and convolutional block attention module (CBAM). Experimental results on different datasets and model architectures show that learning to ignore, i.e., implicit attention, yields superior performance compared to the standard approaches."
  - id: 1603
    order: 106
    poster_session: 1
    session_id: 2
    title: "Mitigating Reverse Engineering Attacks on Local Feature Descriptors"
    authors:
      - author: "Deeksha Dangwal (University of California, Santa Barbar)"
      - author: "Vincent T Lee (Facebook)"
      - author: "Hyo Jin Kim (Facebook)"
      - author: "Tianwei Shen (Faceboook)"
      - author: "Meghan Cowan (University of Washington)"
      - author: "Rajvi Shah (IIIT Hyderabad)"
      - author: "Caroline Trippel (Stanford)"
      - author: "Brandon Reagen (New York University)"
      - author: "Timothy Sherwood (UC Santa Barbara)"
      - author: "Vasileios Balntas (Facebook Reality Labs)"
      - author: "Armin Alaghi (Facebook)"
      - author: "Eddy Ilg (Facebook Reality Labs)"
    all_authors: "Deeksha Dangwal, Vincent T Lee, Hyo Jin Kim, Tianwei Shen, Meghan Cowan, Rajvi Shah, Caroline Trippel, Brandon Reagen, Timothy Sherwood, Vasileios Balntas, Armin Alaghi and Eddy Ilg"
    code: ""
    keywords:
      - word: "localization"
      - word: "reverse-engineering"
      - word: "local descriptors"
      - word: "privacy-preserving mitigation"
    paper: "papers/1603.pdf"
    supp: "supp/1603_supp.zip"
    abstract: "As autonomous driving and augmented reality evolve a practical concern is data privacy, notably when these applications rely on user image-based localization. The widely adopted technology uses local feature descriptors derived from the images. While it was long thought that they could not be reverted back, recent work has demonstrated that under certain conditions reverse engineering attacks are possible and allow an adversary
to reconstruct RGB user images. This poses a potential risk to user privacy.
We take this further and model potential adversaries using a privacy threat model. We show a reverse engineering attack on sparse feature maps under controlled conditions and analyze the vulnerability of popular descriptors including FREAK, SIFT and SOSNet. Finally, we evaluate potential mitigation techniques that select a subset of descriptors to carefully balance privacy reconstruction risk. While preserving image matching accuracy, our results show that similar accuracy can be obtained when revealing less information."
  - id: 1605
    order: 433
    poster_session: 4
    session_id: 11
    title: "Cross-Modal Generative Augmentation for Visual Question Answering"
    authors:
      - author: "Zixu Wang (Imperial College London)"
      - author: "Yishu Miao (Imperial College London)"
      - author: "Lucia Specia (Imperial College London)"
    all_authors: "Zixu Wang, Yishu Miao and Lucia Specia"
    code: ""
    keywords:
      - word: "visual question answering"
      - word: "data augmentation"
      - word: "generative model"
      - word: "multimodal machine learning"
    paper: "papers/1605.pdf"
    supp: ""
    abstract: "Data augmentation has been shown to effectively improve the performance of multimodal machine learning models. This paper introduces a generative model for data augmentation by leveraging the correlations among multiple modalities. Different from conventional data augmentation approaches that apply low-level operations with deterministic heuristics, our method learns a generator that generates samples of the target modality conditioned on observed modalities in the variational auto-encoder framework. Additionally, the proposed model is able to quantify the confidence of augmented data by its generative probability, and can be jointly optimised with a downstream task. Experiments on Visual Question Answering as downstream task demonstrate the effectiveness of the proposed generative model, which is able to improve strong UpDn-based models to achieve state-of-the-art performance."
  - id: 1617
    order: 222
    oral_session: 5
    poster_session: 3
    session_id: 6
    title: "Tensor Component Analysis for Interpreting the Latent Space of GANs"
    authors:
      - author: "James Oldfield (Queen Mary University of London)"
      - author: "Markos Georgopoulos (Imperial College London)"
      - author: "Yannis Panagakis (University of Athens)"
      - author: "Mihalis A  Nicolaou (Cyprus Institute)"
      - author: "Ioannis Patras (Queen Mary University of London)"
    all_authors: "James Oldfield, Markos Georgopoulos, Yannis Panagakis, Mihalis A  Nicolaou and Ioannis Patras"
    code: ""
    keywords:
      - word: "GANs"
      - word: "interpretable directions"
      - word: "image editing"
      - word: "image synthesis"
      - word: "tensor methods"
      - word: ""
    paper: "papers/1617.pdf"
    supp: "supp/1617_supp.zip"
    abstract: "This paper addresses the problem of finding interpretable directions in the latent space of pre-trained Generative Adversarial Networks (GANs) to facilitate controllable image synthesis. Such interpretable directions correspond to transformations that can affect both the style and geometry of the synthetic images. However, existing approaches that utilise linear techniques to find these transformations often fail to provide an intuitive way to separate these two sources of variation. To address this, we propose to a) perform a multilinear decomposition of the tensor of intermediate representations, and b) use a tensor-based regression to map directions found using this decomposition to the latent space. Our scheme allows for both linear edits corresponding to the individual modes of the tensor, and non-linear ones that model the multiplicative interactions between them. We show experimentally that we can utilise the former to better separate style- from geometry-based transformations, and the latter to generate an extended set of possible transformations in comparison to prior works. We demonstrate our approach's efficacy both quantitatively and qualitatively compared to the current state-of-the-art."
  - id: 1631
    order: 107
    poster_session: 1
    session_id: 2
    title: "Parameter Efficient Dynamic Convolution via Tensor Decomposition"
    authors:
      - author: "Zejiang Hou (Princeton University)"
      - author: "Sun-Yuan Kung (Princeton University)"
    all_authors: "Zejiang Hou and Sun-Yuan Kung"
    code: ""
    keywords:
      - word: "dynamic convolution"
      - word: "input-dependent reparameterization"
      - word: "parameter efficiency"
      - word: "tensor decomposition"
      - word: ""
    paper: "papers/1631.pdf"
    supp: "supp/1631_supp.zip"
    abstract: "Dynamic convolution has demonstrated substantial performance improvements for
convolutional neural networks. Previous aggregation based dynamic convolution methods are challenged by the parameter/memory inefficiency, and the learning difficulty due to the scalar type attention for aggregation. To rectify these limitations, we propose a parameter efficient dynamic convolution operator (dubbed as PEDConv) that learns to discriminatively perturb the spatial, input and output filters of a shared base convolution weight through a tensor decomposition based input-dependent reparameterization. Our method considerably reduces the number of parameters compared to prior arts and limit the computational cost to maintain efficient inference. Meanwhile, the proposed PEDConv significantly boosts the accuracy when substituting standard convolutions on a plethora of prevalent deep learning tasks, including ImageNet classification, COCO object detection, ADE20K semantic segmentation, and adversarial robustness. For example, on ImageNet classification, PEDConv applied to ResNet-50 achieves 80.5% Top-1 accuracy at almost the same computation cost as static convolutional baseline, improving previous best dynamic convolution method by 1.9% accuracy. Moreover, the proposed method can be readily extended to both input and spatial dynamic regime with adaptive reparameterization at different spatial locations, in which case ResNet-50 achieves 79.3% Top-1 accuracy while reducing 44% FLOPs compared to the baseline model."
  - id: 1633
    order: 108
    poster_session: 1
    session_id: 2
    title: "Self-Validation: Early Stopping for Single-Instance Deep Generative Priors "
    authors:
      - author: "Taihui Li (University of Minnesota)"
      - author: "Zhong Zhuang (University of Minnesota)"
      - author: "Hengyue Liang (University of Minnesota)"
      - author: "Le Peng (University of Minnesota)"
      - author: "Hengkang Wang (University of Minnesota)"
      - author: "Ju Sun (University of Minnesota)"
    all_authors: "Taihui Li, Zhong Zhuang, Hengyue Liang, Le Peng, Hengkang Wang and Ju Sun"
    code: "https://sun-umn.github.io/Self-Validation/"
    keywords:
      - word: "Early Stopping"
      - word: "Deep Generative Priors"
      - word: "Image Reconstruction"
      - word: "Image Inverse Problem"
      - word: "Deep Image Prior"
      - word: "Deep Decoder"
      - word: "Autoencoder"
      - word: "Overfitting"
      - word: ""
    paper: "papers/1633.pdf"
    supp: "supp/1633_supp.zip"
    abstract: "Recent works have shown the surprising effectiveness of deep generative models in solving numerous image reconstruction (IR) tasks, without the need for any training set. We call these models, such as deep image prior and deep decoder, collectively as single-instance deep generative priors (SIDGPs). All the successes, however, hinge on appropriate early stopping, which by far has largely been handled in an ad hoc manner or even by visual inspection. In this paper, we propose the first principled method for early stopping when applying SIDGPs to image reconstruction, taking advantage of the typical bell trend of the reconstruction quality. In particular, our method is based on collaborative training and self-validation: the primal reconstruction process is monitored by a deep autoencoder, which is trained online with the historic reconstructed images and used to validate the reconstruction quality constantly. On several IR problems and different SIDGPs that we experiment with, our self-validation method is able to reliably detect near-peak performance levels and signal good stopping points."
  - id: 1642
    order: 325
    poster_session: 3
    session_id: 8
    title: "Highly Efficient Natural Image Matting"
    authors:
      - author: "Yijie Zhong (Nanjing University)"
      - author: "Bo Li (Nanjing University)"
      - author: "Lv Tang (Nanjing University)"
      - author: "Hao Tang (ETH Zurich)"
      - author: "Shouhong Ding (Tencent)"
    all_authors: "Yijie Zhong, Bo Li, Lv Tang, Hao Tang and Shouhong Ding"
    code: ""
    keywords:
      - word: "alpha matting"
    paper: "papers/1642.pdf"
    supp: "supp/1642_supp.zip"
    abstract: "Over the last few years, deep learning based approaches have achieved outstanding improvements in natural image matting. However, there are still two drawbacks that impede the widespread application of image matting: the reliance on user-provided trimaps and the heavy model sizes. In this paper, we propose a trimap-free natural image matting method with a lightweight model. With a lightweight basic convolution block, we build a two-stages framework: Segmentation Network (SN) is designed to capture sufﬁcient semantics and classify the pixels into unknown, foreground, and background regions; Matting Refine Network (MRN) aims at capturing detailed texture information and regressing accurate alpha values. With the proposed cross-level fusion Module (CFM), SN can efficiently utilize multi-scale features with less computational cost.  Efficient non-local attention module (ENA) in MRN can efficiently model the relevance between different pixels and help regress high-quality alpha values. Utilizing these techniques, we construct an extremely light-weighted model,  which achieves comparable performance with ~1% parameters (344k) of large models on popular natural image matting benchmarks. "
  - id: 1657
    order: 217
    poster_session: 2
    session_id: 5
    title: "Noisy Differentiable Architecture Search"
    authors:
      - author: "Xiangxiang   Chu (Meituan)"
      - author: "Bo Zhang (Meituan)"
    all_authors: "Xiangxiang   Chu and Bo Zhang"
    code: "https://github.com/xiaomi-automl/NoisyDARTS"
    keywords:
      - word: "Neural architecture search"
      - word: "AutoML"
    paper: "papers/1657.pdf"
    supp: "supp/1657_supp.zip"
    abstract: "Simplicity is the ultimate sophistication. Differentiable Architecture Search (DARTS) has now become one of the mainstream paradigms of neural architecture search. However, it largely suffers from the well-known performance collapse issue due to the aggregation of skip connections. It is thought to have overly benefited from the residual structure which accelerates the information flow. To weaken this impact, we propose to inject unbiased random noise to impede the flow. We name this novel approach NoisyDARTS. In effect, a network optimizer should perceive this difficulty at each training step and refrain from overshooting, especially on skip connections. In the long run, since we add no bias to the gradient in terms of expectation, it is still likely to converge to the right solution area. We also prove that the injected noise plays a role in smoothing the loss landscape, which makes the optimization easier. Our method features extreme simplicity and acts as a new strong baseline.
We perform extensive experiments across various search spaces, datasets, and tasks, where we robustly achieve state-of-the-art results."
