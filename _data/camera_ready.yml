papers:
  - id: 6
    order: 152
    oral_session: 8
    poster_session: 4
    session_id: 10
    title: "Attention Distillation for Learning Video Representations"
    authors:
      - author: "Miao Liu (Georgia Institute of Technology)"
      - author: "Xin Chen (Y-tech, Kuaishou Technology)"
      - author: "Yun Zhang (Georgia Institute of Technology)"
      - author: "Yin Li (University of Wisconsin-Madison)"
      - author: "James Rehg (Georgia Institute of Technology)"
    all_authors: "Miao Liu, Xin Chen, Yun Zhang, Yin Li and James Rehg"
    code: ""
    keywords:
      - word: "Action Recognition"
      - word: "Deep Learning"
      - word: "Representation Learning"
    paper: "papers/0006.pdf"
    supp: "supp/0006_supp.pdf"
    abstract: "We address the challenging problem of learning motion representations using deep models for video recognition. To this end, we make use of attention modules that learn to highlight regions in the video and aggregate features for recognition. Specifically, we propose to leverage output attention maps as a vehicle to transfer the learned representation from a flow network to an RGB network. We systematically study the design of attention modules, and develop a novel method for attention distillation. Our method is evaluated on major action benchmarks. We show that our method not only improves the performance of the baseline RGB network by a significant margin. Moreover, we demonstrate that attention serves a more robust tool for knowledge distillation in video domain. We believe our method provides a step towards learning motion-aware representations in deep models and valuable insights for knowledge distillation. Our project page is available at https://aptx4869lm.github.io/AttentionDistillation/"
  - id: 9
    order: 75
    poster_session: 2
    session_id: 5
    title: "ViewSynth: Learning Local Features from Depth using View Synthesis"
    authors:
      - author: "Jisan Mahmud (University of North Carolina at Chapel Hill)"
      - author: "Rajat Vikram Singh (Siemens Corporation)"
      - author: "Peri Akiva (Rutgers University)"
      - author: "Spondon Kundu (SIemens Corporate Technology)"
      - author: "Kuan-Chuan Peng (Mitsubishi Electric Research Laboratories)"
      - author: "Jan-Michael  Frahm (UNC-Chapel Hill)"
    all_authors: "Jisan Mahmud, Rajat Vikram Singh, Peri Akiva, Spondon Kundu, Kuan-Chuan Peng and Jan-Michael  Frahm"
    code: ""
    keywords:
      - word: "viewpoint invariant representation learning"
      - word: "depth representation learning"
      - word: "view synthesis"
      - word: "correspondence learning"
    paper: "papers/0009.pdf"
    supp: "supp/0009_supp.zip"
    abstract: "The rapid development of inexpensive commodity depth sensors has made keypoint detection and matching in the depth image modality an important problem in computer vision. Despite great improvements in recent RGB local feature learning methods, adapting them directly in the depth modality leads to unsatisfactory performance. Most of these methods do not explicitly reason beyond the visible pixels in the images. To address the limitations of these methods, we propose a framework ViewSynth, to jointly learn: (1) viewpoint invariant keypoint-descriptor from depth images using a proposed Contrastive Matching Loss, and (2) view synthesis of depth images from different viewpoints using the proposed View Synthesis Module and View Synthesis Loss. By learning view synthesis, we explicitly encourage the feature extractor to encode information about not only the visible, but also the occluded parts of the scene. We demonstrate that in the depth modality, ViewSynth outperforms the state-of-the-art depth and RGB local feature extraction techniques in the 3D keypoint matching and camera localization tasks on the RGB-D datasets 7-Scenes, TUM RGBD and CoRBS in most scenarios. We also show the generalizability of ViewSynth in 3D keypoint matching across different datasets."
  - id: 10
    order: 114
    poster_session: 3
    session_id: 8
    title: "Importance of Self-Consistency in Active Learning for Semantic Segmentation"
    authors:
      - author: "S. Alireza Golestaneh (Carnegie Mellon University)"
      - author: "Kris Kitani (CMU)"
    all_authors: "S. Alireza Golestaneh and Kris Kitani"
    code: ""
    keywords:
      - word: "Active Learning"
      - word: "Semantic Segmentation"
      - word: "Self-Supervised Learing"
      - word: "Self-Consistency"
      - word: "Uncertainty"
    paper: "papers/0010.pdf"
    supp: ""
    abstract: "We address the task of active learning in the context of semantic segmentation and show that self-consistency can be a powerful source of self-supervision to greatly improve the performance of a data-driven model with access to only a small amount of labeled data. Self-consistency uses the simple observation that the results of semantic segmentation for a specific image should not change under transformations like horizontal flipping (i.e.,  the results should only be flipped). In other words, the output of a model should be consistent under equivariant transformations. 
The self-supervisory signal of self-consistency is particularly helpful during active learning since the model is prone to overfitting when there is only a small amount of labeled training data. 
In our proposed active learning framework, we iteratively extract small image patches that need to be labeled, by selecting image patches that have high uncertainty (high entropy) under equivariant transformations. 
We enforce pixel-wise self-consistency between the outputs of the segmentation network for each image and its transformation (horizontally flipped) to utilize the rich self-supervisory information and reduce the uncertainty of the network.
In this way, we are able to find the image patches over which the current model struggles the most to classify. By iteratively training over these difficult image patches, our experiments show that our active learning approach reaches 96% of the top performance of a model trained on all data, by using only 12% of the total data on benchmark semantic segmentation datasets (e.g., CamVid and Cityscapes)."
  - id: 14
    order: 157
    poster_session: 4
    session_id: 11
    title: "Localizing Novel Attended Objects in Egocentric Views"
    authors:
      - author: "Shujon Naha (Indiana University)"
      - author: "Md Reza (Indiana University)"
      - author: "Chen Yu (Indiana University)"
      - author: "David Crandall (Indiana University)"
    all_authors: "Shujon Naha, Md Reza, Chen Yu and David Crandall"
    code: ""
    keywords:
      - word: "attended object localization"
      - word: "weakly supervised localization"
      - word: "generalized object localization"
      - word: "egocentric view understanding"
      - word: "knowledge distillation"
      - word: "feature disentanglement"
    paper: "papers/0014.pdf"
    supp: ""
    abstract: "People have foveated vision and thus are generally able to attend to just a single object within their field of view at a time. Our goal is to learn a model that can automatically identify which object is being attended, given a person’s field of view captured by a first person camera. This problem is different from traditional salient object detection because our goal is not to identify all of the salient objects in the scene, but to identify the single object to which the camera wearer is attending. We present a model that learns based on very weak supervision, with just annotations of the label of the class that is attended in each frame, without bounding boxes or other spatial location information. We show that by learning disentangled representations for localization and classification, our model can effectively localize novel attended objects that were never seen during training. We propose a multi-stage knowledge distillation strategy to train our generalized localizer model.  To the best of our knowledge, our work is the first to explore the problem of learning generalized attended object localization models in egocentric views under weak supervision."
  - id: 16
    order: 11
    poster_session: 1
    session_id: 2
    title: "CornerNet-Lite: Efficient Keypoint based Object Detection"
    authors:
      - author: "Hei Law (Princeton University)"
      - author: "Yun Teng (Princeton University)"
      - author: "Olga Russakovsky (Princeton University)"
      - author: "Jia Deng (Princeton University)"
    all_authors: "Hei Law, Yun Teng, Olga Russakovsky and Jia Deng"
    code: "https://github.com/princeton-vl/CornerNet-Lite"
    keywords:
      - word: "detection"
      - word: "objects"
      - word: "efficiency"
      - word: "efficient"
      - word: "keypoint"
      - word: "real-time"
    paper: "papers/0016.pdf"
    supp: ""
    abstract: "Keypoint-based methods are a relatively new paradigm in object detection, eliminating the need for anchor boxes and offering a simplified detection framework. Keypoint-based CornerNet achieves state of the art accuracy among single-stage detectors. However, this accuracy comes at high processing cost. In this work, we tackle the problem of efficient keypoint-based object detection and introduce CornerNet-Lite. CornerNet-Lite is a combination of two efficient variants of CornerNet: CornerNet-Saccade, which uses an attention mechanism to eliminate the need for exhaustively processing all pixels of the image, and CornerNet-Squeeze, which introduces a new compact backbone architecture. Together these two variants address the two critical use cases in efficient object detection: improving efficiency without sacrificing accuracy, and improving accuracy at real-time efficiency. CornerNet-Saccade is suitable for offline processing, improving the efficiency of CornerNet by 6.0x and the AP by 1.0% on COCO. CornerNet-Squeeze is suitable for real-time detection, improving both the efficiency and accuracy of the popular real-time detector YOLOv3 (34.4% AP at 30ms for CornerNet-Squeeze compared to 33.0% AP at 39ms for YOLOv3 on COCO). Together these contributions for the first time reveal the potential of keypoint-based detection to be useful for applications requiring processing efficiency."
  - id: 28
    order: 59
    oral_session: 4
    poster_session: 2
    session_id: 4
    title: "Neighbourhood-Insensitive Point Cloud Normal Estimation Network"
    authors:
      - author: "Zirui Wang (University of Oxford)"
      - author: "Victor Prisacariu (University of Oxford)"
    all_authors: "Zirui Wang and Victor Prisacariu"
    code: "https://github.com/ziruiw-dev/ni_normal"
    keywords:
      - word: "point cloud"
      - word: "normal estimation"
      - word: "attention"
      - word: "temperature"
      - word: ""
    paper: "papers/0028.pdf"
    supp: "supp/0028_supp.pdf"
    abstract: "We introduce a novel self-attention-based normal estimation network that is able to focus softly on relevant points and adjust the softness by learning a temperature parameter, making it able to work naturally and effectively within a large neighbourhood range. As a result, our model outperforms all existing normal estimation algorithms by a large margin, achieving 94.1% accuracy in comparison with the previous state of the art of 91.2%, with a 25x smaller model and 12x faster inference time. We also use point-to-plane Iterative Closest Point (ICP) as an application case to show that our normal estimations lead to faster convergence than normal estimations from other methods, without manually fine-tuning neighbourhood range parameters. Code available at https://code.active.vision."
  - id: 31
    order: 97
    poster_session: 2
    session_id: 5
    title: "Semi-supervised Active Learning for Instance Segmentation via Scoring Predictions"
    authors:
      - author: "Jun Wang (Ping An Technology (Shenzhen) Co. Ltd.)"
      - author: "Shaoguo Wen (Beijing University of Posts and Telecommunications)"
      - author: "Jianghua Yu (Ping An Technology (Shenzhen) Co. Ltd.)"
      - author: "Kaixing Chen (PingAn Technology (Shenzhen) Co., Ltd)"
      - author: "Xin Zhou (PingAn Technology (Shenzhen) Co., Ltd)"
      - author: "Peng Gao (Ping An Technology (Shenzhen) Co. Ltd.)"
      - author: "Guotong Xie (Ping An Technology (Shenzhen) Co. Ltd.)"
      - author: "Changsheng Li (Beijing Institute of Technology)"
    all_authors: "Jun Wang, Shaoguo Wen, Jianghua Yu, Kaixing Chen, Xin Zhou, Peng Gao, Guotong Xie and Changsheng Li"
    code: ""
    keywords:
      - word: "instance segmentation"
      - word: "active learning"
      - word: "semi-supervised learning"
      - word: "medical images"
      - word: ""
    paper: "papers/0031.pdf"
    supp: "supp/0031_supp.zip"
    abstract: "Active learning generally involves querying the most representative samples for human labeling, which has been widely studied in many fields such as image classification and object detection. However, its potential has not been explored in the more complex instance segmentation task that usually has relatively higher annotation cost. In this paper, we propose a novel and principled semi-supervised active learning framework for instance segmentation. Specifically, we present an uncertainty sampling strategy named Triplet Scoring Predictions (TSP) to explicitly incorporate samples ranking clues from classes, bounding boxes and masks. Moreover, we devise a progressive pseudo labeling regime using the above TSP in semi-supervised manner, it can leverage
both the labeled and unlabeled data to minimize labeling effort while maximize performance of instance segmentation. Results on medical images datasets demonstrate that the proposed method results in the embodiment of knowledge from available data in a meaningful way. The extensive quantitatively and qualitatively experiments show that, our method can yield the best-performing model with notable less annotation costs, compared with state-of-the-arts."
  - id: 33
    order: 125
    poster_session: 3
    session_id: 8
    title: "Revisiting Temporal Modeling for Video Super-resolution"
    authors:
      - author: "Takashi Isobe (Tsinghua University)"
      - author: "Fang Zhu (New York University)"
      - author: "Shengjin Wang (Tsinghua University)"
    all_authors: "Takashi Isobe, Fang Zhu and Shengjin Wang"
    code: "https://github.com/junpan19/RRN"
    keywords:
      - word: "Video Super-Resolution"
      - word: "Recurrent Neural Network"
      - word: "Temporal Modeling"
    paper: "papers/0033.pdf"
    supp: ""
    abstract: "Video super-resolution plays an important role in surveillance video analysis and ultra-high-definition video display, which has drawn much attention in both the research and industrial communities. Although many deep learning-based VSR methods have been proposed, it is hard to directly compare these methods since the different loss functions and training datasets have a significant impact on the super-resolution results. In this work, we carefully study and compare three temporal modeling methods (2D CNN with early fusion, 3D CNN with slow fusion and Recurrent Neural Network) for video super-resolution. We also propose a novel Recurrent Residual Network (RRN) for efficient video super-resolution, where residual learning is utilized to stabilize the training of RNN and meanwhile to boost the super-resolution performance. Extensive experiments show that the proposed RRN is highly computational efficiency and produces temporal consistent VSR results with finer details than other temporal modeling methods. Besides, the proposed method achieves state-of-the-art results on several widely used benchmarks."
  - id: 42
    order: 69
    poster_session: 2
    session_id: 5
    title: "Meta-RetinaNet for Few-shot Object Detection"
    authors:
      - author: "Shaoqi Li (Beihang University)"
      - author: "Wenfeng Song (BeihangUniversity)"
      - author: "Shuai Li (BeihangUniversity)"
      - author: "Aimin Hao (BeihangUniversity)"
      - author: "Hong Qin (Stony Brook University)"
    all_authors: "Shaoqi Li, Wenfeng Song, Shuai Li, Aimin Hao and Hong Qin"
    code: ""
    keywords:
      - word: "Few shot"
      - word: "object detection"
      - word: "meta-learning"
      - word: "Meta-RetinaNet"
      - word: "Balanced Loss"
      - word: "coefficient vector"
      - word: ""
    paper: "papers/0042.pdf"
    supp: ""
    abstract: "Few shot object detection (FSD) is gaining popularity, enhanced by the deep learning methods in recent years. Meanwhile, meta-learning has achieved great success in few-shot image classification benefitting from its adaptive capability corresponding to a suite of tasks. Yet, most object detection models are based on deep neural networks (DNNs), and they are prone to the overfitting problem due to limited samples available during training. To adapt the learned prior knowledge more effectively to new tasks, this paper proposes a novel Meta-RetinaNet for FSD, which avoids a biased meta-learner and improves its generalization ability. It employs a Meta Coefficient Learner (MCL) trained by the Balanced Loss (BL) to augment the DNNs. Specifically, the MCL adapts to tasks by the product of pre-trained convolution weights and coefficient vectors densely for all the convolutional layers, such that it could adequately transfer the learned knowledge to new tasks (while overcoming the overfitting problem) by training fewer parameters. In addition, the BL expedites the training of a Meta-RetinaNet by balancing the performance of a host of tasks, and it also retains stable performance for new tasks. Our experiments showcase the effectiveness of our method, which achieves the state-of-the-art performance on the multiple settings of Pascal VOC and COCO datasets."
  - id: 43
    order: 42
    poster_session: 1
    session_id: 2
    title: "Seeing wake words: Audio-visual Keyword Spotting"
    authors:
      - author: "Liliane Momeni (University of Oxford)"
      - author: "Triantafyllos Afouras (University of Oxford)"
      - author: "Themos Stafylakis (Omilia - Conversational Intelligence)"
      - author: "Samuel Albanie (University of Oxford)"
      - author: "Andrew Zisserman (University of Oxford)"
    all_authors: "Liliane Momeni, Triantafyllos Afouras, Themos Stafylakis, Samuel Albanie and Andrew Zisserman"
    code: "https://github.com/lilianemomeni/KWS-Net"
    keywords:
      - word: "keyword spotting"
      - word: "wake word recognition"
      - word: "zero-shot"
      - word: "audio-visual"
      - word: "lip reading"
      - word: "speech recognition"
      - word: "retrieval"
      - word: ""
    paper: "papers/0043.pdf"
    supp: ""
    abstract: "The goal of this work is to automatically determine whether and when a word of interest is spoken by a talking face, with or without the audio. We propose a zero-shot method suitable for \"in the wild\" videos. Our key contributions are: (1) a novel convolutional architecture, KWS-Net, that uses a similarity map intermediate representation to separate the task into (i) sequence matching, and (ii) pattern detection, to decide whether the word is there and when; (2) we demonstrate that if audio is available, visual keyword spotting improves the performance both for a clean and noisy audio signal. Finally, (3) we show that our method generalises to other languages, specifically French and German, and achieves a comparable performance to English with less language specific data, by fine-tuning the network pre-trained on English. The method exceeds the performance of the previous state-of-the-art visual keyword spotting architecture when trained and tested on the same benchmark, and also that of a state-of-the-art lip reading method."
  - id: 44
    order: 109
    poster_session: 3
    session_id: 8
    title: "An ETF view of Dropout regularization"
    authors:
      - author: "Dor Bank (Tel Aviv University)"
      - author: "Raja Giryes (Tel Aviv University)"
    all_authors: "Dor Bank and Raja Giryes"
    code: "https://github.com/dorbank/An-ETF-view-of-Dropout-Regularization"
    keywords:
      - word: "deep learning"
      - word: "regularization"
      - word: "dropout"
      - word: "frames"
      - word: "information theory"
      - word: "error correction"
      - word: "autoencoders"
    paper: "papers/0044.pdf"
    supp: "supp/0044_supp.pdf"
    abstract: "Dropout is a popular regularization technique in deep learning. Yet, the reason for its success is still not fully understood. This paper provides a new interpretation of Dropout from a frame theory perspective. By drawing a connection to recent developments in analog channel coding, we suggest that for a certain family of autoencoders with a linear encoder, optimizing the encoder with dropout regularization leads to an equiangular tight frame (ETF). Since this optimization is non-convex, we add another regularization that promotes such structures by minimizing the cross-correlation between filters in the network. We demonstrate its applicability in convolutional and fully connected layers in both feed-forward and recurrent networks. All these results suggest that there is indeed a relationship between dropout and ETF structure of the regularized linear operations."
  - id: 46
    order: 189
    poster_session: 4
    session_id: 11
    title: "NTGAN: Learning Blind Image Denoising without Clean Reference"
    authors:
      - author: "Rui Zhao (The Hong Kong Polytechnic University	)"
      - author: "Daniel P.K. Lun (The Hong Kong Polytechnic University)"
      - author: "Kin-Man Lam (The Hong Kong Polytechnic University)"
    all_authors: "Rui Zhao, Daniel P.K. Lun and Kin-Man Lam"
    code: ""
    keywords:
      - word: "unsupervised image denoising"
      - word: "blind image denoising"
      - word: "pseudo supervision"
      - word: "noise transference"
      - word: ""
    paper: "papers/0046.pdf"
    supp: "supp/0046_supp.pdf"
    abstract: "Recent studies on learning-based image denoising have achieved promising performance on various noise reduction tasks. Most of these deep denoisers are trained either under the supervision of clean references, or unsupervised on synthetic noise. The assumption with the synthetic noise leads to poor generalization when facing real photographs. To address this issue, we propose a novel deep unsupervised image-denoising method by regarding the noise reduction task as a special case of the noise transference task. Learning noise transference enables the network to acquire the denoising ability by only observing the corrupted samples. The results on real-world denoising benchmarks demonstrate that our proposed method achieves state-of-the-art performance on removing realistic noises, making it a potential solution to practical noise reduction problems."
  - id: 51
    order: 107
    poster_session: 3
    session_id: 8
    title: "Novel View Synthesis from Single Images via Point Cloud Transformation"
    authors:
      - author: "Hoang-An Le (University of Amsterdam)"
      - author: "Thomas Mensink (Google Research / University of Amsterdam)"
      - author: "Partha Das (University of Amsterdam)"
      - author: "Theo Gevers (University of Amsterdam)"
    all_authors: "Hoang-An Le, Thomas Mensink, Partha Das and Theo Gevers"
    code: "https://github.com/lhoangan/pc4novis"
    keywords:
      - word: "novel view synthesis single images self-supervised depth estimation point cloud construction"
    paper: "papers/0051.pdf"
    supp: ""
    abstract: "In this paper the argument is made that for true novel view synthesis of objects, where the object can be synthesized from any viewpoint, an explicit 3D shape representation is desired. Our method estimates point clouds to capture the geometry of the object, which can be freely rotated into the desired view and then projected into a new image. This image, however, is sparse by nature and hence this coarse view is used as the input of an image completion network to obtain the dense target view. The point cloud is obtained using the predicted pixel-wise depth map, estimated from a single RGB input image, combined with the camera intrinsics. By using forward warping and backward warping between the input view and the target view, the network can be trained end-to-end without supervision on depth. The benefit of using point clouds as an explicit 3D shape for novel view synthesis is experimentally validated on the 3D ShapeNet benchmark."
  - id: 53
    order: 39
    poster_session: 1
    session_id: 2
    title: "A Spherical Approach to Planar Semantic Segmentation"
    authors:
      - author: "Chao Zhang (Toshiba Research Europe Ltd)"
      - author: "Sen He (University of Exeter)"
      - author: "Stephan Liwicki (Toshiba Europe Limited)"
    all_authors: "Chao Zhang, Sen He and Stephan Liwicki"
    code: ""
    keywords:
      - word: "Semantic Segmentation"
      - word: "Spherical DNN"
    paper: "papers/0053.pdf"
    supp: "supp/0053_supp.zip"
    abstract: "We investigate a geometrically motivated modification to semantic segmentation. In particular, we reformulate typical planar CNN as a projected spherical CNN where image distortions are reduced, and thus generalisation increased. Since prior formulations of spherical CNNs require computation on full spheres, fair comparison between planar and spherical methods have not been previously presented. In this work, we first extend spherical deep learning to support high-resolution images by exploiting the reduced field of view of classical images. Then, we employ our spherical representation to reduce distortion effects of standard deep learning systems. On typical benchmarks, we apply our spherical representation and consistently outperform the classical representation of multiple existing architectures. Additionally, we introduce direct spherical pretraining  from  planar  datasets  to  further  improve  results. Finally,  we  compare our method on non-planar datasets, where we improve accuracy, and outperform running time of spherical state of the art for non-complete input spheres."
  - id: 55
    order: 178
    poster_session: 4
    session_id: 11
    title: "Generative Appearance Flow: A Hybrid Approach for Outdoor View Synthesis"
    authors:
      - author: "M. Usman Rafique (University of Kentucky)"
      - author: "Hunter Blanton (University of Kentucky)"
      - author: "Noah Snavely (Cornell University and Google AI)"
      - author: "Nathan Jacobs (University of Kentucky)"
    all_authors: "M. Usman Rafique, Hunter Blanton, Noah Snavely and Nathan Jacobs"
    code: "https://mvrl.github.io/GAF"
    keywords:
      - word: "novel view synthesis"
      - word: "image synthesis"
      - word: ""
    paper: "papers/0055.pdf"
    supp: "supp/0055_supp.zip"
    abstract: "We address the problem of view synthesis in complex outdoor scenes. We propose a novel convolutional neural network architecture that includes flow-based and direct synthesis sub-networks. Both sub-networks introduce novel elements that greatly improve the quality of the synthesized images. These images are then adaptively fused to create the final output image. Our approach achieves state-of-the-art performance on the KITTI dataset, which is commonly used to evaluate view-synthesis methods. Unlike many recently proposed methods, ours is trained without the need for additional geometric constraints, such as a ground-truth depth map, making it more broadly applicable. Our approach also achieved the best performance on the Brooklyn Panorama Synthesis dataset, which we introduce as a new, challenging benchmark for view synthesis. Our dataset, code, and pretrained models are available at url{https://mvrl.github.io/GAF}."
  - id: 59
    order: 64
    poster_session: 2
    session_id: 5
    title: "Improved Trainable Calibration Method for Neural Networks"
    authors:
      - author: "Gongbo Liang (University of Kentucky)"
      - author: "Yu Zhang (University of Kentucky)"
      - author: "Xiaoqing Wang (University of Kentucky)"
      - author: "Nathan Jacobs (University of Kentucky)"
    all_authors: "Gongbo Liang, Yu Zhang, Xiaoqing Wang and Nathan Jacobs"
    code: ""
    keywords:
      - word: "medical imaging"
      - word: "neural network calibration"
      - word: "trainable"
    paper: "papers/0059.pdf"
    supp: "supp/0059_supp.pdf"
    abstract: "Recent works have shown that modern neural networks can achieve super-human performance in a wide range of image classification tasks. However, these works have primarily focused on classification accuracy, ignoring the important role of uncertainty quantification in medical decision-making. Empirically, neural networks are often miscalibrated and dramatically overconfident in their predictions. This miscalibration could be problematic in any automatic decision-making system, but we focus on the medical field in which neural network miscalibration has the potential to lead to significant treatment errors. We propose a novel approach to neural network calibration that maintains the overall classification accuracy while significantly improving model calibration. The proposed approach is based on ECE, which is a standard metric for quantifying model calibration error. As such, it is a natural and empirical way of assessing model calibration. Our approach can be easily integrated into any classification task as an auxiliary loss term, thus not requiring an explicit training round for calibration. We show that our approach reduces calibration error significantly across various architectures and datasets and that it performs better than temperature scaling, the current state-of-the-art approach."
  - id: 63
    order: 163
    poster_session: 4
    session_id: 11
    title: "Centroid Based Concept Learning for RGB-D Indoor Scene Classification"
    authors:
      - author: "Ali Ayub (The Pennsylvania State University)"
      - author: "Alan Wagner (The Pennsylvania State University)"
    all_authors: "Ali Ayub and Alan Wagner"
    code: "https://github.com/aliayub7/CBCL_RGBD"
    keywords:
      - word: "cognitively-inspired learning"
      - word: "RGBD analysis"
      - word: "scene classification"
      - word: "category merging"
      - word: "labeling flaws analysis"
    paper: "papers/0063.pdf"
    supp: "supp/0063_supp.zip"
    abstract: "This paper contributes a novel cognitively-inspired method for RGB-D indoor scene classification. High intra-class variance and low inter-class variance makes indoor scene classification an extremely challenging task. To cope with this problem, we propose a clustering approach inspired by the concept learning model of the hippocampus and the neocortex, to generate clusters and centroids for different scene categories. Test images depicting different scenes are classified by using their distance to the closest centroids (concepts). Modeling of RGB-D scenes as centroids not only leads to state-of-the-art classification performance on benchmark datasets (SUN RGB-D and NYU Depth V2), but also offers a method for inspecting and interpreting the space of centroids. Inspection of the centroids generated by our approach on RGB-D datasets leads us to propose a method for merging conceptually similar categories, resulting in improved accuracy for all approaches."
  - id: 66
    order: 1
    oral_session: 1
    poster_session: 1
    session_id: 0
    title: "SIA-GCN: A Spatial Information Aware Graph Neural Network with 2D Convolutions for Hand Pose Estimation"
    authors:
      - author: "Deying Kong (university of california, irvine)"
      - author: "Haoyu Ma (University of California, Irvine)"
      - author: "Xiaohui Xie (University of California, Irvine)"
    all_authors: "Deying Kong, Haoyu Ma and Xiaohui Xie"
    code: ""
    keywords:
      - word: "pose estimation"
      - word: "spatial relationship"
      - word: "graph convolutional network"
      - word: "GCN"
      - word: "hand pose estimation"
    paper: "papers/0066.pdf"
    supp: ""
    abstract: "Graph Neural Networks (GNNs) generalize neural networks from applications on regular structures to applications on arbitrary graphs, and have shown success in many application domains such as computer vision, social networks and chemistry. In this paper, we extend GNNs along two directions: a) allowing features at each node to be represented by 2D spatial confidence maps instead of 1D vectors; and b) proposing an efficient operation to integrate information from neighboring nodes through 2D convolutions with different learnable kernels at each edge.  The proposed SIA-GCN can efficiently extract spatial information from 2D maps at each node and propagate them through graph convolution. By associating each edge with a designated convolution kernel, the SIA-GCN could capture different spatial relationships for different pairs of neighboring nodes. We demonstrate the utility of SIA-GCN on the task of estimating hand keypoints from single-frame images, where the nodes represent the 2D coordinate heatmaps of  keypoints and the edges denote the kinetic relationships between keypoints.  Experiments on multiple datasets show that  SIA-GCN provides a flexible and yet powerful framework to account for structural constraints between keypoints, and can achieve state-of-the-art performance on the task of hand pose estimation. "
  - id: 79
    order: 165
    poster_session: 4
    session_id: 11
    title: "On the Exploration of Incremental Learning for Fine-grained Image Retrieval"
    authors:
      - author: "Wei Chen (Leiden University)"
      - author: "Yu Liu (KU Leuven)"
      - author: "Weiping Wang (National University of Defense Technology)"
      - author: "Tinne Tuytelaars (KU Leuven)"
      - author: "Erwin M. Bakker (Leiden University)"
      - author: "Michael Lew (Leiden Institute of Advanced Computer Science)"
    all_authors: "Wei Chen, Yu Liu, Weiping Wang, Tinne Tuytelaars, Erwin M. Bakker and Michael Lew"
    code: ""
    keywords:
      - word: "Incremental learning"
      - word: "Fine-grained image retrieval"
      - word: "Catastrophic forgetting"
      - word: "Maximum Mean Discrepancy"
    paper: "papers/0079.pdf"
    supp: "supp/0079_supp.zip"
    abstract: "In this paper, we consider the problem of fine-grained image retrieval in an incremental setting, when new categories are added over time. On the one hand, repeatedly training the representation on the extended dataset is time-consuming. On the other hand, fine-tuning the learned representation only with the new classes leads to catastrophic forgetting. To this end, we propose an incremental learning method to mitigate retrieval performance degradation caused by the forgetting issue. Without accessing any samples of the original classes, the classifier of the original network provides soft “labels” to transfer knowledge to train the adaptive network, so as to preserve the previous capability for classification. More importantly, a regularization function based on Maximum Mean Discrepancy is devised to minimize the discrepancy of new classes features from the original network and the adaptive network, respectively. Extensive experiments on two datasets show that our method effectively mitigates the catastrophic forgetting on the original classes while achieving high performance on the new classes."
  - id: 81
    order: 169
    poster_session: 4
    session_id: 11
    title: "Synthetic Training for Accurate 3D Human Pose and Shape Estimation in the Wild"
    authors:
      - author: "Akash Sengupta (University of Cambridge)"
      - author: "Roberto Cipolla (University of Cambridge)"
      - author: "Ignas Budvytis (Department of Engineering, University of Cambridge)"
    all_authors: "Akash Sengupta, Roberto Cipolla and Ignas Budvytis"
    code: "https://github.com/akashsengupta1997/STRAPS-3DHumanShapePose"
    keywords:
      - word: "3D human shape estimation"
      - word: "3D pose estimation"
      - word: "3D reconstruction"
      - word: "smpl"
      - word: "synthetic data"
      - word: "pose and shape optimisation"
      - word: "3D human dataset"
    paper: "papers/0081.pdf"
    supp: "supp/0081_supp.pdf"
    abstract: "This paper addresses the problem of monocular 3D human shape and pose estimation from an RGB image. Despite great progress in this field in terms of pose prediction accuracy, state-of-the-art methods often predict inaccurate body shapes. We suggest that this is primarily due to the scarcity of in-the-wild training data with diverse and accurate body shape labels. Thus, we propose STRAPS (Synthetic Training for Real Accurate Pose and Shape), a system that utilises proxy representations (such as silhouettes and 2D joints) as inputs to a shape and pose regression neural network, which is trained with synthetic training data (generated using the SMPL statistical body model) to overcome data scarcity. We bridge the gap between synthetic training inputs and noisy real inputs, which are predicted by keypoint detection and segmentation CNNs at test-time, by using data augmentation and corruption during training. In order to evaluate our approach, we curate and provide a challenging evaluation dataset for monocular human shape estimation, Sports Shape and Pose 3D (SSP-3D). It consists of RGB images of tightly-clothed sports-persons with a variety of body shapes and corresponding pseudo-ground-truth SMPL shape and pose parameters, obtained via multi-frame optimisation. We show that STRAPS outperforms other state-of-the-art methods on SSP-3D in terms of shape prediction accuracy, while remaining competitive with the state-of-the-art on pose-centric datasets and metrics."
  - id: 82
    order: 33
    poster_session: 1
    session_id: 2
    title: "Cross-dataset Color Constancy Revisited Using Sensor-to-Sensor Transfer"
    authors:
      - author: "Samu Koskinen (Huawei Technologies Oy (Finland) Co. Ltd)"
      - author: "Dan Yang (Tampere University)"
      - author: "Joni-Kristian Kamarainen (Tampere University)"
    all_authors: "Samu Koskinen, Dan Yang and Joni-Kristian Kamarainen"
    code: ""
    keywords:
      - word: "color constancy"
      - word: "white balance"
      - word: "sensor-to-sensor transfer"
      - word: "illuminant augmentation"
      - word: "cross-dataset benchmark"
      - word: "raw to spectral image"
    paper: "papers/0082.pdf"
    supp: ""
    abstract: "Color constancy is required for camera captured images and therefore all digital camera imaging pipelines include an Auto White Balance (AWB) algorithm. An intrinsic problem of AWB is that it is sensor specific and therefore developers need to repeatedly collect new in-house datasets to adjust their methods for new sensors. In literature, the best learning-based methods achieve state-of-the-art performance with clear margin on all available datasets, but performance significantly degrades in cross-dataset experiments due to the aforementioned reason. In this work, we introduce a sensor-to-sensor transfer model that can be used to map datasets with known cameras to any other known camera. The only requirement is that spectral characterizations of the camera models are available. In our experiments, we demonstrate improvements in cross-dataset settings using the proposed sensor-to-sensor transfer model. In addition, for the first time we are able to analyze the characteristics of existing datasets in the common standard observer space and our analysis reveals that certain datasets contain images which are not suitable for color constancy. We introduce a unified cross-dataset color constancy benchmark dataset, compare two state-of-the-art learning-based AWB methods and show superior performance of the proposed sensor-to-sensor model."
  - id: 86
    order: 162
    poster_session: 4
    session_id: 11
    title: "Learning-based Region Selection for End-to-End Gaze Estimation"
    authors:
      - author: "Xucong Zhang (ETH Zurich)"
      - author: "Yusuke Sugano (The University of Tokyo)"
      - author: "Andreas Bulling (University of Stuttgart)"
      - author: "Otmar Hilliges (ETH Zurich)"
    all_authors: "Xucong Zhang, Yusuke Sugano, Andreas Bulling and Otmar Hilliges"
    code: ""
    keywords:
      - word: "gaze estimation"
      - word: "region selection"
      - word: ""
    paper: "papers/0086.pdf"
    supp: "supp/0086_supp.zip"
    abstract: "Traditionally, appearance-based gaze estimation methods use statically defined face regions as input to the gaze estimator, such as eye patches, and therefore suffer from difficult lighting conditions and extreme head poses for which these regions are often not the most informative with respect to the gaze estimation task. We posit that facial regions should be selected dynamically based on the image content and propose a novel gaze estimation method that combines the task of region proposal and gaze estimation into a single end-to-end trainable framework. We introduce a novel loss that allows for unsupervised training of a region proposal network alongside the (supervised) training of the final gaze estimator. We show that our method can learn meaningful region selection strategies and outperforms fixed region approaches. We further show that our method performs particularly well for challenging cases, i.e., those with difficult lighting conditions such as directional lights, extreme head angles, or self-occlusion. Finally, we show that the proposed method achieves better results than the current state-of-the-art method in within and cross-dataset evaluations."
  - id: 90
    order: 132
    poster_session: 3
    session_id: 8
    title: "Transferring Pretrained Networks to Small Data via Category Decorrelation"
    authors:
      - author: "Ying Jin (Tsinghua University)"
      - author: "Zhangjie Cao (Tsinghua University)"
      - author: "Mingsheng Long (Tsinghua University)"
      - author: "Jianmin Wang (Tsinghua University, China)"
    all_authors: "Ying Jin, Zhangjie Cao, Mingsheng Long and Jianmin Wang"
    code: ""
    keywords:
      - word: "Category Decorrelation"
      - word: "Under Transfer"
    paper: "papers/0090.pdf"
    supp: ""
    abstract: "Transfer learning by fine-tuning neural networks pre-trained on large-scale datasets excels at accelerating the training process and improving the model performance for the target task. Previous works have unveiled catastrophic forgetting in fine-tuning, where the model is over-transferred thus losing pre-trained knowledge, especially facing large target datasets. However, when fine-tuning pre-trained networks to small data, under transfer emerges instead, where the model sticks to the pre-trained model and learns little target knowledge. Under transfer severely restricts the wide use of fine-tuning but is still under-investigated. In this paper, we conduct an in-depth study of under transfer problem in fine-tuning and observe that when we finetune model to small data, redundant category correlation becomes stronger in the model prediction, which is a potential cause of under transfer. Based on the observation, we propose a novel regularization approach, Category Decorrelation (CatDec), to minimize category correlation in the model, which introduces a new inductive bias to strengthen the model transfer. CatDec is orthogonal to existing fine-tuning approaches and can collaborate with them to address the dilemma of catastrophic forgetting and under transfer. Experiment results demonstrate that the proposed approach can consistently improve the fine-tuning performance of various mainstream methods. Further analyses prove that CatDec alleviates redundant category correlation and helps transfer."
  - id: 96
    order: 99
    oral_session: 5
    poster_session: 3
    session_id: 6
    title: "EPI-based Oriented Relation Networks for Light Field Depth Estimation"
    authors:
      - author: "Kunyuan Li (Hefei University of Technology)"
      - author: "Jun Zhang (Hefei University of Technology)"
      - author: "Rui Sun (Hefei University of Technology)"
      - author: "Xudong Zhang (Hefei University of Technology)"
      - author: "Jun Gao (Hefei University of Technology)"
    all_authors: "Kunyuan Li, Jun Zhang, Rui Sun, Xudong Zhang and Jun Gao"
    code: "https://github.com/lkyahpu/EPI_ORM.git"
    keywords:
      - word: "depth estimation"
      - word: "light field"
      - word: "relation modeling"
      - word: "epipolar plane image"
      - word: "refocusing"
      - word: "Siamese network"
      - word: ""
    paper: "papers/0096.pdf"
    supp: ""
    abstract: "Light field cameras record not only the spatial information of observed scenes but also the directions of all incoming light rays. The spatial and angular information implicitly contain geometrical characteristics such as multi-view or epipolar geometry, which can be exploited to improve the performance of depth estimation. An Epipolar Plane Image (EPI), the unique 2D spatial-angular slice of the light field, contains patterns of oriented lines. The slope of these lines is associated with the disparity. Benefiting from this property of EPIs, some representative methods estimate depth maps by analyzing the disparity of each line in EPIs. However, these methods often extract the optimal slope of the lines from EPIs while ignoring the relationship between neighboring pix- els, which leads to inaccurate depth map predictions. Based on the observation that an oriented line and its neighboring pixels in an EPI share a similar linear structure, we propose an end-to-end fully convolutional network (FCN) to estimate the depth value of the intersection point on the horizontal and vertical EPIs. Specifically, we present a new feature-extraction module, called Oriented Relation Module (ORM), that constructs the relationship between the line orientations. To facilitate training, we also propose a refocusing-based data augmentation method to obtain different slopes from EPIs of the same scene point. Extensive experiments verify the efficacy of learning relations and show that our approach is competitive to other state-of-the-art methods. The code and the trained models are available at https://github.com/lkyahpu/EPI_ORM.git."
  - id: 97
    order: 188
    poster_session: 4
    session_id: 11
    title: "N2NSkip: Learning Highly Sparse Networks using Neuron-to-Neuron Skip Connections"
    authors:
      - author: "Arvind Subramaniam (Bits Pilani Hyderabad Campus)"
      - author: "Avinash Sharma (CVIT, IIIT-Hyderabad)"
    all_authors: "Arvind Subramaniam and Avinash Sharma"
    code: ""
    keywords:
      - word: "model compression"
      - word: "pruning"
      - word: "heat diffusion"
      - word: "Convolutional Neural Networks (CNN)"
      - word: "undirected graphs"
      - word: "heat diffusion"
      - word: "skip connections"
      - word: "N2NSkip"
      - word: "scree diagram"
      - word: "connection sensitivity"
      - word: ""
    paper: "papers/0097.pdf"
    supp: ""
    abstract: "The over-parametrized nature of Deep Neural Networks (DNNs) leads to considerable hindrances during deployment on low-end devices with time and space constraints. Network pruning strategies that sparsify DNNs using iterative prune-train schemes are often computationally expensive. As a result, techniques that prune at initialization, prior to training, have become increasingly popular. In this work, we propose neuron-to-neuron skip (N2NSkip) connections, which act as sparse weighted skip connections, to enhance the overall connectivity of pruned DNNs. Following a preliminary pruning step, N2NSkip connections are randomly added between individual neurons/channels of the pruned network, while maintaining the overall sparsity of the network. We demonstrate that introducing N2NSkip connections in pruned networks enables significantly superior performance, especially at high sparsity levels, as compared to pruned networks without N2NSkip connections. Additionally, we present a heat diffusion-based connectivity analysis to quantitatively determine the connectivity of the pruned network with respect to the reference network. We evaluate the efficacy of our approach on two different preliminary pruning methods which prune at initialization and consistently obtain superior performance by exploiting the enhanced connectivity resulting from N2NSkip connections."
  - id: 99
    order: 32
    poster_session: 1
    session_id: 2
    title: "Adversarial Color Enhancement: Generating Unrestricted Adversarial Images by Optimizing a Color Filter"
    authors:
      - author: "Zhengyu Zhao (Radboud University)"
      - author: "Zhuoran Liu (Radboud University)"
      - author: "Martha Larson (Radboud University)"
    all_authors: "Zhengyu Zhao, Zhuoran Liu and Martha Larson"
    code: "https://github.com/ZhengyuZhao/ACE"
    keywords:
      - word: "adversarial examples"
      - word: "color filter"
      - word: "image enhancement"
      - word: "image quality"
      - word: "transferability"
      - word: "deep neural networks"
      - word: ""
    paper: "papers/0099.pdf"
    supp: ""
    abstract: "We introduce an approach that enhances images using a color filter in order to create adversarial effects, which fool neural networks into misclassification. Our approach, Adversarial Color Enhancement (ACE), generates unrestricted adversarial images by optimizing the color filter via gradient descent. The novelty of ACE is its incorporation of established practice for image enhancement in a transparent manner. Experimental results validate the white-box adversarial strength and black-box transferability of ACE. A range of examples demonstrates the perceptual quality of images that ACE produces. ACE makes an important contribution to recent work that moves beyond $L_p$ imperceptibility and focuses on unrestricted adversarial modifications that yield large perceptible perturbations, but remain non-suspicious, to the human eye. The future potential of filter-based adversaries is also explored in two directions: guiding ACE with common enhancement practices (e.g., Instagram filters) towards specific attractive image styles and adapting ACE to image semantics. Code is available at https://github.com/ZhengyuZhao/ACE."
  - id: 101
    order: 102
    oral_session: 5
    poster_session: 3
    session_id: 6
    title: "Key-Nets: Optical Transformation Convolutional Networks for Privacy Preserving Vision Sensors"
    authors:
      - author: "Jeffrey Byrne (STR"
      - author: "Visym Labs)"
      - author: "Brian DeCann (Systems & Technology Research)"
      - author: "Scott Bloom (STR)"
    all_authors: "Jeffrey Byrne (STR, Visym Labs), Brian DeCann and Scott Bloom"
    code: "https://visym.github.io/keynet"
    keywords:
      - word: "privacy"
      - word: "homomorphic encryption"
      - word: "sensor"
      - word: "key-net"
      - word: "convolutional network"
      - word: "encryption"
      - word: "optics"
      - word: "ethics"
      - word: "hill cipher"
      - word: "adversarial learning"
      - word: ""
    paper: "papers/0101.pdf"
    supp: "supp/0101_supp.pdf"
    abstract: "Modern cameras are not designed with computer vision or machine learning as the target application.  There is a need for a new class of vision sensors that are privacy preserving by design, that do not leak private information and collect only the information necessary for a target machine learning task.   In this paper, we introduce key-nets, which are convolutional networks paired with a custom vision sensor which applies an optical/analog transform such that the key-net can perform exact encrypted inference on this transformed image, but the image is not interpretable by a human or any other key-net.  We provide five sufficient conditions for an optical transformation suitable for a key-net, and show that generalized stochastic matrices (e.g. scale, bias and fractional pixel shuffling) satisfy these conditions.  We motivate the key-net by showing that without it there is a utility/privacy tradeoff for a network fine-tuned directly on optically transformed images for face identification and object detection. Finally, we show that a key-net is equivalent to homomorphic encryption using a Hill cipher, with an upper bound on memory and runtime that scales quadratically with a user specified privacy parameter. Therefore, the key-net is the first practical, efficient and privacy preserving vision sensor based on optical homomorphic encryption."
  - id: 102
    order: 4
    oral_session: 2
    poster_session: 1
    session_id: 1
    title: "Cross-Modal Hierarchical Modelling for Fine-Grained Sketch Based Image Retrieval"
    authors:
      - author: "Aneeshan Sain (University of Surrey)"
      - author: "Ayan Kumar Bhunia  (University of Surrey)"
      - author: "Yongxin Yang (University of Surrey)"
      - author: "Tao Xiang (University of Surrey)"
      - author: "Yi-Zhe Song (University of Surrey)"
    all_authors: "Aneeshan Sain, Ayan Kumar Bhunia, Yongxin Yang, Tao Xiang and Yi-Zhe Song"
    code: ""
    keywords:
      - word: "cross-modal co-attention"
      - word: "sketch hierarchy"
      - word: "cross-modal retrieval"
      - word: "sketch based image retrieval"
    paper: "papers/0102.pdf"
    supp: ""
    abstract: "Sketch as an image search query is an ideal alternative to text in capturing the fine-grained visual details. Prior successes on fine-grained sketch-based image retrieval (FG-SBIR) have demonstrated the importance of tackling the unique traits of sketches as opposed to photos, e.g., temporal vs. static, strokes vs. pixels, and abstract vs. pixel-perfect. In this paper, we study a further trait of sketches that has been overlooked to date, that is, they are hierarchical in terms of the levels of detail -- a person typically sketches up to various extents of detail to depict an object. This hierarchical structure is often visually distinct. In this paper,  we design a novel network that is capable of cultivating sketch-specific hierarchies and exploiting them to match sketch with photo at corresponding hierarchical levels. In particular, features from a sketch and a photo are enriched using cross-modal co-attention, coupled with hierarchical node fusion at every level to form a better embedding space to conduct retrieval. Experiments on common benchmarks show our method to outperform state-of-the-arts by a significant margin."
  - id: 111
    order: 182
    poster_session: 4
    session_id: 11
    title: "A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer"
    authors:
      - author: "Vladimir Iashin (Tampere University)"
      - author: "Esa Rahtu (Tampere University)"
    all_authors: "Vladimir Iashin and Esa Rahtu"
    code: "https://github.com/v-iashin/BMT"
    keywords:
      - word: "Dense Video Captioning"
      - word: "Temporal Action Proposal Generation"
      - word: "Bi-modal Transformer"
      - word: "Audio-visual Training"
      - word: "Cross-modal"
      - word: "Multi-modal"
      - word: "ActivityNet Captions"
    paper: "papers/0111.pdf"
    supp: "supp/0111_supp.zip"
    abstract: "Dense video captioning aims to localize and describe important events in untrimmed videos. Existing methods mainly tackle this task by exploiting only visual features, while completely neglecting the audio track. Only a few prior works have utilized both modalities, yet they show poor results or demonstrate the importance on a dataset with a specific domain. In this paper, we introduce Bi-modal Transformer which generalizes the Transformer architecture for a bi-modal input. We show the effectiveness of the proposed model with audio and visual modalities on the dense video captioning task, yet the module is capable of digesting any two modalities in a sequence-to-sequence task. We also show that the pre-trained bi-modal encoder as a part of the bi-modal transformer can be used as a feature extractor for a simple proposal generation module. The performance is demonstrated on a challenging ActivityNet Captions dataset where our model achieves outstanding performance. The code is available: v-iashin.github.io/bmt"
  - id: 113
    order: 48
    poster_session: 1
    session_id: 2
    title: "STQ-Nets: Unifying Network Binarization and Structured Pruning"
    authors:
      - author: "Aurobindo Munagala (IIIT Hyderabad)"
      - author: "Ameya Prabhu (University of Oxford)"
      - author: "Anoop Namboodiri (IIIT Hyderabad)"
    all_authors: "Aurobindo Munagala, Ameya Prabhu and Anoop Namboodiri"
    code: ""
    keywords:
      - word: "quantization"
      - word: "binary networks"
      - word: "binarization"
      - word: "pruning"
      - word: "compression"
      - word: "inference"
      - word: ""
    paper: "papers/0113.pdf"
    supp: ""
    abstract: "We discuss a formulation for network compression combining two major paradigms: binarization and pruning. Past works on network binarization have demonstrated that networks are robust to the removal of activation/weight magnitude information, and can perform comparably to full-precision networks with signs alone. Pruning focuses on generating efficient and sparse networks. Both compression paradigms aid deployment in portable settings, where storage, compute and power are limited. 

We argue that these paradigms are complementary, and can be combined to offer high levels of compression and speedup without any significant accuracy loss. Intuitively, weights/activations closer to zero have higher binarization error making them good candidates for pruning. Our proposed formulation incorporates speedups from binary convolution algorithms through structured pruning, enabling the removal of pruned parts of the network entirely post-training, beating previous works attempting the same by a significant margin. Overall, our method brings up to 89x layer-wise compression over the corresponding full-precision networks --  achieving only 0.33% loss on CIFAR-10 with ResNet-18 with a 40% PFR (Prune Factor Ratio for filters), and 0.3% on ImageNet with ResNet-18 with a 19% PFR."
  - id: 115
    order: 149
    oral_session: 7
    poster_session: 4
    session_id: 9
    title: "Multimodal Image Translation with Stochastic Style Representations and Mutual Information Loss"
    authors:
      - author: "Sanghyeon Na (Korea University)"
      - author: "Seungjoo  Yoo (Korea University)"
      - author: "Jaegul Choo (Korea Advanced Institute of Science and Technology)"
    all_authors: "Sanghyeon Na, Seungjoo  Yoo and Jaegul Choo"
    code: ""
    keywords:
      - word: "image-to-image translation"
      - word: "generative adversarial network"
      - word: ""
    paper: "papers/0115.pdf"
    supp: "supp/0115_supp.pdf"
    abstract: "Unpaired multimodal image-to-image translation is a task of converting a given image in a source domain into diverse images in a target domain. We propose two approaches to produce high-quality and diverse images. First, we propose to encode a source image conditioned on a given target style feature. It allows our model to generate higher-quality images than existing models, which are not based on this method. Second, we propose an information-theoretic loss function that effectively captures styles in an image. It allows our model to learn complex high-level styles rather than simple low-level styles, and generate perceptually diverse images. We show our proposed model achieves state-of-the-art performance through extensive experiments on various real-world datasets."
  - id: 118
    order: 134
    poster_session: 3
    session_id: 8
    title: "Point Cloud Super Resolution with Adversarial Residual Graph Networks"
    authors:
      - author: "Huikai Wu (CASIA)"
      - author: "Kaiqi Huang (Institute of Automation, Chinese Academy of Sciences)"
    all_authors: "Huikai Wu and Kaiqi Huang"
    code: "https://github.com/wuhuikai/PointCloudSuperResolution"
    keywords:
      - word: "point cloud"
      - word: "super resolution"
      - word: "GAN"
      - word: "residual graph convolution"
      - word: "upsampling"
      - word: ""
    paper: "papers/0118.pdf"
    supp: ""
    abstract: "Point cloud super-resolution is a fundamental problem for 3D reconstruction and 3D data understanding. It takes a low-resolution (LR) point cloud as input and generates a high-resolution (HR) point cloud with rich details. In this paper, we present a data-driven method for point cloud super-resolution based on graph networks and adversarial losses. The key idea of the proposed network is to exploit the local similarity of point cloud and the analogy between LR input and HR output. For the former, we design a deep network with graph convolution. For the latter, we propose to add residual connections into graph convolution and introduce a skip connection between input and output. The proposed network is trained with a novel loss function, which combines Chamfer Distance (CD) and graph adversarial loss. Such a loss function captures the characteristics of HR point cloud automatically without manual design. We conduct a series of experiments to evaluate our method and validate the superiority over other methods. Results show that the proposed method achieves state-of-the-art performance and have a good generalization ability to unseen data."
  - id: 121
    order: 140
    poster_session: 3
    session_id: 8
    title: "Image Harmonization with Attention-based Deep Feature Modulation"
    authors:
      - author: "Guoqing Hao (University of Tsukuba)"
      - author: "Satoshi Iizuka (University of Tsukuba)"
      - author: "Kazuhiro Fukui (University of Tsukuba)"
    all_authors: "Guoqing Hao, Satoshi Iizuka and Kazuhiro Fukui"
    code: "https://github.com/Dominoer/bmvc2020_image_harmonization"
    keywords:
      - word: "image harmonization"
      - word: "feature map modulation"
      - word: "attention"
    paper: "papers/0121.pdf"
    supp: "supp/0121_supp.zip"
    abstract: "We present a learning-based approach for image harmonization, which allows for adjusting the appearance of the foreground to make it compatible with background. We consider improving the realism by adjusting the high-level feature statistics of the foreground according to those of the background, which is motivated by the fact that specific image statistics between the foreground and background typically match in realistic composite images. Based on a fully convolutional network, we propose a novel attention-based module that aligns the standard deviation of the foreground features with that of the background features, capturing global dependencies in the entire image. This module is easily inserted into any convolutional neural networks, and allows improving the harmony of the composites with only a small additional computational cost. Experimental results on the image harmonization dataset and real composite images show that our method outperforms existing methods both quantitatively and qualitatively. Furthermore, in our experiment, our module is able to boost existing harmonization networks by simply inserting it into intermediate layers of those networks."
  - id: 122
    order: 56
    oral_session: 4
    poster_session: 2
    session_id: 4
    title: "DESC: Domain Adaptation for Depth Estimation via Semantic Consistency"
    authors:
      - author: "Adrian Lopez-Rodriguez (Imperial College London)"
      - author: "Krystian Mikolajczyk (Imperial College London)"
    all_authors: "Adrian Lopez-Rodriguez and Krystian Mikolajczyk"
    code: "https://github.com/alopezgit/DESC"
    keywords:
      - word: "domain adaptation"
      - word: "depth estimation"
      - word: "monocular"
      - word: "depth"
      - word: "domain"
      - word: "KITTI"
      - word: "Virtual KITTI"
      - word: ""
    paper: "papers/0122.pdf"
    supp: ""
    abstract: "Accurate real depth annotations are difficult to acquire, needing the use of special devices such as a LiDAR sensor. Self-supervised methods try to overcome this problem by processing video or stereo sequences, which may not always be available. Instead, in this paper, we propose a domain adaptation approach to train a monocular depth estimation model using a fully-annotated source dataset and a non-annotated target dataset. We bridge the domain gap by leveraging semantic predictions and low-level edge features to provide guidance for the target domain. We enforce consistency between the main model and a second model trained with semantic segmentation and edge maps, and introduce priors in the form of instance heights. Our approach is evaluated on standard domain adaptation benchmarks for monocular depth estimation and show consistent improvement upon the state-of-the-art.
"
  - id: 126
    order: 12
    poster_session: 1
    session_id: 2
    title: "High-speed Light-weight CNN Inference via Strided Convolutions on a Pixel Processor Array"
    authors:
      - author: "Yanan Liu (University of Bristol)"
      - author: "Laurie Bose (University of Bristol)"
      - author: "Jianing Chen (The University of Manchester)"
      - author: "Stephen Carey (The University of Manchester)"
      - author: "Piotr Dudek (School of Electrical and Electronic Engineering, The University of Manchester, UK)"
      - author: "Walterio Mayol-Cuevas (Bristol University)"
    all_authors: "Yanan Liu, Laurie Bose, Jianing Chen, Stephen Carey, Piotr Dudek and Walterio Mayol-Cuevas"
    code: "https://github.com/yananliusdu/scamp/tree/master"
    keywords:
      - word: "Binary CNN"
      - word: "CNN on embedded system"
      - word: "Pixel Processor Array"
      - word: "SCAMP"
      - word: "high-speed CNN"
      - word: "Light-weight CNN"
    paper: "papers/0126.pdf"
    supp: ""
    abstract: "Performance, storage, and power consumption are three major factors that restrict the use of machine learning algorithms on embedded systems. However, new hardware architectures designed with visual computation in mind may hold the key to solving these bottlenecks. This work makes use of a novel visual device: the pixel processor array (PPA), to embed a convolutional neural network (CNN) onto the focal plane. We present a new high-speed implementation of strided convolutions using binary weights for the CNN on PPA devices, allowing all multiplications to be replaced by more efficient addition/subtraction operations. Image convolutions, ReLU activation functions, max-pooling and a fully-connected layer are all performed directly on the PPA’s imaging plane, exploiting its massive parallel computing capabilities. We demonstrate CNN inference across 4 different applications, running between 2,000 and 17,500 fps with power consumption lower than 1.5W. These tasks include identifying 8 classes of plankton, hand gesture classification and digit recognition."
  - id: 129
    order: 28
    poster_session: 1
    session_id: 2
    title: "ASAP-Net: Attention and Structure Aware Point Cloud Sequence Segmentation"
    authors:
      - author: "Hanwen Cao (Shanghai Jiao Tong University)"
      - author: "Yongyi Lu (Johns Hopkins University)"
      - author: "Bo Pang (Shanghai Jiao Tong University)"
      - author: "Cewu Lu (Shanghai Jiao Tong University)"
      - author: "Alan Yuille (Johns Hopkins University)"
      - author: "Gongshen Liu (Shanghai Jiao Tong University)"
    all_authors: "Hanwen Cao, Yongyi Lu, Bo Pang, Cewu Lu, Alan Yuille and Gongshen Liu"
    code: "https://github.com/intrepidChw/ASAP-Net"
    keywords:
      - word: "Point Cloud Sequence"
      - word: "Semantic Segmentation"
    paper: "papers/0129.pdf"
    supp: ""
    abstract: "Recent works of point clouds show that mulit-frame spatio-temporal modeling outperforms single-frame versions by utilizing cross-frame information. In this paper, we further improve spatio-temporal point cloud feature learning with a flexible module called ASAP considering both attention and structure information across frames, which we find as two important factors for successful segmentation in dynamic point clouds. Firstly, our ASAP module contains a novel attentive temporal embedding layer to fuse the relatively informative local features across frames in a recurrent fashion. Secondly, an efficient spatio-temporal correlation method is proposed to exploit more local structure for embedding, meanwhile enforcing temporal consistency and reducing computation complexity. Finally, we show the generalization ability of the proposed ASAP module with different backbone networks for point cloud sequence segmentation. Our ASAP-Net (backbone plus ASAP module) outperforms baselines and previous methods on both Synthia and SemanticKITTI datasets (+3.4 to +15.2 mIoU points with different backbones). The source codes will be made publicly available."
  - id: 130
    order: 153
    oral_session: 8
    poster_session: 4
    session_id: 10
    title: "Procedure Completion by Learning from Partial Summaries"
    authors:
      - author: "Ehsan Elhamifar (Northeastern University)"
      - author: "Zwe Naing (Northeastern University)"
    all_authors: "Ehsan Elhamifar and Zwe Naing"
    code: ""
    keywords:
      - word: "procedure learning"
      - word: "instructional videos"
      - word: "summarization"
      - word: "subset selection"
      - word: "representation learning"
      - word: "partial summaries"
    paper: "papers/0130.pdf"
    supp: ""
    abstract: "We address the problem of procedure completion in videos, which is to find and localize all key-steps of a task given only a small observed subset of key-steps. We cast the problem as learning summarization from partial summaries that allows to incorporate prior knowledge and learn from incomplete key-steps. Given multiple pairs of (video, subset of key-steps), we address the problem by learning representations of input data and finding the remaining key-steps that generalizes well to key-step discovery in new videos. We propose a loss function on the parameters of a network that promotes to recover unseen key-steps that together with the observed key-steps optimize a desired subset selection criterion. To tackle the highly non-convex learning problem, involving both discrete and continuous variables, we develop an efficient learning algorithm that alternates between representation learning and recovering unseen key-steps while incorporating prior knowledge, via a greedy algorithm. By extensive experiments on two instructional video datasets, we show the effectiveness of our framework."
  - id: 131
    order: 129
    poster_session: 3
    session_id: 8
    title: "M2KD: Incremental Learning via Multi-model and Multi-level Knowledge Distillation"
    authors:
      - author: "Peng Zhou (University of Maryland)"
      - author: "Long Mai (Adobe Research)"
      - author: "Jianming Zhang (Adobe Research)"
      - author: "Ning Xu (Adobe Research)"
      - author: "Zuxuan Wu (UMD)"
      - author: "Larry Davis (University of Maryland)"
    all_authors: "Peng Zhou, Long Mai, Jianming Zhang, Ning Xu, Zuxuan Wu and Larry Davis"
    code: ""
    keywords:
      - word: "Incremental learning"
      - word: "Knowledge distillation"
      - word: "Pruning"
    paper: "papers/0131.pdf"
    supp: "supp/0131_supp.pdf"
    abstract: "Incremental learning targets at achieving good performance on new categories without forgetting old ones. Knowledge distillation has been shown critical in preserving the performance on old classes. Conventional methods, however, sequentially distill knowledge only from the last model, leading to performance degradation on the old classes in later incremental learning steps. In this paper, we propose a multi-model and multi-level knowledge distillation strategy. Instead of sequentially distilling knowledge only from the last model, we directly leverage all previous model snapshots. In addition, we incorporate an auxiliary distillation to further preserve knowledge encoded at the intermediate feature levels. To make the model more memory efficient, we adapt mask based pruning to reconstruct all previous models with a small memory footprint. Experiments on standard incremental learning benchmarks show that our method improves the overall performance over standard distillation techniques."
  - id: 139
    order: 103
    oral_session: 6
    poster_session: 3
    session_id: 7
    title: "BriNet: Towards Bridging the Intra-class and Inter-class Gaps in One-Shot Segmentation"
    authors:
      - author: "Xianghui Yang (The University of Sydney)"
      - author: "Bairun Wang (SenseTime Group Limited)"
      - author: "Xinchi Zhou (The University of Sydney)"
      - author: "Kaige Chen (SenseTime Group Limited)"
      - author: "Shuai Yi (SenseTime Group Limited)"
      - author: "Wanli Ouyang (The University of Sydney)"
      - author: "Luping Zhou (University of Sydney)"
    all_authors: "Xianghui Yang, Bairun Wang, Xinchi Zhou, Kaige Chen, Shuai Yi, Wanli Ouyang and Luping Zhou"
    code: "https://github.com/Wi-sc/BriNet"
    keywords:
      - word: "Few-shot Semantic Segmentation"
      - word: "Few-shot learning"
      - word: "Semantic Segmentation"
    paper: "papers/0139.pdf"
    supp: ""
    abstract: "Few-shot segmentation focuses on the generalization of models to segment unseen object instances with limited training samples. Although tremendous improvements have been achieved, existing methods are still constrained by two factors. (1) The information interaction between query and support images is not adequate, leaving intra-class gap. (2) The object categories at the training and inference stages have no overlap, leaving the inter-class gap. Thus, we propose a framework, BriNet, to bridge these gaps. First, more information interactions are encouraged between the extracted features of the query and support images, i.e., using an Information Exchange Module to emphasize the common objects. Furthermore, to precisely localize the query objects, we design a multi-path fine-grained strategy which is able to make better use of the support feature representations. Second, a new online refinement strategy is proposed to help the trained model adapt to unseen classes, achieved by switching the roles of the query and the support images at the inference stage. The effectiveness of our framework is demonstrated by experimental results, which outperforms other competitive methods and leads to a new state-of-the-art on both PASCAL VOC and MSCOCO dataset."
  - id: 145
    order: 95
    poster_session: 2
    session_id: 5
    title: "Towards Fast and Light-Weight Restoration of Dark Images"
    authors:
      - author: "mohit lamba (Indian Institute of Technology Madras)"
      - author: "Atul Balaji (Indian Institute of Technology Madras)"
      - author: "Kaushik Mitra (IIT Madras)"
    all_authors: "Mohit Lamba, Atul Balaji and Kaushik Mitra"
    code: "https://github.com/MohitLamba94/LLPackNet"
    keywords:
      - word: "low light"
      - word: "low-light"
      - word: "dark image"
      - word: "image enhancement"
      - word: "low level"
      - word: "fast"
      - word: "image restoration"
      - word: ""
    paper: "papers/0145.pdf"
    supp: "supp/0145_supp.zip"
    abstract: "The ability to capture good quality images in the dark and textit{near-zero lux} conditions has been a long-standing pursuit of the computer vision community. The seminal work by Chen etal cite{chen2018learning} has especially caused renewed interest in this area, resulting in methods that build on top of their work in a bid to improve the reconstruction. However, for practical utility and deployment of low-light enhancement algorithms on edge devices such as embedded systems, surveillance cameras, autonomous robots and smartphones, the solution must respect additional constraints such as limited GPU memory and processing power. With this in mind, we propose a deep neural network architecture that aims to strike a balance between the network latency, memory utilization, model parameters, and reconstruction quality. 
The key idea is to forbid any computation in the High-Resolution (HR) space and instead restrict most of the computations to Low-Resolution (LR) space. However, doing the bulk of computations in the LR space causes a lot of artifacts in the restored image. 
We propose textit{Pack} and textit{UnPack} operations, which allow us to effectively transit between the HR and LR spaces without incurring much artifacts in the restored image.
Most of the state-of-the-art algorithms on dark image enhancement need to pre-amplify the image before processing it. However, they generally use ground truth information to find the amplification factor even during inference,  
which restricts their applicability for unknown scenes. In contrast, we propose a simple yet effective light-weight mechanism for automatically determining the amplification factor from the input image itself.
We show that we can enhance a full resolution, $2848 times 4256$, extremely dark single-image in the ballpark of $3$ seconds even on a CPU. We achieve this with $2-7times$ fewer model parameters, $2-3times$ lower memory utilization, $5-20times$ speed up and yet maintain a competitive image reconstruction quality compared to the current state-of-the-art algorithms."
  - id: 149
    order: 88
    poster_session: 2
    session_id: 5
    title: "Inducing Predictive Uncertainty Estimation for Face Verification"
    authors:
      - author: "Weidi Xie (University of Oxford)"
      - author: "Jeffrey Byrne (STR"
      - author: "Visym Labs)"
      - author: "Andrew Zisserman (University of Oxford)"
    all_authors: "Weidi Xie, Jeffrey Byrne (STR, Visym Labs) and Andrew Zisserman"
    code: ""
    keywords:
      - word: "Face Recognition"
      - word: "Quality Estimation"
      - word: "Uncertainty Estimation"
      - word: "Explainable AI"
    paper: "papers/0149.pdf"
    supp: ""
    abstract: "
Knowing when an output can be trusted is critical for reliably using face recognition systems. While there has been enormous effort in recent research on improving face verification performance, understanding when a model's predictions should or should not be trusted has received far less attention. 

Our goal is a method can predict a confidence score for a face image that reflects
its quality in terms of recognizable information.
To this end, we propose a method for generating image quality training data
automatically from `mated-pairs' of face images, and use the generated data to train a lightweight Predictive Confidence Network, termed as PCNet, 
for estimating the confidence score of a face image.
We systematically evaluate the usefulness of PCNet using its error versus reject performance, 
and demonstrate that it can be universally paired with and improve the robustness of any verification model.  
We describe three use cases on the public IJB-C face verification benchmark: 
(i) to  improve 1:1 image-based verification error rates by rejecting low-quality face
images; 
(ii) to improve quality score based fusion performance on the 1:1 set-based
verification benchmark; 
and (iii) its use as a quality measure for selecting high quality (unblurred, good lighting, 
more frontal) faces from a collection, e.g. for automatic enrolment or display."
  - id: 151
    order: 112
    poster_session: 3
    session_id: 8
    title: "Explicit Residual Descent for 3D Human Pose Estimation from 2D Joint Locations"
    authors:
      - author: "Yangyuxuan Kang (Institute of Software Chinese Academy of Sciences)"
      - author: "Anbang Yao (Intel Labs China)"
      - author: "Shandong Wang (Intel Labs China)"
      - author: "Ming Lu (Intel Labs China)"
      - author: "Yurong Chen (Intel Labs China)"
      - author: "Enhua Wu (CAS)"
    all_authors: "Yangyuxuan Kang, Anbang Yao, Shandong Wang, Ming Lu, Yurong Chen and Enhua Wu"
    code: "Code will be made publicly available later."
    keywords:
      - word: "3D human pose estimation"
      - word: "pose lifting network"
      - word: "feedback optimization"
      - word: "deep neural network"
      - word: "supervised learning"
    paper: "papers/0151.pdf"
    supp: ""
    abstract: "Recent studies show that the end-to-end learning paradigm based on well-designed lifting networks merely using 2D joint locations as the input can achieve impressive performance in handling 3D human pose estimation problem. However, in the viewpoint of optimization design, existing methods of this category have two drawbacks: (1) The inherent feature relation between the 2D pose input and the corresponding 3D pose estimate is not sufficiently explored. (2) The regression procedure is usually performed in a one-step manner. To address these two issues, this paper proposes an efficient yet accurate method called Explicit Residual Descent (ERD). Given an arbitrary lifting network which takes 2D joint locations in a single image as the input and generates an initial 3D pose estimate, our ERD learns a sequence of descent directions encoded with a shared lightweight differentiable structure, progressively refining the previous 3D pose estimate via adding in a 3D increment obtained from projecting the reconstructed 2D pose features onto each learnt descent direction. Extensive experiments on public benchmarks including Human3.6M dataset validate the superior performance of the proposed method against state-of-the-art methods. Code will be made publicly available."
  - id: 154
    order: 158
    poster_session: 4
    session_id: 11
    title: "How to Train Your Energy-Based Model for Regression"
    authors:
      - author: "Fredrik Gustafsson (Uppsala University)"
      - author: "Martin Danelljan (ETH Zurich)"
      - author: "Radu Timofte (ETH Zurich)"
      - author: "Thomas Schön (Uppsala University)"
    all_authors: "Fredrik Gustafsson, Martin Danelljan, Radu Timofte and Thomas Schön"
    code: "https://github.com/fregu856/ebms_regression"
    keywords:
      - word: "energy-based models"
      - word: "regression"
      - word: "visual tracking"
      - word: "object detection"
      - word: "noise contrastive estimation"
    paper: "papers/0154.pdf"
    supp: "supp/0154_supp.pdf"
    abstract: "Energy-based models (EBMs) have become increasingly popular within computer vision in recent years. While they are commonly employed for generative image modeling, recent work has applied EBMs also for regression tasks, achieving state-of-the-art performance on object detection and visual tracking. Training EBMs is however known to be challenging. While a variety of different techniques have been explored for generative modeling, the application of EBMs to regression is not a well-studied problem. How EBMs should be trained for best possible regression performance is thus currently unclear. We therefore accept the task of providing the first detailed study of this problem. To that end, we propose a simple yet highly effective extension of noise contrastive estimation, and carefully compare its performance to six popular methods from literature on the tasks of 1D regression and object detection. The results of this comparison suggest that our training method should be considered the go-to approach. We also apply our method to the visual tracking task, achieving state-of-the-art performance on five datasets. Notably, our tracker achieves 63.7% AUC on LaSOT and 78.7% Success on TrackingNet. Code is available at https://github.com/fregu856/ebms_regression."
  - id: 160
    order: 100
    oral_session: 5
    poster_session: 3
    session_id: 6
    title: "Deep Sparse Light Field Refocusing"
    authors:
      - author: "Shachar Ben Dayan (Tel Aviv University)"
      - author: "David Mendlovic (Tel Aviv University)"
      - author: "Raja Giryes (Tel Aviv University)"
    all_authors: "Shachar Ben Dayan, David Mendlovic and Raja Giryes"
    code: ""
    keywords:
      - word: "Light Field"
      - word: "Refocusing"
      - word: "Denoising"
      - word: "Convolutional Neural Networks"
    paper: "papers/0160.pdf"
    supp: "supp/0160_supp.pdf"
    abstract: "Light field photography enables to record 4D images, containing angular information alongside spatial information of the scene. One of the important applications of light field imaging is post-capture refocusing. Current methods require for this purpose a dense field of angle views; those can be acquired with a micro-lens system or with a compressive system. Both techniques have major drawbacks to consider, including bulky structures and angular-spatial resolution trade-off. We present a novel implementation of digital refocusing based on sparse angular information using neural networks. This allows recording high spatial resolution in favor of the angular resolution, thus, enabling to design compact and simple devices with improved hardware as well as better performance of compressive systems.  We use a novel convolutional neural network whose relatively small structure enables fast reconstruction with low memory consumption. Moreover, it allows handling without re-training various refocusing ranges and noise levels. Results show major improvement compared to existing methods."
  - id: 161
    order: 40
    poster_session: 1
    session_id: 2
    title: "Visualizing point cloud classifiers by curvature smoothing"
    authors:
      - author: "Chen Ziwen (Grinnell College)"
      - author: "Wenxuan Wu (Oregon State University)"
      - author: "Zhongang Qi (Tencent)"
      - author: "Li Fuxin (Oregon State University)"
    all_authors: "Chen Ziwen, Wenxuan Wu, Zhongang Qi and Li Fuxin"
    code: "https://github.com/arthurhero/PC-IGOS"
    keywords:
      - word: "point cloud"
      - word: "visualization"
      - word: "explainable AI"
      - word: "pointconv"
      - word: ""
    paper: "papers/0161.pdf"
    supp: "supp/0161_supp.pdf"
    abstract: "Recently, several networks that operate directly on point clouds have been proposed. There is significant utility in understanding their mechanisms to classify point clouds, which can potentially help diagnosing them and designing better architectures.  In this paper, we propose a novel learning-based approach to visualize features important to the point cloud classifiers. Our approach is based on deleting and inserting curvatures on a point cloud. The resulting point cloud is then evaluated on the original point cloud network to assess the importance of the feature. A technical contribution of the paper is an approximated curvature smoothing algorithm, which can smoothly transition from the original point cloud to one of constant curvature, such as a uniform sphere. We propose PCI-GOS (Point Cloud Integrated-Gradients Optimized Saliency), a visualization technique that can automatically find the minimal saliency map that covers the most important features on a shape. Experiment results revealed insights into those classifiers."
  - id: 166
    order: 29
    poster_session: 1
    session_id: 2
    title: "Unsupervised Domain Adaptation for Spatio-Temporal Action Localization"
    authors:
      - author: "Nakul Agarwal (UC Merced)"
      - author: "Yi-Ting Chen (Honda Research Institute USA)"
      - author: "Behzad Dariush (Honda Research Institute US)"
      - author: "Ming-Hsuan Yang (University of California at Merced)"
    all_authors: "Nakul Agarwal, Yi-Ting Chen, Behzad Dariush and Ming-Hsuan Yang"
    code: ""
    keywords:
      - word: "Spatio-Temporal Action Localization"
      - word: "Unsupervised Domain Adaptation"
      - word: "Adversarial Learning"
      - word: "Video Analysis"
      - word: "Deep Learning"
    paper: "papers/0166.pdf"
    supp: "supp/0166_supp.zip"
    abstract: "Spatio-temporal action localization is an important problem in computer vision that involves detecting where and when activities occur, and therefore requires modeling of both spatial and temporal features. This problem is typically formulated in the context of supervised learning, where the learned classifiers operate on the premise that both training and test data are sampled from the same underlying distribution. However, this assumption does not hold when there is a significant domain shift, leading to poor generalization performance on the test data. To address this, we focus on the hard and novel task of generalizing training models to test samples without access to any labels from the latter for spatio-temporal action localization by proposing an end-to-end unsupervised domain adaptation algorithm. We extend the state-of-the-art object detection framework to localize and classify actions. In order to minimize the domain shift, three domain adaptation modules at image level (temporal and spatial) and instance level (temporal) are designed and integrated. We design a new experimental setup and evaluate the proposed method and different adaptation modules on the UCF-Sports, UCF-101 and JHMDB benchmark datasets. We show that significant performance gain can be achieved when spatial and temporal features are adapted separately, or jointly for the most effective results."
  - id: 167
    order: 133
    poster_session: 3
    session_id: 8
    title: "Integrating Long-Short Term Network for Efficient Video Object Segmentation"
    authors:
      - author: "Jingjing Wang (BJTU)"
      - author: "Zhu  Teng (Beijing Jiaotong University)"
      - author: "Baopeng Zhang (BJTU)"
      - author: "Jianping Fan (UNCC)"
    all_authors: "Jingjing Wang, Zhu  Teng, Baopeng Zhang and Jianping Fan"
    code: ""
    keywords:
      - word: "Video Object Segmentation"
      - word: "Long-Short Term Network"
      - word: "Multiple-object segmentation"
    paper: "papers/0167.pdf"
    supp: ""
    abstract: "Real-world application of video object segmentation (VOS) is a very challenging problem, especially for multiple video object segmentation. The deep-learning-based approaches have recently dominated VOS by fine-tuning the networks at the first frame to seize the object dynamics, but they may result in impractical frame-rates and risk of over-fitting. To overcome this limitation, we develop an efficient and fully end-to-end model to achieve fast and accurate VOS, named Long-Short Term Network (LSTNet). It contains a long term network to encode absolute object variations and a short term network to capture relative object dynamics. The segmentation results of video objects can be directly acquired by an attentional gate operation based on these two networks. Our proposed model runs at a very high speed and can conveniently tackle multi-object segmentation without post-processing. Extensive experiments on widely used benchmarks including YouTube-VOS and DAVIS 2017 have demonstrated that our proposed model can achieve a competitive accuracy and speed in comparison to a number of state-of-the-art methods.
"
  - id: 174
    order: 46
    poster_session: 1
    session_id: 2
    title: "Boosting Image and Video Compression via Learning Latent Residual Patterns"
    authors:
      - author: "Yen-Chung Chen (National Chiao Tung University)"
      - author: "Keng-Jui Chang (National Chiao Tung University)"
      - author: "Yi-Hsuan Tsai (NEC Labs America)"
      - author: "Wei-Chen Chiu (National Chiao Tung University)"
    all_authors: "Yen-Chung Chen, Keng-Jui Chang, Yi-Hsuan Tsai and Wei-Chen Chiu"
    code: ""
    keywords:
      - word: "compression artifacts"
      - word: "image compression"
      - word: "video compression"
      - word: "latent residual"
      - word: ""
    paper: "papers/0174.pdf"
    supp: ""
    abstract: "Reducing compression artifacts is essential for streaming videos with a better quality under a limited bandwidth. To tackle this problem, existing methods aim to directly recover details from the compressed video but do not consider learning rich information in uncompressed videos to aid this process. In this paper, we focus on utilizing the residual information, which is the difference between a compressed video and its corresponding original/uncompressed one, and propose a fairly efficient way to transmit the residual with the compressed video in order to boost the quality of video compression. Our proposed method is realized by learning to discover the patterns in the residual and storing them offline as dictionary-like patterns. During the testing stage, e.g., for video streaming, the residual is transmitted in the form of pattern indexes to reduce the cost of communication, and thus the original residual information can be easily retrieved back from the dictionary of learned residual patterns. We show the effectiveness of our framework on numerous datasets under various video compression coding methods. In addition, the proposed pipeline can be widely applied to the image compression task and reduce artifacts produced from conventional and CNN-based compression algorithms."
  - id: 176
    order: 35
    poster_session: 1
    session_id: 2
    title: "The Resistance to Label Noise in K-NN and DNN Depends on its Concentration"
    authors:
      - author: "Amnon Drory (Tel-Aviv University)"
      - author: "Oria Ratzon (Tel Aviv University)"
      - author: "Shai Avidan (Tel Aviv University)"
      - author: "Raja Giryes (Tel Aviv University)"
    all_authors: "Amnon Drory, Oria Ratzon, Shai Avidan and Raja Giryes"
    code: "https://github.com/AmnonDrory/CNNs-With-Label-Noise"
    keywords:
      - word: "Deep Neural Networks"
      - word: "K-NN"
      - word: "Label Noise"
      - word: "Randomly Spread Noise"
      - word: "Locally Concentrated Noise"
      - word: "Noise Models"
      - word: "Noise Types"
      - word: ""
    paper: "papers/0176.pdf"
    supp: "supp/0176_supp.zip"
    abstract: "We investigate the classification performance of K-nearest neighbors (K-NN) and deep neural networks (DNNs) in the presence of label noise. We first show empirically that a DNN’s prediction for a given test example depends on the labels of the training examples in its local neighborhood. This motivates us to derive a realizable analytic expression that approximates the multi-class K-NN classification error in the presence of label noise, which is of independent importance. We then suggest that the expression for K-NN may serve as a first-order approximation for the DNN error. Finally, we demonstrate empirically the proximity of the developed expression to the observed performance of K-NN and DNN classifiers. Our result may explain the already observed surprising resistance of DNN to some types of label noise. It also characterizes an important factor of it, showing that the more concentrated the noise the greater is the degradation in performance."
  - id: 184
    order: 53
    oral_session: 3
    poster_session: 2
    session_id: 3
    title: "Bipartite Conditional Random Fields for Panoptic Segmentation"
    authors:
      - author: "Sadeep Jayasumana (University of Oxford)"
      - author: "Kanchana  Ranasinghe (University of Moratuwa)"
      - author: "Sahan Liyanaarachchi (University of Moratuwa)"
      - author: "Bethmage Mayuka Jayawardhana (University of Moratuwa)"
      - author: "Harsha Ranasinghe (University of Moratuwa)"
      - author: "Sina Samangooei (Five AI Ltd.)"
    all_authors: "Sadeep Jayasumana, Kanchana  Ranasinghe, Sahan Liyanaarachchi, Bethmage Mayuka Jayawardhana, Harsha Ranasinghe and Sina Samangooei"
    code: "https://github.com/sahan-liyanaarachchi/bcrf-detectron"
    keywords:
      - word: "conditional random fields"
      - word: "panoptic segmentation"
      - word: "deep learning"
      - word: ""
    paper: "papers/0184.pdf"
    supp: "supp/0184_supp.pdf"
    abstract: " We tackle the panoptic segmentation problem with a conditional random field (CRF) model. Panoptic segmentation involves assigning a semantic label and an instance label to each pixel of a given image. At each pixel, the semantic label and the instance label should be compatible. Furthermore, a good panoptic segmentation should have a number of other desirable properties such as the spatial and color consistency of the labeling. To tackle this problem, we propose a CRF model, named Bipartite CRF or BCRF, with two types of random variables for semantic and instance labels. In this formulation, various energies are defined within and across the two types of random variables to encourage a consistent panoptic segmentation. We propose a mean-field-based efficient inference algorithm for solving the CRF and empirically show its convergence properties. This algorithm is fully differentiable, and therefore, BCRF inference can be included as a trainable module in any deep network. In the experimental evaluation, we quantitatively and qualitatively show that the BCRF yields superior panoptic segmentation results in practice."
  - id: 186
    order: 23
    poster_session: 1
    session_id: 2
    title: "Adaptation Across Extreme Variations using Unlabeled Bridges"
    authors:
      - author: "Shuyang Dai (Duke University)"
      - author: "Kihyuk Sohn (Google)"
      - author: "Yi-Hsuan Tsai (NEC Labs America)"
      - author: "Lawrence Carin Duke (CS)"
      - author: "Manmohan Chandraker (NEC Labs America)"
    all_authors: "Shuyang Dai, Kihyuk Sohn, Yi-Hsuan Tsai, Lawrence Carin Duke and Manmohan Chandraker"
    code: ""
    keywords:
      - word: "domain adaptation"
      - word: "image recognition"
      - word: "deep learning"
      - word: ""
    paper: "papers/0186.pdf"
    supp: "supp/0186_supp.zip"
    abstract: "We tackle an unsupervised domain adaptation problem for which the domain discrepancy between labeled source and unlabeled target domains is large, due to many factors of inter- and intra-domain variation. While deep domain adaptation methods have been realized by reducing the domain discrepancy, these are difficult to apply when domains are significantly different. We propose to decompose domain discrepancy into multiple but smaller, and thus easier to minimize, discrepancies by introducing unlabeled bridging domains that connect the source and target domains. We realize our proposed approach through an extension of the domain adversarial neural network with multiple discriminators, each of which accounts for reducing discrepancies between unlabeled (bridge, target) domains and a mix of all precedent domains including source. We validate the effectiveness of our method on several adaptation tasks including object recognition and semantic segmentation."
  - id: 187
    order: 50
    poster_session: 1
    session_id: 2
    title: "Attentive Action and Context Factorization"
    authors:
      - author: "Yang Wang (Stony Brook University)"
      - author: "Vinh Tran (Stony Brook University)"
      - author: "Gedas Bertasius (Facebook AI)"
      - author: "Lorenzo Torresani (Dartmouth College)"
      - author: "Minh Hoai Nguyen (Stony Brook University)"
    all_authors: "Yang Wang, Vinh Tran, Gedas Bertasius, Lorenzo Torresani and Minh Hoai Nguyen"
    code: ""
    keywords:
      - word: "action factorization"
      - word: "attention"
      - word: "conjugate samples"
    paper: "papers/0187.pdf"
    supp: ""
    abstract: "We propose a method for human action recognition, one that can localize the spatiotemporal regions that `define' the actions. This is a challenging task due to the subtlety of human actions in video and the co-occurrence of contextual elements. To address this challenge, we utilize conjugate samples of human actions, which are video clips that are contextually similar to human action samples but do not contain the action. We introduce a novel attentional mechanism that can spatially and temporally separate human actions from the co-occurring contextual factors. The separation of action and context factors is weakly supervised, eliminating the need for laboriously detailed annotation of these two factors in training samples. Our method can build human action classifiers with higher accuracy and better interpretability. Experiments on several human action recognition datasets demonstrate the quantitative and qualitative benefits of our approach."
  - id: 191
    order: 136
    poster_session: 3
    session_id: 8
    title: "Residual Likelihood Forests"
    authors:
      - author: "Yan Zuo (Monash University)"
      - author: "Tom Drummond (Monash University)"
    all_authors: "Yan Zuo and Tom Drummond"
    code: ""
    keywords:
      - word: "Random Forests"
      - word: "Boosting"
      - word: "Classification"
      - word: "Ensemble Methods"
      - word: "Decision Forests"
      - word: "Decision Trees"
      - word: ""
    paper: "papers/0191.pdf"
    supp: ""
    abstract: "This paper presents a novel ensemble learning approach called Residual Likelihood Forests (RLF). Our weak learners produce conditional likelihoods that are sequentially optimized using global loss in the context of previous learners within a boosting-like framework (rather than probability distributions that are measured from observed data) and are combined multiplicatively (rather than additively). This increases the efficiency of our strong classifier, allowing for the design of classifiers which are more compact in terms of model capacity. We apply our method to several machine learning classification tasks, showing significant improvements in performance. When compared against several ensemble approaches including Random Forests and Gradient Boosted Trees, RLFs offer a significant improvement in performance whilst concurrently reducing the required model size."
  - id: 193
    order: 15
    poster_session: 1
    session_id: 2
    title: "FairFaceGAN: Fairness-aware Facial Image-to-Image Translation"
    authors:
      - author: "sunhee hwang (Yonsei university)"
      - author: "Sungho Park (Yonsei University)"
      - author: "Dohyung Kim (Yonsei University)"
      - author: "Mirae Do (Yonsei University)"
      - author: "Hyeran Byun (Yonsei University)"
    all_authors: "Sunhee Hwang, Sungho Park, Dohyung Kim, Mirae Do and Hyeran Byun"
    code: ""
    keywords:
      - word: "fairness in computer vision"
      - word: "image-to-image translation"
      - word: "equality of opportunity"
      - word: "equalized odds"
    paper: "papers/0193.pdf"
    supp: "supp/0193_supp.pdf"
    abstract: "In this paper, we introduce FairFaceGAN, a fairness-aware facial Image-to-Image translation model, mitigating the problem of unwanted translation in protected attributes (e.g., gender, age, race) during facial attributes editing. Unlike existing models, FairFaceGAN learns fair representations with two separate latents - one related to the target attributes to translate, and the other unrelated to them. This strategy enables FairFaceGAN to separate the information about protected attributes and that of target attributes. It also prevents unwanted translation in protected attributes while target attributes editing. To evaluate the degree of fairness, we perform two types of experiments on CelebA dataset. First, we compare the fairness-aware classification performances when augmenting data by existing image translation methods and FairFaceGAN respectively. Moreover, we propose a new fairness metric, namely Fréchet Protected Attribute Distance (FPAD), which measures how well protected attributes are preserved. Experimental results demonstrate that FairFaceGAN shows consistent improvements in terms of fairness over the existing image translation models. Further, we also evaluate image translation performances, where FairFaceGAN shows competitive results, compared to those of existing methods.
"
  - id: 195
    order: 192
    poster_session: 4
    session_id: 11
    title: "Unified Representation Learning for Cross Model Compatibility"
    authors:
      - author: "Chien-Yi Wang (Microsoft)"
      - author: "Ya-Liang Chang (National Taiwan University)"
      - author: "Shang-Ta Yang (Microsoft)"
      - author: "Dong Chen (Microsoft Research Asia)"
      - author: "Shang-Hong Lai  (Microsoft)"
    all_authors: "Chien-Yi Wang, Ya-Liang Chang, Shang-Ta Yang, Dong Chen and Shang-Hong Lai"
    code: ""
    keywords:
      - word: "representation learning"
      - word: "metric learning"
      - word: "face recognition"
      - word: "person re-identification"
      - word: "model compatibility"
      - word: "open-set recognition"
      - word: ""
    paper: "papers/0195.pdf"
    supp: ""
    abstract: "We propose a unified representation learning framework to address the Cross Model Compatibility (CMC) problem in the context of visual search applications. Cross-compatibility between different embedding models enables the visual search systems to correctly recognize and retrieve identities without re-encoding user images, which are usually not available due to privacy concerns. While there are existing approaches to address CMC in face identification, they fail to work in a more challenging setting where the distributions of embedding models shift drastically. The proposed solution improves CMC performance by introducing a light-weight Residual Bottleneck Transformation (RBT) module and a new training scheme to optimize the embedding spaces. Extensive experiments demonstrate that our proposed solution outperforms previous approaches by a large margin for various challenging visual search scenarios of face recognition and person re-identification. "
  - id: 196
    order: 118
    poster_session: 3
    session_id: 8
    title: "What do CNNs gain by imitating the visual development of primate infants?"
    authors:
      - author: "Shantanu Jaiswal (Agency for Science, Technology and Research )"
      - author: "Dongkyu Choi (Agency for Science, Technology and Research)"
      - author: "Basura Fernando (Agency for Science, Technology and Research, ASTAR, Singapore)"
    all_authors: "Shantanu Jaiswal, Dongkyu Choi and Basura Fernando"
    code: ""
    keywords:
      - word: "Biologically inspired vision"
      - word: "Primate visual development"
      - word: "Greedy layer-wise training"
      - word: "Supervised training approaches"
    paper: "papers/0196.pdf"
    supp: "supp/0196_supp.zip"
    abstract: "Deep convolutional neural networks have emerged as strong candidates for a model of human vision, often outperforming competing models on both computer vision benchmarks and computational neuroscience benchmarks of neural response correspondence. The design of these models has undergone several refinements in recent years drawing on both statistical and cognitive insights and, in the process, shown increasing correspondence to primate visual processing representations. However, their training methodology still remains in contrast to the process of primate visual development, and we believe that it can benefit from being more aligned with this natural process. Primate visual development is characterized by low visual acuity and colour sensitivity as well as high plasticity and neuronal growth in the first year of infancy, prior to the development of specific visual-cognitive functions such as visual object recognition. In this work, we investigate the synergy between the gradual variation in the distribution of visual input and the concurrent growth of a statistical model of vision on the task of large-scale object classification, and discuss how it may yield better approaches to training deep convolutional neural networks. The experiments we performed across multiple object classification benchmarks indicate that a growing statistical model trained with a gradually varying visual input distribution converges to a better generalization at a faster rate than traditional, more static training setups."
  - id: 209
    order: 94
    poster_session: 2
    session_id: 5
    title: "When Humans Meet Machines: Towards Efficient Segmentation Networks"
    authors:
      - author: "Peike Li (UTS)"
      - author: "Xuanyi Dong (University of Technology Sydney)"
      - author: "Xin Yu (University of Technology Sydney)"
      - author: "Yi Yang (UTS)"
    all_authors: "Peike Li, Xuanyi Dong, Xin Yu and Yi Yang"
    code: ""
    keywords:
      - word: "semantic segmentation"
      - word: "efficient network"
      - word: "neural architecture search"
      - word: "network design"
      - word: ""
    paper: "papers/0209.pdf"
    supp: "supp/0209_supp.zip"
    abstract: "In this paper, we investigate how to achieve a high-performance yet lightweight segmentation network for real-time applications. By analyzing three typical segmentation networks, we observe that the segmentation backbones and heads are often imbalanced which restricts network efficiency. Thus, we develop a lightweight context fusion (LCF) module and a lightweight global enhancement (LGE) module to construct our lightweight segmentation head. Specifically, LCF fuses multi-resolution features to capture image details and LGE is designed to enhance feature representations. In this manner, our lightweight head facilities network efficiency and significantly reduces network parameters. Furthermore, we design a Multi-Resolution Macro Segmentation structure (MRMS) to incorporate human knowledge into our network architecture composition. Given the resource-aware constraint (e.g., latency time), we optimize our network with network architecture search while considering the relationships among atomic operators, network depth and feature resolution in segmentation tasks. Since MRMS embeds the segmentation-specific knowledge, it also provides a better architecture search space. Our Human-Machine collaboratively designed Segmentation network (HMSeg) achieves better performance and faster inference speed. Experiments demonstrate that our network achieves 71.4% mean intersection over union (mIOU) on Cityscapes with only 0.7M parameters at 172.4 FPS on NVIDIA GTX1080Ti."
  - id: 211
    order: 186
    poster_session: 4
    session_id: 11
    title: "A Novel Baseline for Zero-shot Learning via Adversarial Visual-Semantic Embedding"
    authors:
      - author: "Yu Liu (KU Leuven)"
      - author: "Tinne Tuytelaars (KU Leuven)"
    all_authors: "Yu Liu and Tinne Tuytelaars"
    code: "https://github.com/Liuy8/AVSE"
    keywords:
      - word: "zero-shot learning"
      - word: "generalized zero-shot learning"
      - word: "visual-semantic embedding"
      - word: "adversarial learning"
      - word: "image synthesis"
      - word: ""
    paper: "papers/0211.pdf"
    supp: "supp/0211_supp.zip"
    abstract: "Zero-shot learning (ZSL) has been attracting ever-increasing research interest due to its capability of recognizing novel or unseen classes.A lot of studies on ZSL are based mainly on two baseline models: compatible visual-semantic embedding (CVSE) and adversarial visual feature generation (AVFG). In this work, we integrate the merits of the two baselines and propose a novel and effective baseline model, coined adversarial visual-semantic embedding (AVSE). Different from CVSE and AVFG, AVSE learns visual and semantic embeddings adversarially and jointly in a latent feature space. Additionally, AVSE integrates a classifier to make latent embeddings discriminative, and a regressor to preserve semantic consistency during the embedding procedure. Moreover, we perform embedding-to-image generation which visually exhibits the embeddings learned in AVSE.The experiments on four standard benchmarks show the advantage of AVSE over CVSE and AVFG, and empirical insights through quantitative and qualitative results. Our code is at https://github.com/Liuy8/AVSE."
  - id: 213
    order: 79
    poster_session: 2
    session_id: 5
    title: "Branched Multi-Task Networks: Deciding what layers to share"
    authors:
      - author: "Simon Vandenhende (KU Leuven)"
      - author: "Stamatios Georgoulis (ETH Zurich)"
      - author: "Luc Van Gool (ETH Zurich)"
      - author: "Bert De Brabandere (KU Leuven)"
    all_authors: "Simon Vandenhende, Stamatios Georgoulis, Luc Van Gool and Bert De Brabandere"
    code: ""
    keywords:
      - word: "multi-task learning"
      - word: "neural architecture search"
      - word: "scene understanding"
      - word: "MTL"
      - word: "efficient"
      - word: "NAS"
      - word: "transfer learning"
      - word: "taskonomy"
      - word: "task affinity"
    paper: "papers/0213.pdf"
    supp: "supp/0213_supp.zip"
    abstract: "In the context of multi-task learning, neural networks with branched architectures have often been employed to jointly tackle the tasks at hand. Such ramified networks typically start with a number of shared layers, after which different tasks branch out into their own sequence of layers. Understandably, as the number of possible network configurations is combinatorially large, deciding what layers to share and where to branch out becomes cumbersome. Prior works have either relied on ad hoc methods to determine the level of layer sharing, which is suboptimal, or utilized neural architecture search techniques to establish the network design, which is considerably expensive. In this paper, we go beyond these limitations and propose an approach to automatically construct branched multi-task networks, by leveraging the employed tasks' affinities. Given a specific budget, i.e. number of learnable parameters, the proposed approach generates architectures, in which shallow layers are task-agnostic, whereas deeper ones gradually grow more task-specific. Extensive experimental analysis across numerous, diverse multi-tasking datasets shows that, for a given budget, our method consistently yields networks with the highest performance, while for a certain performance threshold it requires the least amount of learnable parameters."
  - id: 220
    order: 73
    poster_session: 2
    session_id: 5
    title: "Relational Generalized Few-Shot Learning"
    authors:
      - author: "Xiahan Shi (Bosch Center for Artificial Intelligence)"
      - author: "Leonard Salewski (University of Stuttgart)"
      - author: "Martin Schiegg (Bosch Center for Artificial Intelligence)"
      - author: "Max Welling (University of Amsterdam)"
    all_authors: "Xiahan Shi, Leonard Salewski, Martin Schiegg and Max Welling"
    code: ""
    keywords:
      - word: "few-shot learning"
      - word: "generalized few-shot learning"
      - word: "side information"
      - word: "graph convolution"
      - word: "classification"
      - word: "machine learning"
    paper: "papers/0220.pdf"
    supp: "supp/0220_supp.zip"
    abstract: "Transferring learned models to novel tasks is a challenging problem, particularly if only very few labeled examples are available. Most proposed methods for this few-shot learning setup focus on discriminating novel classes only. Instead, we consider the extended setup of generalized few-shot learning (GFSL), where the model is required to perform classification on the joint label space consisting of both previously seen and novel classes. We propose a graph-based framework that explicitly models relationships between all seen and novel classes in the joint label space. Our model Graph-convolutional Global Prototypical Networks (GcGPN) incorporates these inter-class relations using graph-convolution in order to embed novel class representations into the existing space of previously seen classes in a globally consistent manner. Our approach ensures both fast adaptation and global discrimination, which is the major challenge in GFSL. We demonstrate the benefits of our model on two challenging benchmark datasets."
  - id: 221
    order: 43
    poster_session: 1
    session_id: 2
    title: "Unsupervised Domain Adaptation by Uncertain Feature Alignment"
    authors:
      - author: "Tobias Ringwald (Karlsruhe Institute of Technology)"
      - author: "Rainer Stiefelhagen (Karlsruhe Institute of Technology)"
    all_authors: "Tobias Ringwald and Rainer Stiefelhagen"
    code: "https://gitlab.com/tringwald/ufal"
    keywords:
      - word: "transfer learning"
      - word: "domain adaptation"
      - word: "unsupervised domain adaptation"
      - word: "uncertainty"
      - word: ""
    paper: "papers/0221.pdf"
    supp: "supp/0221_supp.zip"
    abstract: "Unsupervised domain adaptation (UDA) deals with the adaptation of models from a given source domain with labeled data to an unlabeled target domain. In this paper, we utilize the inherent prediction uncertainty of a model to accomplish the domain adaptation task. The uncertainty is measured by Monte-Carlo dropout and used for our proposed Uncertainty-based Filtering and Feature Alignment (UFAL) that combines an Uncertain Feature Loss (UFL) function and an Uncertainty-Based Filtering (UBF) approach for alignment of features in Euclidean space. Our method surpasses recently proposed architectures and achieves state-of-the-art results on multiple challenging datasets. Code is available on the project website."
  - id: 222
    order: 119
    poster_session: 3
    session_id: 8
    title: "SOFA-Net: Second-Order and First-order Attention Network for Crowd Counting"
    authors:
      - author: "Haoran Duan (Newcastle University)"
      - author: "Shidong Wang (Newcastle University)"
      - author: "Yu Guan (Newcastle University)"
    all_authors: "Haoran Duan, Shidong Wang and Yu Guan"
    code: ""
    keywords:
      - word: "Second Order"
      - word: "First Order"
      - word: "Crowd Counting"
    paper: "papers/0222.pdf"
    supp: ""
    abstract: "Automated crowd counting from images/videos has attracted more attention in recent years because of its wide application in smart cities. But modelling the dense crowd heads is challenging and most of the existing works become less reliable. To obtain the appropriate crowd representation, in this work we proposed SOFA-Net(Second-Order and First-order Attention Network): second-order statistics were extracted to retain selectivity of the channel-wise spatial information for dense heads while first-order statistics, which can enhance the feature discrimination for the heads' areas, were used as complementary information. Via a multi-stream architecture, the proposed second/first-order statistics were learned and transformed into attention for robust representation refinement. We evaluated our method on four public datasets and the performance reached state-of-the-art. Extensive experiments were also conducted to study the components in the proposed SOFA-Net, and the results suggested the high-capability of second/first-order statistics on modelling crowd in challenging scenarios. To the best of our knowledge, we are the first work to explore the second/first-order statistics for crowd counting. The source code will be available."
  - id: 223
    order: 113
    poster_session: 3
    session_id: 8
    title: "Adversarial Training for Multi-Channel Sign Language Production"
    authors:
      - author: "Ben Saunders (University of Surrey)"
      - author: "Richard Bowden (University of Surrey)"
      - author: "Necati Cihan Camgoz (University of Surrey)"
    all_authors: "Ben Saunders, Richard Bowden and Necati Cihan Camgoz"
    code: ""
    keywords:
      - word: "Sign Language Production"
      - word: "Adversarial Training"
      - word: "Multi-Channel"
      - word: "Continuous Sequence Synthesis"
      - word: "Human Pose Generation"
      - word: ""
    paper: "papers/0223.pdf"
    supp: "supp/0223_supp.mp4"
    abstract: "Sign Languages are rich multi-channel languages, requiring articulation of both manual (hands) and non-manual (face and body) features in a precise, intricate manner. Sign Language Production (SLP), the automatic translation from spoken to sign languages, must embody this full sign morphology to be truly understandable by the Deaf community. Previous work has mainly focused on manual feature production, with an under-articulated output caused by regression to the mean.

In this paper, we propose an Adversarial Multi-Channel approach to SLP. We frame sign production as a minimax game between a transformer-based Generator and a conditional Discriminator. Our adversarial discriminator evaluates the realism of sign production conditioned on the source text, pushing the generator towards a realistic and articulate output. Additionally, we fully encapsulate sign articulators with the inclusion of non-manual features, producing facial features and mouthing patterns.

We evaluate on the challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset, and report state-of-the art SLP back-translation performance for manual production. We set new benchmarks for the production of multi-channel sign to underpin future research into realistic SLP."
  - id: 226
    order: 111
    poster_session: 3
    session_id: 8
    title: "ALBA: Reinforcement Learning for Video Object Segmentation"
    authors:
      - author: "Shreyank Gowda (University of Edinburgh)"
      - author: "Panagiotis Eustratiadis (The University of Edinburgh)"
      - author: "Timothy Hospedales (Edinburgh University)"
      - author: "Laura Sevilla-Lara (Facebook)"
    all_authors: "Shreyank Gowda, Panagiotis Eustratiadis, Timothy Hospedales and Laura Sevilla-Lara"
    code: ""
    keywords:
      - word: "video object segmentation"
      - word: "tracking"
      - word: ""
    paper: "papers/0226.pdf"
    supp: ""
    abstract: "We consider the challenging problem of zero-shot video object segmentation (VOS). That is, segmenting and tracking multiple moving objects within a video fully automatically, without any manual initialization. We treat this as a grouping problem by exploiting object proposals and making a joint inference about grouping over both space and time. We propose a network architecture for tractably performing proposal selection and joint grouping. Crucially, we then show how to train this network with reinforcement learning so that it learns to perform the optimal non-myopic sequence of grouping decisions to segment the whole video. Unlike standard supervised techniques, this also enables us to directly optimize for the non-differentiable overlap-based metrics used to evaluate VOS. We show state-of-the-art results on DAVIS-2017 and Youtube-VOS benchmarks."
  - id: 233
    order: 172
    poster_session: 4
    session_id: 11
    title: "Making a Case for 3D Convolutions for Object Segmentation in Videos"
    authors:
      - author: "Sabarinath Mahadevan (RWTH Aachen University)"
      - author: "Ali Athar (RWTH Aachen)"
      - author: "Aljosa Osep (TUM Munich)"
      - author: "Laura Leal-Taixé (TUM)"
      - author: "Bastian Leibe (RWTH Aachen University-)"
      - author: "Sebastian Hennen (RWTH Aachen)"
    all_authors: "Sabarinath Mahadevan, Ali Athar, Aljosa Osep, Laura Leal-Taixé, Bastian Leibe and Sebastian Hennen"
    code: "https://github.com/sabarim/3DC-Seg"
    keywords:
      - word: "object tracking"
      - word: "video segmentation"
      - word: "video object segmentation"
      - word: "video scene understanding"
      - word: "object segmentation"
    paper: "papers/0233.pdf"
    supp: "supp/0233_supp.zip"
    abstract: "The task of object segmentation in videos is usually accomplished by processing appearance and motion information separately using standard 2D convolutional networks, followed by a learned fusion of the two sources of information. On the other hand, 3D convolutional networks have been successfully applied for video classification tasks, but have not been leveraged as effectively to problems involving dense per-pixel interpretation of videos compared to their 2D convolutional counterparts and lag behind the aforementioned networks in terms of performance. In this work, we show that 3D CNNs can be effectively applied to dense video prediction tasks such as salient object segmentation. We propose a simple yet effective encoder-decoder network architecture consisting entirely of 3D convolutions that can be trained end-to-end using a standard cross-entropy loss. To this end, we leverage an efficient 3D encoder, and propose a 3D decoder architecture, that comprises novel 3D Global Convolution layers and 3D Refinement modules. Our approach outperforms existing state-of-the-arts by a large margin on the DAVIS'16 Unsupervised, FBMS and ViSal dataset benchmarks in addition to being faster, thus showing that our architecture can efficiently learn expressive spatio-temporal features and produce high quality video segmentation masks. We have made our code and trained models publicly available at: https://github.com/sabarim/3DC-Seg"
  - id: 234
    order: 77
    poster_session: 2
    session_id: 5
    title: "Mid-level Fusion for End-to-End Temporal Activity Detection in Untrimmed Video"
    authors:
      - author: "Md Atiqur Rahman (University of Ottawa)"
      - author: "Robert Laganiere (University of Ottawa)"
    all_authors: "Md Atiqur Rahman and Robert Laganiere"
    code: ""
    keywords:
      - word: "temporal activity detection"
      - word: "action detection"
      - word: "untrimmed video processing"
      - word: "single-stage detection"
      - word: ""
    paper: "papers/0234.pdf"
    supp: ""
    abstract: "In this paper, we address the problem of human activity detection in temporally untrimmed long video sequences, where the goal is to classify and temporally localize each activity instance in the input video. Inspired by the recent success of the single-stage object detection methods, we propose an end-to-end trainable framework capable of learning task-specific spatio-temporal features of a video sequence for direct classification and localization of the activities. We, further, systematically investigate how and where to fuse multi-stream feature representations of a video and propose a new fusion strategy for temporal activity detection. Together with the proposed fusion strategy, the novel architecture sets new state-of-the-art on the highly challenging THUMOS'14 benchmark -- up from 44.2% to 53.9% mAP (an absolute 9.7% improvement)."
  - id: 238
    order: 120
    poster_session: 3
    session_id: 8
    title: "CoMoGCN: Coherent Motion Aware Trajectory Prediction with Graph Representation"
    authors:
      - author: "Yuying Chen (Hong Kong University of Science and Technology)"
      - author: "Congcong LIU (Hong Kong University of Science and Technology)"
      - author: "Bertram Shi (ECE Department of HKUST)"
      - author: "Ming Liu (HKUST)"
    all_authors: "Yuying Chen, Congcong LIU, Bertram Shi and Ming Liu"
    code: ""
    keywords:
      - word: "trajectory prediction"
      - word: "coherent motion"
      - word: "graph convolutional network"
    paper: "papers/0238.pdf"
    supp: "supp/0238_supp.zip"
    abstract: "Forecasting human trajectories is critical for tasks such as robot crowd navigation and autonomous driving.
Modeling social interactions is of great importance for accurate group-wise motion prediction.
However, most existing methods do not consider information about coherence within the crowd, but rather only pairwise interactions.
In this work, we propose a novel framework, coherent motion aware graph convolutional network (CoMoGCN), for trajectory prediction in crowded scenes with group constraints. First, we cluster pedestrian trajectories into groups according to motion coherence.
Then, we use graph convolutional networks to aggregate crowd information efficiently.
The CoMoGCN also takes advantage of variational inference to capture the variability in human trajectories by modeling the distribution.
Our method achieves state-of-the-art performance on several different trajectory prediction benchmarks, and the best average performance among all benchmarks considered."
  - id: 242
    order: 2
    oral_session: 1
    poster_session: 1
    session_id: 0
    title: "Explicit Knowledge Distillation for 3D Hand Pose Estimation from Monocular RGB"
    authors:
      - author: "Yumeng Zhang (Tsinghua University)"
      - author: "Li Chen (Tsinghua University)"
      - author: "Yufeng Liu (Kuaishou Technology)"
      - author: "Wen Zheng (Kuaishou Technology)"
      - author: "JunHai Yong (Tsinghua University)"
    all_authors: "Yumeng Zhang, Li Chen, Yufeng Liu, Wen Zheng and JunHai Yong"
    code: ""
    keywords:
      - word: "3D hand pose estimation"
      - word: "knowledge distillation"
    paper: "papers/0242.pdf"
    supp: "supp/0242_supp.zip"
    abstract: "RGB-based 3D hand pose estimation methods frequently produce physiologically invalid gestures due to depth ambiguity and self-occlusion. Existing methods typically adopt complex networks and a large amount of data to avoid invalid gestures by automatically mining the physical constraints of the hand. These networks exhibit high computational complexity and thus are difficult to be deployed into mobile devices. In consideration of this problem, a novel knowledge distillation framework, called Explicit Knowledge Distillation, is proposed to enhance the performance of small pose estimation networks. The proposed teacher network has interpretable knowledge, explicitly passing the physical constraints to the student network. Experimental results on three benchmark datasets with different sized models demonstrate the potential of our approach."
  - id: 243
    order: 16
    poster_session: 1
    session_id: 2
    title: "Annealing Genetic GAN for Minority Oversampling"
    authors:
      - author: "Jingyu Hao (Sun Yat-sen University)"
      - author: "Chengjia Wang (University of Edinburgh)"
      - author: "Heye Zhang (Sun Yat-sen University )"
      - author: "Guang Yang (Imperial College London)"
    all_authors: "Jingyu Hao, Chengjia Wang, Heye Zhang and Guang Yang"
    code: "https://github.com/Heye-SYSU/AGGAN"
    keywords:
      - word: "annealing genetic algorithm"
      - word: "generative adversarial networks"
      - word: "class imbalance problem"
      - word: "mode collapse"
    paper: "papers/0243.pdf"
    supp: ""
    abstract: "The key to overcome class imbalance problems is to capture the distribution of minority class accurately. Generative Adversarial Networks (GANs) have shown some potentials to tackle class imbalance problems due to their capability of reproducing data distributions given ample training data samples. However, the scarce samples of one or more classes still pose a great challenge for GANs to learn accurate distributions for the minority classes. In this work, we propose an Annealing Genetic GAN (AGGAN) method, which aims to reproduce the distributions closest to the ones of the minority classes using only limited data samples. Our AGGAN renovates the training of GANs as an evolutionary process that incorporates the mechanism of simulated annealing. In particular, the generator uses different training strategies to generate multiple offspring and retain the best. Then, we use the Metropolis criterion in the simulated annealing to decide whether we should update the best offspring for the generator. As the Metropolis criterion allows a certain chance to accept the worse solutions, it enables our AGGAN steering away from the local optimum. According to both theoretical analysis and experimental studies on multiple imbalanced image datasets, we prove that the proposed training strategy can enable our AGGAN to reproduce the distributions of minority classes from scarce samples and provide an effective and robust solution for the class imbalance problem."
  - id: 249
    order: 104
    oral_session: 6
    poster_session: 3
    session_id: 7
    title: "Multi-label Zero-shot Classification by Learning to Transfer from External Knowledge"
    authors:
      - author: "He Huang (University of Illinois at Chicago)"
      - author: "Wei Tang (University of Illinois at Chicago)"
      - author: "Philip  Yu (UIC)"
      - author: "Yuanwei Chen (Alibaba Group)"
      - author: "Wenhao Zheng (Alibaba Group)"
      - author: "Qing-Guo Chen (Alibaba)"
    all_authors: "He Huang, Wei Tang, Philip  Yu, Yuanwei Chen, Wenhao Zheng and Qing-Guo Chen"
    code: ""
    keywords:
      - word: "zero-shot learning"
      - word: "graph neural networks"
      - word: "multi-label classification"
    paper: "papers/0249.pdf"
    supp: ""
    abstract: "Multi-label zero-shot classification aims to predict multiple unseen class labels for an input image. It is more challenging than its single-label counterpart. On one hand, the unconstrained number of labels assigned to each image makes the model more easily overfit to those seen classes. On the other hand, there is a large semantic gap between seen and unseen classes in the existing multi-label classification datasets. To address these difficult issues, this paper introduces a novel multi-label zero-shot classification framework by learning to transfer from external knowledge. We observe that ImageNet is commonly used to pretrain the feature extractor and has a large and fine-grained label space. This motivates us to exploit it as external knowledge to bridge the seen and unseen classes and promote generalization. Specifically, we construct a knowledge graph including not only classes from the target dataset but also those from ImageNet. Since ImageNet labels are not available in the target dataset, we propose a novel PosVAE module to infer their initial states in the extended knowledge graph. Then we design a relational graph convolutional network (RGCN) to propagate information among classes and achieve knowledge transfer. Experimental results on two benchmark datasets demonstrate the effectiveness of the proposed approach."
  - id: 250
    order: 187
    poster_session: 4
    session_id: 11
    title: "BiHand: Recovering Hand Mesh with Multi-stage Bisected Hourglass Networks"
    authors:
      - author: "Lixin Yang (Shanghai Jiao Tong University)"
      - author: "Jiasen Li (Shanghai Jiao Tong University)"
      - author: "Wenqiang Xu (Shanghai Jiao Tong University)"
      - author: "Yiqun Diao (Shanghai Jiao Tong University)"
      - author: "Cewu Lu (Shanghai Jiao Tong University)"
    all_authors: "Lixin Yang, Jiasen Li, Wenqiang Xu, Yiqun Diao and Cewu Lu"
    code: "https://github.com/lixiny/bihand"
    keywords:
      - word: "hand pose estimation"
      - word: "3d hand"
      - word: "hand reconstruction"
      - word: "hand mesh"
      - word: ""
    paper: "papers/0250.pdf"
    supp: ""
    abstract: "3D hand estimation has been a long-standing research topic in computer vision. A recent trend aims not only to estimate the 3D hand joint locations but also to recover the mesh model. However, achieving those goals from a single RGB image remains challenging. In this paper, we introduce an end-to-end learnable model, BiHand, which consists of three cascaded stages, namely 2D seeding stage, 3D lifting stage, and mesh generation stage. At the output of BiHand, the full hand mesh will be recovered using the joint rotations and shape parameters predicted from the network. Inside each stage, BiHand adopts a novel bisecting design which allows the networks to encapsulate two closely related information (e.g. 2D keypoints and silhouette in 2D seeding stage, 3D joints, and depth map in 3D lifting stage, joint rotations and shape parameters in the mesh generation stage) in a single forward pass. As the information represents different geometry or structure details, bisecting the data flow can facilitate optimization and increase robustness. For quantitative evaluation, we conduct experiments on two public benchmarks, namely the Rendered Hand Dataset (RHD) and the Stereo Hand Pose Tracking Benchmark (STB). Extensive experiments show that our model can achieve superior accuracy in comparison with state-of-the-art methods, and can produce appealing 3D hand meshes in several severe conditions. The training codes, model and dataset are publicly available at https://github.com/lixiny/bihand."
  - id: 257
    order: 185
    poster_session: 4
    session_id: 11
    title: "Robust Scene Text Recognition Through Adaptive Image Enhancement"
    authors:
      - author: "Ye Qian (Nanjing University)"
      - author: "Yuyang Wang (Nanjing University)"
      - author: "Feng Su (Nanjing University)"
    all_authors: "Ye Qian, Yuyang Wang and Feng Su"
    code: "https://github.com/qy-NJU/TextREN/"
    keywords:
      - word: "text recognition"
      - word: "image enhancement"
      - word: "spatial rectification"
      - word: "end-to-end"
      - word: "scene text"
      - word: ""
    paper: "papers/0257.pdf"
    supp: "supp/0257_supp.zip"
    abstract: "Scene text in natural images often has a complex and varied appearance and a variety of degradations, which pose a great challenge to the reliable recognition of text. In this paper, we propose a novel scene text recognition method that introduces an effective, end-to-end trainable text image enhancement network prior to an attention-based recognition network, which adaptively improves the text image and enhances the performance of the whole recognition model. 
Specifically, the enhancement network combines a novel hierarchical residual enhancement network, which generates and refines pixel-wise enhancement details that are added to the input text image, and a spatial rectification network regularizing the shape of the text. Through end-to-end training with the recognition network in a weak supervision way with word annotations only, the enhancement network effectively learns to transform the text image to a more favorable form for subsequent recognition. The state-of-the-art results on several standard benchmarks demonstrate the effectiveness of our enhancement-based scene text recognition method."
  - id: 261
    order: 27
    poster_session: 1
    session_id: 2
    title: "Bias-Awareness for Zero-Shot Learning the Seen and Unseen"
    authors:
      - author: "William Thong (University of Amsterdam)"
      - author: "Cees Snoek (University of Amsterdam)"
    all_authors: "William Thong and Cees Snoek"
    code: "https://github.com/twuilliam/bias-gzsl"
    keywords:
      - word: "generalized zero-shot learning"
      - word: "classifier bias"
      - word: "temperature calibration"
      - word: "entropy regularization"
      - word: ""
    paper: "papers/0261.pdf"
    supp: ""
    abstract: "Generalized zero-shot learning recognizes inputs from both seen and unseen classes. Yet, existing methods tend to be biased towards the classes seen during training. In this paper, we strive to mitigate this bias. We propose a bias-aware learner to map inputs to a semantic embedding space for generalized zero-shot learning. During training, the model learns to regress to real-valued class prototypes in the embedding space with temperature scaling, while a margin-based bidirectional entropy term regularizes seen and unseen probabilities. Relying on a real-valued semantic embedding space provides a versatile approach, as the model can operate on different types of semantic information for both seen and unseen classes. Experiments are carried out on four benchmarks for generalized zero-shot learning and demonstrate the benefits of the proposed bias-aware classifier, both as a stand-alone method or in combination with generated features."
  - id: 264
    order: 19
    poster_session: 1
    session_id: 2
    title: "Object Detection as a Positive-Unlabeled Problem"
    authors:
      - author: "Yuewei Yang (Duke University)"
      - author: "Kevin Liang (Duke University)"
      - author: "Lawrence Carin Duke (CS)"
    all_authors: "Yuewei Yang, Kevin Liang and Lawrence Carin Duke"
    code: ""
    keywords:
      - word: "object detections"
      - word: "positive unlabeled learning"
      - word: ""
    paper: "papers/0264.pdf"
    supp: "supp/0264_supp.pdf"
    abstract: "As with other deep learning methods, label quality is important for learning modern convolutional object detectors. However, the potentially large number and wide diversity of object instances that can be found in complex image scenes makes constituting complete annotations a challenging task. Indeed, objects missing annotations can be observed in a variety of popular object detection datasets. These missing annotations can be problematic, as the standard cross-entropy loss employed to train object detection models treats classification as a positive-negative (PN) problem: unlabeled regions are implicitly assumed to be background. As such, any object missing a bounding box results in a confusing learning signal, the effects of which we observe empirically. To remedy this, we propose treating object detection as a positive-unlabeled (PU) problem, which removes the assumption that unlabeled regions must be negative. We demonstrate that our proposed PU classification loss outperforms the standard PN loss on PASCAL VOC and MS COCO across a range of label missingness, as well as on Visual Genome and DeepLesion with full labels."
  - id: 266
    order: 3
    oral_session: 1
    poster_session: 1
    session_id: 0
    title: "Text Attribute Aggregation and Visual Feature Decomposition for Person Search"
    authors:
      - author: "sara iodice (Imperial College London)"
      - author: "Krystian Mikolajczyk (Imperial College London)"
    all_authors: "Sara Iodice and Krystian Mikolajczyk"
    code: "https://github.com/iodicesara/Text-Attribute-Aggregation-and-Visual-Feature-Decomposition-for-Person-Search"
    keywords:
      - word: "person search"
      - word: "person re-identification"
      - word: "text-to-image retrieval"
      - word: "data augmentation"
      - word: "cross modality matching"
    paper: "papers/0266.pdf"
    supp: ""
    abstract: "Person search is the task of retrieving a pedestrian image given a list of text attributes. We investigate a novel mechanism that operates in feature embedding space for matching data across visual and text modalities. We propose a framework (TAVD) with two complementary modules: Text attribute feature aggregation (TA) that aggregates multiple semantic attributes in a bimodal space for globally matching text descriptions with images and Visual feature decomposition (VD) which performs feature embedding for locally matching image regions with text attributes. The results and comparisons to the state of the art on three standard benchmarks demonstrate that our solution is an effective strategy for retrieving person images while retaining the semantic of each query text attribute."
  - id: 274
    order: 66
    poster_session: 2
    session_id: 5
    title: "Pose Proposal Critic: Robust Pose Refinement by Learning Reprojection Errors"
    authors:
      - author: "Lucas Brynte (Chalmers University of Technology)"
      - author: "Fredrik  Kahl (Chalmers)"
    all_authors: "Lucas Brynte and Fredrik  Kahl"
    code: ""
    keywords:
      - word: "Pose Refinement"
      - word: "Partial Occlusion"
      - word: "Rigid Object Pose Estimation"
      - word: "Rendering"
    paper: "papers/0274.pdf"
    supp: "supp/0274_supp.zip"
    abstract: "In recent years, considerable progress has been made for the task of rigid object pose estimation from a single RGB-image, but achieving robustness to partial occlusions remains a challenging problem. Pose refinement via rendering has shown promise in order to achieve improved results, in particular, when data is scarce.

In this paper we focus our attention on pose refinement, and show how to push the state-of-the-art further in the case of partial occlusions. The proposed pose refinement method leverages on a simplified learning task, where a CNN is trained to estimate the reprojection error between an observed and a rendered image. We experiment by training on purely synthetic data as well as a mixture of synthetic and real data. Current state-of-the-art results are outperformed for two out of three metrics on the Occlusion LINEMOD benchmark, while performing on-par for the final metric."
  - id: 275
    order: 83
    poster_session: 2
    session_id: 5
    title: "Superpixel Masking and Inpainting for Self-Supervised Anomaly Detection"
    authors:
      - author: "Zhenyu Li (Xi’an Jiaotong University(XJTU))"
      - author: "Ning Li (Xi'an Jiaotong University)"
      - author: "Kaitao Jiang (Xi'an Jiaotong University)"
      - author: "Zhiheng Ma (Xi'an Jiaotong University)"
      - author: "Xing Wei (Xi'an Jiaotong University)"
      - author: "Xiaopeng Hong (Xi'an Jiaotong University)"
      - author: "Yihong Gong (Xi'an Jiaotong University)"
    all_authors: "Zhenyu Li, Ning Li, Kaitao Jiang, Zhiheng Ma, Xing Wei, Xiaopeng Hong and Yihong Gong"
    code: ""
    keywords:
      - word: "Anomaly Detection"
      - word: "Self-supervise"
      - word: "Inpainting"
      - word: "Superpixel"
    paper: "papers/0275.pdf"
    supp: ""
    abstract: "Anomaly detection aims at identifying abnormal samples from the normal ones. Existing methods are usually supervised or detect anomalies at the instance level without localization. In this work, we propose an unsupervised method called Superpixel Masking And Inpainting (SMAI) to identify and locate anomalies in images. Specifically, superpixel segmentation is first performed on the images. Then an inpainting module is trained to learn the spatial and texture information of the normal samples through random superpixel masking and restoration. Therefore, the model can reconstruct the superpixel mask with normal content. At the inference stage, we mask the image using superpixels and restore them one by one. By comparing the mask areas of the original image and its reconstruction, we can identify and locate the abnormal regions. We conducted a comprehensive evaluation of SMAI on the latest MVTec anomaly detection dataset, and it shows that SMAI plays favorably against state-of-the-art methods."
  - id: 277
    order: 55
    oral_session: 4
    poster_session: 2
    session_id: 4
    title: "A CNN Based Approach for the Near-Field Photometric Stereo Problem"
    authors:
      - author: "Fotios Logothetis (Toshiba research)"
      - author: "Ignas Budvytis (Department of Engineering, University of Cambridge)"
      - author: "Roberto Mecca (cambridge university)"
      - author: "Roberto Cipolla (University of Cambridge)"
    all_authors: "Fotios Logothetis, Ignas Budvytis, Roberto Mecca and Roberto Cipolla"
    code: ""
    keywords:
      - word: "Photometric Stereo"
      - word: "BRDF"
      - word: "Rendering"
    paper: "papers/0277.pdf"
    supp: ""
    abstract: "Reconstructing the 3D shape of an object using several images under different light sources is a very challenging task, especially when realistic assumptions such as light propagation and attenuation, perspective viewing geometry and specular light reflection are considered. Many of works tackling Photometric Stereo (PS) problems often relax most of the aforementioned assumptions. Especially they ignore specular reflection and global illumination effects. In this work, we propose the first CNN based approach capable of handling these realistic assumptions in Photometric Stereo. We leverage recent improvements of deep neural networks for far-field Photometric Stereo and adapt them to near field setup. We achieve this by employing an iterative procedure for shape estimation which has two main steps. Firstly we train a per-pixel CNN to predict surface normals from reflectance samples. Secondly, we compute the depth by integrating the normal field in order to iteratively estimate light directions and attenuation which is used to compensate the input images to compute reflectance samples for the next iteration. To the best of our knowledge this is the first near-field framework which is able to accurately predict 3D shape from highly specular objects. Our method outperforms competing state-of-the-art near-field Photometric Stereo approaches on both synthetic and real experiments."
  - id: 279
    order: 36
    poster_session: 1
    session_id: 2
    title: "Anti-Litter Surveillance based on Person Understanding via Multi-Task Learning"
    authors:
      - author: "Kangmin Bae (ETRI)"
      - author: "Kimin Yun (ETRI)"
      - author: "Hyung-Il Kim (ETRI)"
      - author: "Youngwan Lee (ETRI)"
      - author: "Jongyoul Park (ETRI)"
    all_authors: "Kangmin Bae, Kimin Yun, Hyung-Il Kim, Youngwan Lee and Jongyoul Park"
    code: ""
    keywords:
      - word: "action recognition"
      - word: "visual surveillance"
      - word: ""
    paper: "papers/0279.pdf"
    supp: ""
    abstract: "In this paper, we propose a new framework for an anti-litter visual surveillance system to prevent garbage dumping as a real-world application. There have been many efforts to deploy an action recognition based visual surveillance system. However, many conventional methods were overfitted for only specific scenes due to hand-crafted rules and lack of real-world data. To overcome this problem, we propose a novel algorithm that handles the diverse scene properties of the real-world surveillance. In addition to collecting data from the real-world, we train the effective model to understand the person through multiple datasets such as human poses, human coarse action (e.g., upright, bent), and fine action (e.g., pushing a cart) via multi-task learning. As a result, our approach eliminates the need for scene-by-scene tuning and provides robustness to behavior understanding performance in a visual surveillance system. In addition, we propose a new object detection network that is optimized for detecting carryable objects and a person. The proposed detection network reduces the computational cost by specifying potential suspects only to the person who carries an object. Our method outperforms the state-of-the-art methods in detecting the garbage dumping action on real‐world surveillance video dataset."
  - id: 282
    order: 63
    poster_session: 2
    session_id: 5
    title: "Non-Probabilistic Cosine Similarity Loss for Few-Shot Image Classification"
    authors:
      - author: "Joonhyuk Kim (KAIST)"
      - author: "Inug Yoon (KAIST)"
      - author: "Gyeong-Moon Park (ETRI)"
      - author: "Jong-Hwan Kim (KAIST)"
    all_authors: "Joonhyuk Kim, Inug Yoon, Gyeong-Moon Park and Jong-Hwan Kim"
    code: ""
    keywords:
      - word: "few-shot learning"
      - word: "image classification"
      - word: "NPC loss"
    paper: "papers/0282.pdf"
    supp: "supp/0282_supp.pdf"
    abstract: "A few-shot image classification problem aims to recognize previously unseen objects with a small amount of data. Many works have been offered to solve the problem, while a simple transfer learning method with the cosine similarity based cross-entropy loss is still powerful compared with other methods. To improve the performance, we propose a novel Non-Probabilistic Cosine similarity (NPC) loss for few-shot classification that can replace the cross-entropy loss with the cosine similarity. A key difference of NPC loss is that it uses values of inputs instead of their probabilities. By simply changing the loss function, our model avoids overfitting on a training set and performs well on few-shot tasks. Experimental results show that the model with NPC loss clearly outperforms those with other loss functions and also achieves excellent performance compared with state-of-the-art algorithms on Mini-Imagenet and CUB-200-2011 datasets."
  - id: 287
    order: 131
    poster_session: 3
    session_id: 8
    title: "6DoF Object Pose Estimation via Differentiable Proxy Voting Regularizer"
    authors:
      - author: "Xin Yu (University of Technology Sydney)"
      - author: "Zheyu Zhuang (Australian National University)"
      - author: "Piotr Koniusz (Data61/CSIRO, ANU)"
      - author: "HONGDONG LI (Australian National University, Australia)"
    all_authors: "Xin Yu, Zheyu Zhuang, Piotr Koniusz and HONGDONG LI"
    code: ""
    keywords:
      - word: "Object pose estimation"
      - word: "6DOF pose estimation"
      - word: "differentiable voting"
      - word: ""
    paper: "papers/0287.pdf"
    supp: "supp/0287_supp.pdf"
    abstract: "Estimating a 6DOF object pose from a single image is very challenging due to occlusions or textureless appearances. Vector-field based keypoint voting has demonstrated its effectiveness and superiority on tackling those issues. 
However, direct regression of vector-fields neglects that the distances between pixels and keypoints also affect the deviations of hypotheses dramatically. In other words, small errors in direction vectors may generate severely deviated hypotheses when pixels are far away from a keypoint. In this paper, we aim to reduce such errors by incorporating the distances between pixels and keypoints into our objective. To this end, we develop a simple yet effective differentiable proxy voting Regularizer (DPVR) which mimics the hypothesis selection in the voting procedure.
By exploiting our voting regularizer, we are able to train our network in an end-to-end manner. Experiments on widely used datasets, ie, LINEMOD and Occlusion LINEMOD, manifest that our DPVR improves pose estimation performance significantly and speeds up the training convergence. "
  - id: 294
    order: 146
    poster_session: 3
    session_id: 8
    title: "3D-GMNet: Single-View 3D Shape Recovery as A Gaussian Mixture"
    authors:
      - author: "Kohei Yamashita (Kyoto University)"
      - author: "Shohei Nobuhara (Kyoto University)"
      - author: "Ko Nishino (Kyoto University)"
    all_authors: "Kohei Yamashita, Shohei Nobuhara and Ko Nishino"
    code: ""
    keywords:
      - word: "single image 3D reconstruction"
      - word: "3D shape representation"
      - word: "Gaussian mixture model"
    paper: "papers/0294.pdf"
    supp: "supp/0294_supp.zip"
    abstract: "In this paper, we introduce 3D-GMNet, a deep neural network for single-image 3D shape recovery. As the name suggests, 3D-GMNet recovers 3D shape as a Gaussian mixture model. In contrast to voxels, point clouds, or meshes, a Gaussian mixture representation requires a much smaller footprint for representing 3D shapes and, at the same time, offers a number of additional advantages including instant pose estimation, automatic level-of-detail computation, and a distance measure. The proposed 3D-GMNet is trained end-to-end with single input images and corresponding 3D models by using two novel loss functions: a 3D Gaussian mixture loss and a multi-view 2D loss. The first maximizes the likelihood of the Gaussian mixture shape representation by considering the target point cloud as samples from the true distribution, and the latter improves the consistency between the input silhouette and the projection of the Gaussian mixture shape model. Extensive quantitative evaluations with synthesized and real images demonstrate the effectiveness of the proposed method."
  - id: 303
    order: 141
    poster_session: 3
    session_id: 8
    title: "BCaR: Beginner Classifier as Regularization Towards Generalizable Re-ID"
    authors:
      - author: "Masato Tamura (Hitachi, Ltd.)"
      - author: "Tomoaki Yoshinaga (Hitachi, Ltd.)"
    all_authors: "Masato Tamura and Tomoaki Yoshinaga"
    code: "https://github.com/hitachi-rd-cv/bcar"
    keywords:
      - word: "person re-identification"
      - word: "generalizable"
      - word: "soft label"
      - word: "knowledge distillation"
      - word: "Re-ID"
      - word: "domain generalization"
    paper: "papers/0303.pdf"
    supp: ""
    abstract: "In recent years, the performance of person re-identification has been dramatically improved by virtue of sophisticated training methods. However, most of the existing methods are based on the assumption that the statistics of a target domain can be utilized during training. This inevitably introduces huge costs for data collection each time a person re-identification system is deployed, which hinders the applicability to real-world scenarios. To mitigate this issue, we expand upon the concept of domain generalization. Typical person re-identification datasets are composed of a large amount of identities. However, examples for each identity are rather scarce. It is widely known that if examples are highly biased, over-fitting is likely to occur and degrade the performance. To alleviate this problem, we propose a novel soft-label regularization method that combines an expert feature extractor with a beginner classifier for generating soft labels. From a representation learning perspective, a convolutional neural network-based feature extractor is thought to prioritize common patterns. Therefore, the subsequent classifier typically fits common examples first, followed by rare ones. On the basis of this observation, we force the beginner classifier to remain uncertain towards rare examples by means of periodic initialization. Accordingly, the beginner classifier assigns highly confident labels to common examples and ambiguous labels to rare ones, thus enabling soft labels to mitigate over-fitting to biased examples (e.g., highly occluded ones). Extensive analysis shows that our method successfully assigns ambiguous labels to biased examples and thus increases the rank-1 accuracy by 3.4%, 1.6%, 0.9%, and 5.2% on the VIPeR, PRID, GRID, and i-LIDS datasets, respectively. To facilitate future research, the source codes will be released."
  - id: 306
    order: 74
    poster_session: 2
    session_id: 5
    title: "Uncovering Hidden Challenges in Query-Based Video Moment Retrieval"
    authors:
      - author: "Mayu Otani (CyberAgent, Inc.)"
      - author: "Yuta Nakashima (Osaka University)"
      - author: "Esa Rahtu (Tampere University)"
      - author: "Janne Heikkila (University of Oulu, Finland)"
    all_authors: "Mayu Otani, Yuta Nakashima, Esa Rahtu and Janne Heikkila"
    code: "https://github.com/mayu-ot/hidden-challenges-MR"
    keywords:
      - word: "video moment retrieval"
      - word: "temporal sentence grounding"
      - word: "dataset analysis"
      - word: "negative result"
      - word: ""
    paper: "papers/0306.pdf"
    supp: "supp/0306_supp.pdf"
    abstract: "The query-based moment retrieval is a problem of localising a specific clip from an untrimmed video according a query sentence. This is a challenging task that requires interpretation of both the natural language query and the video content. Like in many other areas in computer vision and machine learning, the progress in query-based moment retrieval is heavily driven by the benchmark datasets and, therefore, their quality has significant impact on the field. In this paper, we present a series of experiments assessing how well the benchmark results reflect the true progress in solving the moment retrieval task. Our results indicate substantial biases in the popular datasets and unexpected behaviour of the state-of-the-art models. Moreover, we present new sanity check experiments and approaches for visualising the results. Finally, we suggest possible directions to improve the temporal sentence grounding in the future. "
  - id: 316
    order: 76
    poster_session: 2
    session_id: 5
    title: "Domain Adaptation of Learned Featuresfor Visual Localization"
    authors:
      - author: "Sungyong Baik (Seoul National University)"
      - author: "Hyo Jin Kim (Facebook)"
      - author: "Tianwei Shen (Faceboook)"
      - author: "Eddy Ilg (Facebook Reality Labs)"
      - author: "Kyoung Mu Lee (Seoul National University)"
      - author: "Christopher Sweeney (Facebook Reality Labs)"
    all_authors: "Sungyong Baik, Hyo Jin Kim, Tianwei Shen, Eddy Ilg, Kyoung Mu Lee and Christopher Sweeney"
    code: ""
    keywords:
      - word: "visual localization"
      - word: "local features"
      - word: "domain adaptation"
      - word: "few-shot learning"
      - word: "deep learning"
    paper: "papers/0316.pdf"
    supp: "supp/0316_supp.pdf"
    abstract: "We tackle the problem of visual localization under changing conditions, such as time of day, weather, and seasons. Recent learned local features based on deep neural networks have shown superior performance over classical hand-crafted local features. However, in a real-world scenario, there often exists a large domain gap between training and target images, which can significantly degrade the localization accuracy. We present an approach that mitigates the domain gap by proposing a few-shot domain adaptation framework for learned local features that deals with varying conditions in visual localization. The experimental results demonstrate the superior performance over baselines, while using only a few training examples from the target domain. "
  - id: 324
    order: 8
    poster_session: 1
    session_id: 2
    title: "Semantic Estimation of 3D Body Shape and Pose using Minimal Cameras"
    authors:
      - author: "Andrew Gilbert (University of Surrey)"
      - author: "Matthew Trumble (University of Surrey)"
      - author: "Adrian Hilton (University of  Surrey)"
      - author: "John Collomosse (Adobe Research)"
    all_authors: "Andrew Gilbert, Matthew Trumble, Adrian Hilton and John Collomosse"
    code: ""
    keywords:
      - word: "3D Pose estimation"
      - word: "Generative"
      - word: "3D Reconstruction"
      - word: "Multiple Viewpoint Video"
      - word: ""
    paper: "papers/0324.pdf"
    supp: ""
    abstract: "We present an approach to accurately estimate high fidelity markerless 3D pose and volumetric reconstruction of human performance using only a small set of camera views ( 2). Our method utilises a dual loss in a generative adversarial network that can yield improved performance in both reconstruction and pose estimate error. We use a deep prior implicitly learnt by the network trained over a dataset of view-ablated multi-view video footage of a wide range of subjects and actions. Uniquely we use a multi-channel symmetric 3D convolutional encoder-decoder with a dual loss to enforce the learning of a latent embedding that enforces skeletal joint positions and a deep volumetric reconstruction of the performer.
An extensive evaluation is performed with state of the art performance reported on three datasets; Human 3.6M, TotalCapture and TotalCaptureOutdoor. The method opens the possibility of high-end volumetric and pose performance capture in on-set and prosumer scenarios where time or cost prohibit a high witness camera count."
  - id: 328
    order: 61
    poster_session: 2
    session_id: 5
    title: "Deep Metric Learning Meets Deep Clustering: An Novel Unsupervised Approach for Feature Embedding"
    authors:
      - author: "Binh Nguyen (AIOZ)"
      - author: "Binh Nguyen (AIOZ)"
      - author: "Gustavo Carneiro (University of Adelaide)"
      - author: "Erman Tjiputra (AIOZ)"
      - author: "Quang Tran (AIOZ)"
      - author: "Thanh-Toan Do (The University of Liverpool)"
    all_authors: "Binh Nguyen, Binh Nguyen, Gustavo Carneiro, Erman Tjiputra, Quang Tran and Thanh-Toan Do"
    code: "https://github.com/aioz-ai/BMVC20_CBSwR"
    keywords:
      - word: "unsupervised deep metric learning"
      - word: "unsupervised feature learning"
      - word: "unsupervised metric loss"
      - word: "negative mining"
      - word: "deep clustering"
      - word: "pseudo labels"
      - word: "reconstruction"
      - word: "centroid representations"
      - word: "retrieval"
      - word: "multi-task"
      - word: ""
    paper: "papers/0328.pdf"
    supp: ""
    abstract: "Unsupervised Deep Distance Metric Learning (UDML) aims to learn sample similarities in the embedding space from an unlabeled dataset. Traditional UDML methods usually use the triplet loss or pairwise loss which requires the mining of positive and negative samples w.r.t. anchor data points. This is, however, challenging in an unsupervised setting as the label information is not available. In this paper, we propose a new UDML method that overcomes that challenge. In particular, we propose to use a deep clustering loss to learn centroids, i.e., pseudo labels, that represent semantic classes. During learning, these centroids are also used to reconstruct the input samples. It hence ensures the representativeness of centroids — each centroid represents visually similar samples. Therefore, the centroids give information about positive (visually similar) and negative (visually dissimilar) samples. Based on pseudo labels, we propose a novel unsupervised metric loss which enforces the positive concentration and negative separation of samples in the embedding space. Experimental results on benchmarking datasets show that the proposed approach outperforms other UDML methods."
  - id: 330
    order: 9
    poster_session: 1
    session_id: 2
    title: "Weakly Supervised Generative Network for Multiple 3D Human Pose Hypotheses"
    authors:
      - author: "Chen Li  (National University of Singapore)"
      - author: "Gim Hee Lee (National University of Singapore)"
    all_authors: "Chen Li and Gim Hee Lee"
    code: "https://github.com/chaneyddtt/weakly-supervised-3d-pose-generator"
    keywords:
      - word: "3d human pose"
      - word: "multiple hypotheses"
      - word: "weak supervision"
      - word: "generative model"
      - word: ""
    paper: "papers/0330.pdf"
    supp: ""
    abstract: "3D human pose estimation from a single image is an inverse problem 
due to the inherent ambiguity of the missing depth. 
Some previous works addressed the inverse problem by generating multiple hypotheses. However, these works are strongly supervised and require ground truth 2D-to-3D correspondences which can be difficult to obtain. In this paper, we propose a weakly supervised deep generative network to address the inverse problem and circumvent the need for ground truth 2D-to-3D correspondences. To this end, we design our network to model a proposal distribution which we use to approximate the unknown multi-modal target posterior distribution. We achieve the approximation by minimizing the KL divergence between the proposal and target distributions, and this leads to a 2D reprojection error and a prior loss term that can be weakly supervised. Furthermore, we determine the most probable solution as the conditional mode of the samples using the mean-shift algorithm.
We evaluate our method on three benchmark datasets -- Human3.6M, MPII and MPI-INF-3DHP. Experimental results show that 
our approach is capable of generating multiple feasible hypotheses and achieves state-of-the-art results compared to existing weakly supervised approaches."
  - id: 335
    order: 164
    poster_session: 4
    session_id: 11
    title: "Learning To Pay Attention To Mistakes"
    authors:
      - author: "Moucheng Xu (UCL)"
      - author: "Neil Oxtoby (University College London)"
      - author: "Daniel Alexander (University College London)"
      - author: "Joseph Jacob (University College London)"
    all_authors: "Moucheng Xu, Neil Oxtoby, Daniel Alexander and Joseph Jacob"
    code: ""
    keywords:
      - word: "attention"
      - word: "effective receptive field"
      - word: "segmentation"
      - word: "medical image"
      - word: "inductive bias"
    paper: "papers/0335.pdf"
    supp: ""
    abstract: "In Convolutional Neural Network based medical image segmentation, the periphery of foreground regions representing malignant tissues may be disproportionately assigned as belonging to the background class as healthy tissues. As evidenced in visual results in [18][21][24][12][4], misclassification of foreground pixels as the background class can lead to high False Negative detection rates. In this paper, we propose a novel attention mechanism to directly address such high false negative rates, called Paying Attention to False Positives. Our attention mechanism attempts to steer the models towards false positive identification, thereby addressing the bias towards high false negative rates in segmentation outcomes. The proposed mechanism has two complementary implementations: (a) “explicit” steering of the model to attend to the “enlarged” Effective Receptive Field on the foreground areas; (b) “implicit” learning towards false positives, by attending to the “shrunken” Effective Receptive Field on the background areas. We first compare our models with state-of-the-art attention baselines in medical imaging, on a binary
dense prediction task between vehicles and the background using CityScapes. We then perform a second task which is to segment Enhanced Tumour Core areas in multi-modal MRI scans from the BRATS2018 datast, under 5-fold cross validation. In the second task, we include more baselines including self-attention, spatial attention and spatial-channel mixed attention. Additionally, we conduct comprehensive ablation studies on our models. Lastly, we evaluate our proposed mechanism against another brain lesion segmentation task, using ultrasound images from the ISLES2018 dataset. Across all of the three different tasks, our models consistently outperform the baseline models in terms of Hausdorff Distance (HD) or/and Intersection Over Union (IoU). For instance, in the second task, the “explicit” implementation of our mechanism reduces the HD of the best baseline by more than 26%, whilst improving the IoU by more than 3%. We believe our proposed attention mechanism can provide safer computer-aided-detection in a wide range of medical applications. The link to our codes on GitHub is hidden to maintain anonymity during the review period."
  - id: 342
    order: 174
    poster_session: 4
    session_id: 11
    title: "Neighbours Matter: Image Captioning with Similar Images"
    authors:
      - author: "Qingzhong Wang (Department of Computer Science, City University of Hong Kong)"
      - author: "Jiuniu Wang (City University of Hong Kong)"
      - author: "Antoni Chan (City University of Hong Kong, Hong, Kong)"
      - author: "Siyu Huang (Baidu Research)"
      - author: "Haoyi Xiong (Baidu Research)"
      - author: "Xingjian Li (Baidu Research)"
      - author: "Dejing Dou (Baidu)"
    all_authors: "Qingzhong Wang, Jiuniu Wang, Antoni Chan, Siyu Huang, Haoyi Xiong, Xingjian Li and Dejing Dou"
    code: ""
    keywords:
      - word: "Image captioning"
      - word: "graph neural networks"
      - word: "attention mechanism"
    paper: "papers/0342.pdf"
    supp: ""
    abstract: "Most image captioning models aim to generate captions based solely on the input image.  However images that are similar to the given input image  contain variations of the same or similar concepts as the input image. Thus, aggregating information over similar images could be used to improve image captioning models, by strengthening or inferring concepts that are in the input image. In this paper, we propose an image captioning model based on KNN graphs composed of the input image and its similar images, where each node denotes an image or a caption.  An attention-in-attention (AiA)  model is developed to refine the node representations. Using the refined features significantly improves the baseline performance, eg, CIDEr score obtained by Updown model increases from 120.1 to 125.6. Compared with the state-of-the-art performance, our proposed method obtains 129.3 of CIDEr and 22.6 of SPICE on Karpathy's test split, which is competitive with the models that employ fine-grained image features such as scene graphs and image parsing trees."
  - id: 353
    order: 105
    oral_session: 6
    poster_session: 3
    session_id: 7
    title: "Few-Shot Learning with Complex-valued Neural Networks"
    authors:
      - author: "Zhen Liu (Beihang University)"
      - author: "Baochang Zhang (Beihang University)"
      - author: "Guodong Guo (Baidu)"
    all_authors: "Zhen Liu, Baochang Zhang and Guodong Guo"
    code: "https://github.com/ZhenLiuBuaa/Complex-valued-few-shot-learning"
    keywords:
      - word: "few-shot learning"
      - word: "complex-valued network"
      - word: "metric-learning"
      - word: "image classification"
      - word: ""
    paper: "papers/0353.pdf"
    supp: ""
    abstract: "Feature representation is fundamental and attracts much attention in few-shot learning. Convolutional neural networks (CNNs) are among the best feature extractors so far in this field, which are successfully combined with metric learning, leading to the state-of-the-art performance. However, the subtle difference among inter-class samples challenges existing CNN based methods, which only use real-valued CNNs that fail to extract more detailed information.  In this paper, we introduce complex metric module (CMM) into metric learning, aiming to better measure the inter- and intra-class relations based on both amplitude and phase information. Specifically, building upon the recent episodic training mechanism, our CMM can enhance the representation capacity by extracting robust complex-valued features to facilitate modeling subtle relationships among samples, which can enhance the performance of the few-shot classification task when only few samples are available. Moreover, we introduce a new transductive method into CMM, by considering not only query and support but also query and query relationships to predict classes of unlabeled samples. Experiments on two benchmark datasets show that the proposed CMM significantly improves the performance over other approaches and achieves the state-of-the-art results."
  - id: 356
    order: 47
    poster_session: 1
    session_id: 2
    title: "Conditional Attention for Content-based Image Retrieval"
    authors:
      - author: "Zechao Hu (University of York)"
      - author: "Adrian Bors (University of York)"
    all_authors: "Zechao Hu and Adrian Bors"
    code: ""
    keywords:
      - word: "content based image retrieval"
      - word: "conditional attention"
    paper: "papers/0356.pdf"
    supp: ""
    abstract: "Deep learning based feature extraction combined with visual attention mechanism is shown to provide good results in content-based image retrieval (CBIR). Ideally, CBIR should rely on regions which contain objects of interest that appear in the query image. However, most existing attention models just predict the most likely region of interest based on the knowledge learned from the training dataset regardless of the content in the query image. As a result, they may look towards contexts outside the object of interest, especially when there are multiple potential objects of interest in a given image. In this paper, we propose a conditional attention model which is sensitive to the input query image content and can generate more accurate attention maps. A key-point detection and description based method is proposed for training data generation. Consequently, our model does not require any additional attention label for training. The proposed attention model enables the spatial pooling feature extraction method (generalized mean pooling) improves image feature representation and leads to better image retrieval performance. The proposed framework is tested on a series of databases where it is shown to perform well in challenging situations."
  - id: 359
    order: 68
    poster_session: 2
    session_id: 5
    title: "Automated Search for Resource-Efficient Branched Multi-Task Networks"
    authors:
      - author: "David Brüggemann (ETH Zurich)"
      - author: "Menelaos Kanakis (ETH Zurich)"
      - author: "Stamatios Georgoulis (ETH Zurich)"
      - author: "Luc Van Gool (ETH Zurich)"
    all_authors: "David Brüggemann, Menelaos Kanakis, Stamatios Georgoulis and Luc Van Gool"
    code: "https://github.com/brdav/bmtas"
    keywords:
      - word: "multi task"
      - word: "neural architecture search"
      - word: "resource efficient networks"
      - word: "dense prediction"
      - word: "encoder branching"
      - word: "proxyless resource loss"
      - word: "differentiable search space"
      - word: "branched networks"
      - word: "tree-like networks"
      - word: "Gumbel-Softmax"
      - word: ""
    paper: "papers/0359.pdf"
    supp: "supp/0359_supp.zip"
    abstract: "The multi-modal nature of many vision problems calls for neural network architectures that can perform multiple tasks concurrently. Typically, such architectures have been handcrafted in the literature. However, given the size and complexity of the problem, this manual architecture exploration likely exceeds human design abilities. In this paper, we propose a principled approach, rooted in differentiable neural architecture search, to automatically define branching (tree-like) structures in the encoding stage of a multi-task neural network. To allow flexibility within resource-constrained environments, we introduce a proxyless, resource-aware loss that dynamically controls the model size. Evaluations across a variety of dense prediction tasks show that our approach consistently finds high-performing branching structures within limited resource budgets."
  - id: 366
    order: 101
    oral_session: 5
    poster_session: 3
    session_id: 6
    title: "High-speed event-based camera tracking"
    authors:
      - author: "William Chamorro (Institut de Robòtica i Informàtica Industrial, CSIC-UPC)"
      - author: "Juan Andrade-Cetto (Institut de Robòtica i Informàtica Industrial CSIC-UPC)"
      - author: "Joan Solà (Institut de Robòtica i Informàtica Industrial, CSIC-UPC)"
    all_authors: "William Chamorro, Juan Andrade-Cetto and Joan Solà"
    code: ""
    keywords:
      - word: "High speed tracking"
      - word: "Event Cameras"
      - word: "SLAM"
      - word: "Kalman Filter"
      - word: ""
    paper: "papers/0366.pdf"
    supp: "supp/0366_supp.zip"
    abstract: "Event cameras are bioinspired sensors with reaction times in the order of microseconds. This property makes them appealing for use in highly-dynamic computer vision applications. 
In this work, we explore the limits of this sensing technology and present an ultra-fast tracking algorithm able to estimate six-degree-of-freedom motion with dynamics over 25.8g, at a throughput of 10kHz, processing over a million events per second. 
Our method is capable of tracking either camera motion or the motion of an object in front of it, using an error-state Kalman filter formulated in a Lie-theoretic sense. 
The method includes a robust mechanism for the matching of events with projected line segments with very fast outlier rejection. 
Meticulous treatment of sparse matrices is applied to achieve real-time performance. Different motion models of varying complexity are considered for the sake of comparison and performance analysis."
  - id: 367
    order: 14
    poster_session: 1
    session_id: 2
    title: "Mixup-CAM: Weakly-supervised Semantic Segmentation via Uncertainty Regularization"
    authors:
      - author: "Yu-Ting Chang (University of California, Merced)"
      - author: "Qiaosong Wang (eBay Inc.)"
      - author: "Wei-Chih Hung (University of California, Merced)"
      - author: "Robinson Piramuthu (eBay Inc.)"
      - author: "Yi-Hsuan Tsai (NEC Labs America)"
      - author: "Ming-Hsuan Yang (University of California at Merced)"
    all_authors: "Yu-Ting Chang, Qiaosong Wang, Wei-Chih Hung, Robinson Piramuthu, Yi-Hsuan Tsai and Ming-Hsuan Yang"
    code: ""
    keywords:
      - word: "semantic segmentation"
      - word: "weakly-supervised learning"
      - word: "class activatin map"
      - word: "mixup augmentation"
      - word: "entropy regularization"
      - word: ""
    paper: "papers/0367.pdf"
    supp: "supp/0367_supp.pdf"
    abstract: "Obtaining object response maps is one important step to achieve weakly-supervised semantic segmentation using image-level labels.  However, existing methods rely on the classification task, which could result in a response map only attending on discriminative object regions as the network does not need to see the entire object for optimizing the classification loss.  To tackle this issue, we propose a principled and end-to-end train-able framework to allow the network to pay attention to other parts of the object, while producing a more complete and uniform response map.  Specifically, we introduce the mixup data augmentation scheme into the classification network and design two uncertainty regularization terms to better interact with the mixup strategy.   In experiments, we conduct extensive analysis to demonstrate the proposed method and show favorable performance against state-of-the-art approaches."
  - id: 368
    order: 123
    poster_session: 3
    session_id: 8
    title: "Learning Gaussian Maps for Dense Object Detection"
    authors:
      - author: "Sonaal Kant (ParallelDots, Inc.)"
    all_authors: "Sonaal Kant"
    code: ""
    keywords:
      - word: "Dense Object Detection"
      - word: "RetinaNet"
      - word: "Multitask learning"
    paper: "papers/0368.pdf"
    supp: ""
    abstract: "Object detection is a popular branch of research in computer vision, many state of the art object detection algorithms have been introduced in the recent past, but how good are  those object detectors when it comes to dense object detection? In this paper we review common and highly accurate object detection methods on the scenes where numerous similar looking objects are placed in close proximity with each other.  We also show that, multi-task learning of gaussian maps along with classification and bounding box regression gives us a significant boost in accuracy over the baseline. We introduce Gaussian Layer and Gaussian Decoder in the existing RetinaNet network for better accuracy in dense scenes, with the same computational cost as the RetinaNet. We show the gain of 6% and 5% in mAP with respect to baseline RetinaNet.
Our method also achieves the state of the art accuracy on the SKU110K  dataset.
"
  - id: 369
    order: 87
    poster_session: 2
    session_id: 5
    title: "Semantically Adaptive Image-to-image Translation for Domain Adaptation of Semantic Segmentation"
    authors:
      - author: "Luigi Musto (University of Parma)"
      - author: "Andrea Zinelli (University of Parma)"
    all_authors: "Luigi Musto and Andrea Zinelli"
    code: ""
    keywords:
      - word: "domain adaptation"
      - word: "semantic segmentation"
      - word: "image-to-image translation"
      - word: "generative models"
      - word: "image translation"
    paper: "papers/0369.pdf"
    supp: "supp/0369_supp.zip"
    abstract: "Domain shift is a very challenging problem for semantic segmentation. Any model can be easily trained on synthetic data, where images and labels are artificially generated, but it will perform poorly when deployed on real environments. In this paper, we address the problem of domain adaptation for semantic segmentation of street scenes. Many state-of-the-art approaches focus on translating the source image while imposing that the result should be semantically consistent with the input. However, we advocate that the image semantics can also be exploited to guide the translation algorithm. To this end, we rethink the generative model to enforce this assumption and strengthen the connection between pixel-level and feature-level domain alignment. We conduct extensive experiments by training common semantic segmentation models with our method and show that the results we obtain on the synthetic-to-real benchmarks surpass the state-of-the-art."
  - id: 375
    order: 126
    poster_session: 3
    session_id: 8
    title: "Learning to Adapt Multi-View Stereo by Self-Supervision"
    authors:
      - author: "Arijit Mallick (University of Tuebingen)"
      - author: "Joerg Stueckler (Max-Planck-Institute for Intelligent Systems)"
      - author: "Hendrik Lensch (University of Tübingen)"
    all_authors: "Arijit Mallick, Joerg Stueckler and Hendrik Lensch"
    code: ""
    keywords:
      - word: "multi-view stereo reconstruction"
      - word: "self-supervised learning"
      - word: "domain adaptation"
      - word: "meta learning"
    paper: "papers/0375.pdf"
    supp: "supp/0375_supp.pdf"
    abstract: "3D scene reconstruction from multiple views is an important classical problem in computer vision. Deep learning based approaches have recently demonstrated impressive reconstruction results. When training such models, self-supervised methods are favourable since they do not rely on ground truth data which would be needed for supervised training and is often difficult to obtain. Moreover, learned multi-view stereo reconstruction is prone to environment changes and should robustly generalise to different domains. We propose an adaptive learning approach for multi-view stereo which trains a deep neural network for improved adaptability to new target domains. We use model-agnostic meta-learning (MAML) to train base parameters which, in turn, are adapted for multi-view stereo on new domains through self-supervised training. Our evaluations demonstrate that the proposed adaptation method is effective in learning self-supervised multi-view stereo reconstruction in new domains."
  - id: 378
    order: 167
    poster_session: 4
    session_id: 11
    title: "Graph Density-Aware Losses for Novel Compositions in Scene Graph Generation"
    authors:
      - author: "Boris Knyazev (University of Guelph)"
      - author: "Harm De Vries (Element AI)"
      - author: "Cătălina Cangea (University of Cambridge)"
      - author: "Graham Taylor (University of Guelph)"
      - author: "Aaron Courville (Universite de Montreal)"
      - author: "Eugene Belilovsky (Mila, University of Montreal)"
    all_authors: "Boris Knyazev, Harm De Vries, Cătălina Cangea, Graham Taylor, Aaron Courville and Eugene Belilovsky"
    code: "https://github.com/bknyaz/sgg"
    keywords:
      - word: "scene graphs"
      - word: "scene graph generation"
      - word: "graph density"
      - word: "compositional generalization"
      - word: "visual genome"
      - word: "gqa"
      - word: "message passing"
    paper: "papers/0378.pdf"
    supp: "supp/0378_supp.pdf"
    abstract: "Scene graph generation (SGG) aims to predict graph-structured descriptions of input images, in the form of objects and relationships between them. This task is becoming increasingly useful for progress at the interface of vision and language. Here, it is important—yet challenging—to perform well on novel (zero shot) or rare (few shot) compositions of objects and relationships. In this paper, we identify two key issues that limit such generalization. Firstly, we show that the standard loss used in this task is unintentionally a function of scene graph density. This leads to the neglect of individual edges in large sparse graphs during training, even though these contain diverse few shot examples that are important for generalization. Secondly, the frequency of relationships can create a strong bias in this task, such that a ``blind'' model predicting the most frequent relationship achieves good performance. Consequently, some state-of-the-art models exploit this bias to improve results. We show that such models can suffer the most in their ability to generalize to rare compositions, evaluating two different models on the Visual Genome dataset and its more recent, improved version, GQA. To address these issues, we introduce a density-normalized edge loss, which provides more than a two-fold improvement in certain generalization metrics. Compared to other works in this direction, our enhancements require only a few lines of code and no added computational cost. We also highlight the difficulty of accurately evaluating models using existing metrics, especially on zero/few shots, and introduce a novel weighted metric."
  - id: 383
    order: 26
    poster_session: 1
    session_id: 2
    title: "Stratified Autocalibration of Cameras with Euclidean Image Plane"
    authors:
      - author: "Devesh Adlakha (University of Strasbourg)"
      - author: "Adlane Habed (University of Strasbourg, CNRS, France)"
      - author: "Fabio Morbidi (University of Picardie Jules Vernes)"
      - author: "Cedric Demonceaux (Univ. Bourgogne Franche-Comte, France)"
      - author: "Michel de Mathelin (University of Strasbourg)"
    all_authors: "Devesh Adlakha, Adlane Habed, Fabio Morbidi, Cedric Demonceaux and Michel de Mathelin"
    code: ""
    keywords:
      - word: "camera autocalibration"
      - word: "Euclidean image plane"
      - word: "polynomial optimization"
      - word: "plane at infinity"
      - word: "3D reconstruction"
      - word: "multi-view geometry"
      - word: ""
    paper: "papers/0383.pdf"
    supp: "supp/0383_supp.zip"
    abstract: "This paper tackles the problem of stratified autocalibration of a moving camera with Euclidean image plane (i.e. zero skew and unit aspect ratio) and constant intrinsic parameters. We show that with these assumptions, in addition to the polynomial derived from the so-called modulus constraint, each image pair provides a new quartic polynomial in the unknown plane at infinity. For three or more images, the plane at infinity estimation is stated as a constrained polynomial optimization problem that can efficiently be solved using Lasserre’s hierarchy of semidefinite relaxations. The calibration parameters and thus a metric reconstruction are subsequently obtained by solving a system of linear equations. Synthetic data and real image experiments show that the new polynomial in our proposed algorithm leads to a more reliable performance than existing methods."
  - id: 384
    order: 108
    poster_session: 3
    session_id: 8
    title: "Attribute-Guided Image Generation from Layout"
    authors:
      - author: "Ke Ma (University of British Columbia )"
      - author: "Bo Zhao (Bank of Montreal)"
      - author: "Leonid Sigal (University of British Columbia)"
    all_authors: "Ke Ma, Bo Zhao and Leonid Sigal"
    code: ""
    keywords:
      - word: "conditional image generation"
      - word: "GAN"
      - word: ""
    paper: "papers/0384.pdf"
    supp: "supp/0384_supp.pdf"
    abstract: "Recent approaches have achieved great successes in image generation from structured inputs, e.g., semantic segmentation, scene graph or layout. Although these methods allow specification of objects and their locations at image-level, they lack the fidelity and semantic control to specify visual appearance of these objects at an instance-level. To address this limitation, we propose a new image generation method that enables instance-level attribute specification. Specifically, the input to our attribute-guided generative model is a tuple that contains: (1) object bounding boxes, (2) object categories and (3) a (optional) set of attributes for each object. The output is a generated image where the requested objects are in the desired locations and have prescribed attributes. Several losses work collaboratively to encourage accurate, consistent and diverse image generation. Experiments on Visual Genome datasets demonstrate our model's capacity to control object-level attributes in generated images, and validate plausibility of disentangled object-attribute representation in the image generation from layout task. Also, the generated images from our model have higher resolution, object classification accuracy and consistency than the previous state-of-the-art. "
  - id: 391
    order: 160
    poster_session: 4
    session_id: 11
    title: "Refinement of Boundary Regression Using Uncertainty in Temporal Action Localization"
    authors:
      - author: "Yunze Chen (Center of Precision Sensing and Control, Institute of Automation, Chinese Academy of Sciences, School of Artificial Intelligence, University of Chinese Academy of Sciences)"
      - author: "Mengjuan Chen (Center of Precision Sensing and Control, Institute of Automation, Chinese Academy of Sciences)"
      - author: "Rui Wu (Horizon Robotics)"
      - author: "Jiagang Zhu (Institute of Automation, Chinese Academy of Sciences, University of Chinese Academy of Sciences )"
      - author: "Zheng Zhu (Institute of Automation, Chinese Academy of Sciences)"
      - author: "Qingyi Gu (Institute of Automation, Chinese Academy of Sciences)"
    all_authors: "Yunze Chen, Mengjuan Chen, Rui Wu, Jiagang Zhu, Zheng Zhu and Qingyi Gu"
    code: ""
    keywords:
      - word: "Temporal Action Localization"
      - word: "Temporal Action Detection"
      - word: "Activity recognition and understanding"
      - word: ""
    paper: "papers/0391.pdf"
    supp: ""
    abstract: "Boundary localization is a key component of most temporal action localization frameworks for untrimmed video. Deep-learning methods have brought remarkable progress in this field due to large-scale annotated datasets (e.g., THUMOS14 and ActivityNet). However, natural ambiguity exists for labeling accurate action boundary with such datasets. In this paper, we propose a method to model this uncertainty. Specifically, we construct a Gaussian model for predicting the uncertainty variance of boundary. The captured variance is further used to select more reliable proposals, and to refine proposal boundary by variance voting during post-processing. For most existing one- and two-stage frameworks, more accurate boundaries and reliable proposals can be obtained without additional computation. For the one-stage decoupled single-shot temporal action detection (Decouple-SSAD) framework, we first apply adaptive pyramid feature fusion method to fuse its features of different scales and optimize its structure. Then, we introduce the uncertainty based method, and improve state-of-the-art mAP@0.5 value from 37.9% to 41.6% on THUMOS14. Moreover, for the two-stage proposal–proposal interaction through a graph convolutional network (P-GCN), with such uncertainty method, we also gain significant improvements on both THUMOS14 and ActivityNet v1.3 datasets."
  - id: 395
    order: 166
    poster_session: 4
    session_id: 11
    title: "View-consistent 4D Light Field Depth Estimation"
    authors:
      - author: "Numair Khan (Brown University)"
      - author: "Min H. Kim (KAIST)"
      - author: "James Tompkin (Brown University)"
    all_authors: "Numair Khan, Min H. Kim and James Tompkin"
    code: "http://visual.cs.brown.edu/lightfielddepth"
    keywords:
      - word: "light fields"
      - word: "depth estimation"
      - word: "angular consistency"
      - word: "epipolar plane images"
      - word: "depth inpainting"
      - word: "diffusion"
      - word: "light field editing"
    paper: "papers/0395.pdf"
    supp: "supp/0395_supp.mp4"
    abstract: "We propose a method to compute depth maps for every sub-aperture image in a light field in a view consistent way. Previous light field depth estimation methods typically estimate a depth map only for the central sub-aperture view, and struggle with view consistent estimation. Our method precisely defines depth edges via EPIs, then we diffuse these edges spatially within the central view. These depth estimates are then propagated to all other views in an occlusion-aware way. Finally, disoccluded regions are completed by diffusion in EPI space. Our method runs efficiently with respect to both other classical and deep learning-based approaches, and achieves competitive quantitative metrics and qualitative performance on both synthetic and real-world light fields."
  - id: 400
    order: 124
    poster_session: 3
    session_id: 8
    title: "From Quantized DNNs to Quantizable DNNs"
    authors:
      - author: "Kunyuan Du (Cooperative Medianet Innovation Center, Shang hai Jiao Tong University)"
      - author: "Ya Zhang (Cooperative Medianet Innovation Center, Shang hai Jiao Tong University)"
      - author: "Haibing Guan (Shanghai Jiao Tong University)"
    all_authors: "Kunyuan Du, Ya Zhang and Haibing Guan"
    code: ""
    keywords:
      - word: "Quantized DNNs"
      - word: "Dynamic Bit-width"
    paper: "papers/0400.pdf"
    supp: ""
    abstract: "This paper proposes Quantizable DNNs, a special type of DNNs that can flexibly quantize its bit-width (denoted as `bit modes' thereafter) during execution without further re-training. To simultaneously optimize all bit modes, a combinational loss of all bit modes is proposed, which enforces consistent predictions ranging from low-bit mode to 32-bit mode. This consistency-based loss may also be viewed as certain form of regularization during training. Because outputs of matrix multiplication in different bit modes have different distributions, we introduce Bit-Specific Batch Normalization so as to reduce conflicts among different bit modes. Experiments on CIFAR100 and ImageNet have shown that compared to quantized DNNs, quantizable DNNs not only have much better flexibility, but also achieve even higher classification accuracy. Ablation studies further verify that the regularization through the consistency-based loss indeed improves the model's generalization performance. Source codes will be released in the future."
  - id: 401
    order: 128
    poster_session: 3
    session_id: 8
    title: "RankPose: Learning Generalised Feature with Rank Supervision for Head Pose Estimation"
    authors:
      - author: "donggen dai (None)"
      - author: "Wangkit Wong (PingAn Tech.)"
      - author: "Zhuojun Chen (Baidu Inc.)"
    all_authors: "Donggen Dai, Wangkit Wong and Zhuojun Chen"
    code: "https://github.com/seathiefwang/RankPose"
    keywords:
      - word: "head pose estimation"
      - word: "rank supervision"
      - word: "arccos transformation"
      - word: "ranking loss"
    paper: "papers/0401.pdf"
    supp: ""
    abstract: "We address the challenging problem of RGB image-based head pose estimation. We first reformulate head pose representation learning to constrain it to a bounded space. Head pose represented as vector projection or vector angles shows helpful to improving performance. Further, a ranking loss combined with MSE regression loss is proposed. The ranking loss supervises a neural network with paired samples of the same person and penalises incorrect ordering of pose prediction. Analysis on this new loss function suggests it contributes to a better local feature extractor, where features are generalised to Abstract Landmarks which are pose-related features instead of pose-irrelevant information such as identity, age, and lighting. Extensive experiments show that our method significantly outperforms the current state-of-the-art schemes on public datasets: AFLW2000 and BIWI. Our model achieves significant improvements over previous SOTA MAE on AFLW2000 and BIWI from 4.50 [11] to 3.66 and from 4.0 [24] to 3.71 respectively. Source code is available at: https://github.com/seathiefwang/RankHeadPose."
  - id: 406
    order: 143
    poster_session: 3
    session_id: 8
    title: "Loss Functions for Person Image Generation"
    authors:
      - author: "Haoyue Shi (Xi’an Jiaotong University)"
      - author: "Le Wang (Xi'an Jiaotong University)"
      - author: "Wei Tang (University of Illinois at Chicago)"
      - author: "Nanning Zheng (Xi'an Jiaotong University)"
      - author: "Gang Hua (Wormpex AI Research)"
    all_authors: "Haoyue Shi, Le Wang, Wei Tang, Nanning Zheng and Gang Hua"
    code: ""
    keywords:
      - word: "person image generation"
      - word: "pose transfer"
      - word: "generative adversarial networks"
      - word: "structural similarity loss"
    paper: "papers/0406.pdf"
    supp: ""
    abstract: "Pose-guided person image generation aims to transform a source person image to a target pose. It is an ill-posed problem as we often need to generate pixels that are invisible in the source image. Recent works focus on designing new architectures of deep neural networks and have shown promising results. However, they simply adopt the loss functions commonly used for generic image synthesis and restoration, e.g., L1 loss, adversarial loss, and perceptual loss. This can be suboptimal due to the unique appearance and structure patterns of person images. In this paper, we first have a comprehensive study of the strengths and weaknesses of these prior loss functions for person image generation. We also consider the structural similarity index (SSIM) as a loss function since it is widely used as the evaluation metric and can capture the perceptual quality of generated images. Moreover, motivated by the observation that a person can be divided into part regions with homogeneous pixel values or textures, we extend the SSIM into a novel part-based similarity loss to explicitly account for the articulated body structure. Quantitative and qualitative results indicate that (1) using different loss functions significantly impacts the generated person images and (2) the proposed part-based loss is complementary to the prior losses and helps improve the performance."
  - id: 410
    order: 168
    poster_session: 4
    session_id: 11
    title: "Sentence Guided Temporal Modulation for Dynamic Video Thumbnail Generation"
    authors:
      - author: "Mrigank Rochan (University of Manitoba)"
      - author: "Mahesh Kumar Krishna Reddy (University of Manitoba)"
      - author: "Yang Wang (University of Manitoba)"
    all_authors: "Mrigank Rochan, Mahesh Kumar Krishna Reddy and Yang Wang"
    code: ""
    keywords:
      - word: "video thumbnail generation"
      - word: "conditional normalization"
    paper: "papers/0410.pdf"
    supp: ""
    abstract: "We consider the problem of sentence specified dynamic video thumbnail generation. Given an input video and a user query sentence, the goal is to generate a video thumbnail that not only provides the preview of the video content, but also semantically corresponds to the sentence. In this paper, we propose a sentence guided temporal modulation (SGTM) mechanism that utilizes the sentence embedding to modulate the normalized temporal activations of the video thumbnail generation network. Unlike the existing state-of-the-art method that uses recurrent architectures, we propose a non-recurrent framework that is simple and allows much more parallelization. Extensive experiments and analysis on a large-scale dataset demonstrate the effectiveness of our framework."
  - id: 413
    order: 81
    poster_session: 2
    session_id: 5
    title: "PMD-Net: Privileged Modality Distillation Network for 3D Hand Pose Estimation from a Single RGB Image"
    authors:
      - author: "Kewen Wang (Institute of Computing Technology of the Chinese Academy of Sciences)"
      - author: "Xilin Chen (Institute of Computing Technology, Chinese Academy of Sciences)"
    all_authors: "Kewen Wang and Xilin Chen"
    code: ""
    keywords:
      - word: "hand pose estimation"
      - word: "modality distillation"
      - word: "privileged information"
    paper: "papers/0413.pdf"
    supp: ""
    abstract: "3D Hand Pose Estimation from a single RGB image is a challenging task due to the significant depth ambiguities and occlusions. In this paper, we propose a Privileged Modality Distillation Network (PMD-Net), which improves the RGB-based hand pose estimation by excavating the privileged information from depth prior during training. Different from existing methods, the PMD-Net is composed of three sub-networks to regress X, Y, and Z coordinates respectively and distills the privileged information from the depth network to the RGB network by transferring constraints between corresponded layers. Furthermore, a random block replacement is adopted and a refine module is added to enhance the robustness of PMD-Net. Experiments on both synthesized and real-world hand pose estimation datasets are conducted, and extensive results demonstrate that the proposed PMD-Net achieves state-of-the-art results and is superior to existing methods."
  - id: 421
    order: 57
    oral_session: 4
    poster_session: 2
    session_id: 4
    title: "Visibility-aware Multi-view Stereo Network"
    authors:
      - author: "Jingyang Zhang (The Hong Kong University of Science and Technology)"
      - author: "Yao Yao (The Hong Kong University of Science and Technology)"
      - author: "Shiwei Li (Altizure)"
      - author: "Zixin Luo (HKUST)"
      - author: "Tian Fang (Altizure)"
    all_authors: "Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo and Tian Fang"
    code: "https://github.com/jzhangbs/Vis-MVSNet"
    keywords:
      - word: "multiview stereo"
      - word: "3d reconstruction"
    paper: "papers/0421.pdf"
    supp: "supp/0421_supp.pdf"
    abstract: "Learning-based multi-view stereo (MVS) methods have demonstrated promising results. However, very few existing networks explicitly take the pixel-wise visibility into consideration, resulting in erroneous cost aggregation from occluded pixels. In this paper, we explicitly infer and integrate the pixel-wise occlusion information in the MVS network via the matching uncertainty estimation. The pair-wise uncertainty map is jointly inferred with the pair-wise depth map, which is further used as weighting guidance during the multi-view cost volume fusion. As such, the adverse influence of occluded pixels is suppressed in the cost fusion. The proposed framework Vis-MVSNet significantly improves depth accuracies in the scenes with severe occlusion. Extensive experiments are performed on DTU, BlendedMVS, and Tanks and Temples datasets to justify the effectiveness of the proposed framework."
  - id: 423
    order: 5
    oral_session: 2
    poster_session: 1
    session_id: 1
    title: "WAMDA: Weighted Alignment of Sources for Multi-source Domain Adaptation"
    authors:
      - author: "Surbhi Aggarwal (Indian Institute of Science)"
      - author: "Jogendra Nath Kundu (Indian Institute of Science)"
      - author: "Venkatesh Babu Radhakrishnan (Indian Institute of Science)"
      - author: "Anirban Chakraborty (Indian Institute of Science)"
    all_authors: "Surbhi Aggarwal, Jogendra Nath Kundu, Venkatesh Babu Radhakrishnan and Anirban Chakraborty"
    code: ""
    keywords:
      - word: "Domain Adaptation"
    paper: "papers/0423.pdf"
    supp: "supp/0423_supp.zip"
    abstract: "Unsupervised Domain Adaptation aims to learn a model for an unlabelled target domain, given access to a single labelled but differently distributed source domain. However, often multiple labelled sources which share complementary information are present, resulting in the more practical problem of multi-source domain adaptation (MSDA). Recent works in MSDA learn a domain-invariant space from the sources and target. However, they treat each source to be equally relevant to the target and are not sensitive towards the intrinsic statistical similarities amongst domains. In this work, we propose a novel method for MSDA, termed WAMDA, which utilizes the multiple sources based on their relative importance to the target. Our aim is to explore the relevance of each source-target alignment and source-source alignment, and then perform weighted alignment of domains by using the relevance scores. We experimentally validate the performance of our proposed method on multiple datasets, and achieve either state-of-the-art results or competitive performances across all these datasets."
  - id: 430
    order: 7
    oral_session: 2
    poster_session: 1
    session_id: 1
    title: "Weakly-supervised Salient Instance Detection"
    authors:
      - author: "Xin Tian (Dalian University of Technology, City University of Hong Kong)"
      - author: "Ke Xu (Dalian University of Technology, City University of Hong Kong)"
      - author: "Xin Yang (Dalian University of Technology)"
      - author: "Baocai Yin (Dalian University of Technology)"
      - author: "Rynson Lau (City University of Hong Kong)"
    all_authors: "Xin Tian, Ke Xu, Xin Yang, Baocai Yin and Rynson Lau"
    code: ""
    keywords:
      - word: "Salient Instance Detection"
      - word: "SID"
      - word: "weak supervision"
      - word: "saliency detection"
      - word: "subitizing"
      - word: ""
    paper: "papers/0430.pdf"
    supp: "supp/0430_supp.zip"
    abstract: "Existing salient instance detection (SID) methods typically learn from pixel-level annotated datasets. In this paper, we present the first weakly-supervised approach to the SID problem. Although weak supervision has been considered in general saliency detection, it is mainly based on using class labels for object localization. However, it is non-trivial to use only class labels to learn instance-aware saliency information, as salient instances with high semantic affinities may not be easily separated by the labels. We note that subitizing information provides an instant judgement on the number of salient items, which naturally relates to detecting salient instances and may help separate instances of the same class while grouping different parts of the same instance. Inspired by this insight, we propose to use class and subitizing labels as weak supervision for the SID problem. We propose a novel weakly-supervised network with three branches: a Saliency Detection Branch leveraging class consistency information to locate candidate objects; a Boundary Detection Branch exploiting class discrepancy information to delineate object boundaries; and a Centroid Detection Branch using subitizing information to detect salient instance centroids. This complementary information is further fused to produce salient instance maps. We conduct extensive experiments to demonstrate that the proposed method plays favorably against carefully designed baseline methods adapted from related tasks."
  - id: 436
    order: 10
    poster_session: 1
    session_id: 2
    title: "Determinantal Point Process as an alternative to NMS"
    authors:
      - author: "Samik Some (IIT Kanpur)"
      - author: "Mithun Gupta (Microsoft,India)"
      - author: "Vinay Namboodiri (University of Bath)"
    all_authors: "Samik Some, Mithun Gupta and Vinay Namboodiri"
    code: "https://cse.iitk.ac.in/users/samik/dpp/code.zip"
    keywords:
      - word: "dpp"
      - word: "determinantal point process"
      - word: "object detection"
      - word: "faster-rcnn"
      - word: "nms"
    paper: "papers/0436.pdf"
    supp: ""
    abstract: "We present a determinantal point process (DPP) inspired alternative to non-maximum suppression (NMS) which has become an integral step in all state-of-the-art object detection frameworks. DPPs have been shown to encourage diversity in subset selection problems. We pose NMS as a subset selection problem and posit that directly incorporating DPP like framework can improve the overall performance of the object detection system. We propose an optimization problem which takes the same inputs as NMS, but introduces a novel sub-modularity based diverse subset selection functional. Our results strongly indicate that the modifications proposed in this paper can provide consistent improvements to state-of-the-art object detection pipelines."
  - id: 440
    order: 85
    poster_session: 2
    session_id: 5
    title: "Align-and-Attend Network for Globally and Locally Coherent Video Inpainting"
    authors:
      - author: "Sanghyun Woo (KAIST)"
      - author: "Dahun Kim (KAIST)"
      - author: "KwanYong Park (KAIST)"
      - author: "Joon-Young Lee (Adobe Research)"
      - author: "In So Kweon (KAIST, Korea)"
    all_authors: "Sanghyun Woo, Dahun Kim, KwanYong Park, Joon-Young Lee and In So Kweon"
    code: ""
    keywords:
      - word: "Video Inpainting"
      - word: "Video Processing"
      - word: "Spatio-Temporal Alignment"
      - word: "Spatio-Temporal Non-local Attention"
    paper: "papers/0440.pdf"
    supp: "supp/0440_supp.zip"
    abstract: "Video inpainting is more challenging than image inpainting because of the extra temporal dimension. It requires inpainted contents to be globally coherent in both space and time. A natural solution for this problem is aggregating features from other frames, and thus, existing state-of-the-art methods rely heavily on 3D convolution or optical flow. However, these methods emphasize more on the temporally nearby frames, and long-term temporal information is not sufficiently stressed. In this work, we propose a novel two-stage alignment method. The first stage is an alignment module that uses computed homographies between the target frame and the reference frames. The visible patches are then aggregated based on the frame similarity to fill in the target holes roughly. Despite being able to model only global transformations, we empirically verify that homography-based alignment allows larger temporal window size than the flow-based counterpart. The second stage is an attention module that matches the generated patches with known reference patches in a non-local manner to refine the previous global alignment stage. Both stages consist of large spatial-temporal window size for the reference and thus enable modeling long-range correlations between distant information and the hole regions. Finally, even challenging scenes with large or slowly moving holes can be handled, which have been hardly modeled by existing approaches. Experiments on video object removal demonstrate that our method significantly outperforms previous state-of-the-art learning approaches."
  - id: 443
    order: 62
    poster_session: 2
    session_id: 5
    title: "A Simple and Scalable Shape Representation for 3D Reconstruction."
    authors:
      - author: "Mateusz Michalkiewicz (University of Queensland)"
      - author: "Eugene Belilovsky (Mila)"
      - author: "Mahsa Baktashmotlagh (University of Queensland)"
      - author: "Anders Eriksson (University of Queensland )"
    all_authors: "Mateusz Michalkiewicz, Eugene Belilovsky, Mahsa Baktashmotlagh and Anders Eriksson"
    code: ""
    keywords:
      - word: "shape from x"
      - word: "3d reconstruction from a single image"
      - word: "implicit shape representation"
      - word: "deep level sets"
      - word: ""
    paper: "papers/0443.pdf"
    supp: "supp/0443_supp.pdf"
    abstract: "Deep learning applied to the reconstruction of 3D shapes has seen growing interest. A popular approach to 3D reconstruction and generation in recent years has been the CNN encoder-decoder model usually applied in voxel space. However, this often scales very poorly with the resolution limiting the effectiveness of these models. Several sophisticated alternatives for decoding to 3D shapes have been proposed typically relying on complex deep learning architectures for the decoder model. In this work, we show that this additional complexity is not necessary, and that we can actually obtain high quality 3D reconstruction using a linear decoder, obtained from principal component analysis on the signed distance function (SDF) of the surface. This approach allows easily scaling to larger resolutions. We show in multiple experiments that our approach is competitive with state-of-the-art methods. It also allows the decoder to be fine-tuned on the target task using a loss designed specifically for SDF transforms, obtaining further gains. "
  - id: 445
    order: 21
    poster_session: 1
    session_id: 2
    title: "Physics-informed detection and segmentation of type II solar radio bursts"
    authors:
      - author: "Joseph Jenkins (Swansea University)"
      - author: "Adeline Paiement (Université de Toulon)"
      - author: "Jean Aboudarham (Swansea University)"
      - author: "Xavier Bonnin (Swansea University)"
    all_authors: "Joseph Jenkins, Adeline Paiement, Jean Aboudarham and Xavier Bonnin"
    code: ""
    keywords:
      - word: "physics-informed machine learning"
      - word: "domain knowledge"
      - word: "detection"
      - word: "segmentation"
      - word: "applied computer vision"
      - word: "solar spectrogram"
      - word: "astrophysics data"
    paper: "papers/0445.pdf"
    supp: "supp/0445_supp.pdf"
    abstract: "Type II solar radio bursts have proven to be a useful tool for gaining insights into the behaviour of complex solar events and for forecasting and mitigating their damages on Earth. In this work, we detect and segment the occurrence of type II bursts in solar radio spectrograms, thereby facilitating the extraction of parameters needed to gain insight into solar events. We utilise prior knowledge of how type II bursts drift through frequencies over time to assist with these tasks of detection and segmentation. A new adaptive Region of Interest (ROI) is proposed, to constrain the search to regions that follow the burst curvature at a given frequency. It comes with an implicit data normalisation that reduces the variance of burst appearance in the data, hence simplifying the learning process from small datasets. We demonstrate the effectiveness of our methodology using a simple and popular HOG and logistic regression detector and basic segmentation based on voting and background subtraction. On a custom dataset representative of different levels of solar activity, at a wavelength range where no other detection algorithm currently operates, our adaptive ROI significantly improves over traditional sliding windows. In future work, it may be applied to other, state-of-the-art, machine learning algorithms."
  - id: 448
    order: 34
    poster_session: 1
    session_id: 2
    title: "SD-MTCNN: Self-Distilled Multi-Task CNN"
    authors:
      - author: "Ankit Jha (IIT Bombay)"
      - author: "Awanish Kumar (Indian Institute of Technology Bombay)"
      - author: "Biplab Banerjee (Indian Institute of Technology, Bombay)"
      - author: "Vinay Namboodiri (IIT Kanpur)"
    all_authors: "Ankit Jha, Awanish Kumar, Biplab Banerjee and Vinay Namboodiri"
    code: ""
    keywords:
      - word: "self-distillation"
      - word: "multi-task learning"
    paper: "papers/0448.pdf"
    supp: "supp/0448_supp.pdf"
    abstract: "Multi-task learning (MTL) using convolutional neural networks (CNN) deals with training the network for multiple correlated tasks in concert.
For accuracy-critical applications, there are endeavors to boost the model performance by resorting to a deeper network, which also increases the model complexity. However, such burdensome models are difficult to be deployed on mobile or edge devices.
To ensure a trade-off between performance and complexity of CNNs in the context of MTL, we introduce the novel paradigm of self-distillation within the network. Different from traditional knowledge distillation (KD), which trains the Student in accordance with a cumbersome Teacher, our self-distilled multi-task CNN model: SD-MTCNN aims at distilling knowledge from deeper CNN layers into the shallow layers. Precisely, we follow a hard-sharing based MTL setup where all the tasks share a generic feature-encoder on top of which separate task-specific decoders are enacted. Under this premise, SD-MTCNN distills the more abstract features from the decoders to the encoded feature space, which guarantees improved multi-task performance from different parts of the network.
We validate SD-MTCNN on three benchmark datasets: CityScapes, NYUv2, and Mini-Taskonomy, and results confirm the improved generalization capability of self-distilled multi-task CNNs in comparison to the literature and baselines."
  - id: 451
    order: 121
    poster_session: 3
    session_id: 8
    title: "Class Interference Regularization"
    authors:
      - author: "Bharti Munjal (OSRAM)"
      - author: "Sikandar Amin (OSRAM GmbH)"
      - author: "Fabio Galasso (Sapienza University)"
    all_authors: "Bharti Munjal, Sikandar Amin and Fabio Galasso"
    code: ""
    keywords:
      - word: "regularization"
      - word: "few-shot learning"
      - word: "person search"
      - word: "person re-identification"
      - word: "average embeddings"
      - word: "class interference"
      - word: "classification"
    paper: "papers/0451.pdf"
    supp: ""
    abstract: "Contrastive losses yield state-of-the-art performance for person re-identification, face verification and few shot learning. They have recently outperformed the cross-entropy loss on classification at the ImageNet scale and outperformed all self-supervision prior results by a large margin (SimCLR). Simple and effective regularization techniques such as label smoothing and self-distillation do not apply anymore, because they act on multinomial label distributions, adopted in cross-entropy losses, and not on tuple comparative terms, which characterize the contrastive losses.

Here we propose a novel, simple and effective regularization technique, the Class Interference Regularization (CIR), which applies to cross-entropy losses but is especially effective on contrastive losses. CIR perturbs the output features by randomly moving them towards the average embeddings of the negative classes. To the best of our knowledge, CIR is the first regularization technique to act on the output features.

In experimental evaluation, the combination of CIR and a plain Siamese-net with
triplet loss yields best few-shot learning performance on the challenging tieredImageNet. CIR also improves the state-of-the-art technique in person re-identification on the Market-1501 dataset, based on triplet loss, and the state-of-the-art technique in person search on the CUHK-SYSU dataset, based on a cross-entropy loss. Finally, on the task of classification CIR performs on par with the popular label smoothing, as demonstrated for CIFAR-10 and -100.
"
  - id: 452
    order: 180
    poster_session: 4
    session_id: 11
    title: "Towards Convolutional Neural Networks Compression via Global&Progressive Product Quantization"
    authors:
      - author: "Weihan Chen (CASIA)"
      - author: "Peisong  Wang (Institute of Automation, Chinese Academy of Sciences)"
      - author: "Jian Cheng (Chinese Academy of Sciences, China)"
    all_authors: "Weihan Chen, Peisong  Wang and Jian Cheng"
    code: ""
    keywords:
      - word: "convolutional neural network compression"
      - word: "product quantization"
    paper: "papers/0452.pdf"
    supp: ""
    abstract: "In recent years, we have witnessed the great success of convolutional neural networks in a wide range of visual applications. However, these networks are typically deficient due to the high cost in storage and computation, which prohibits their further extensions to resource-limited applications. In this paper, we introduce Global&Progressive Product Quantization(G&P PQ), an end-to-end product quantization based network compression method, to merge the separate quantization and finetuning process into a consistent training framework. Compared to existing two-stage methods, we avoid the time-consuming process of choosing layer-wise finetuning hyperparameters and also make the network capable of learning complex dependencies among layers by quantizing globally and progres- sively. To validate the effectiveness, we benchmark G&P PQ by applying it to ResNet-like architectures for image classification and demonstrate state-of- the-art tradeoff in terms of model size vs. accuracy with extensive compression configurations.
"
  - id: 453
    order: 60
    poster_session: 2
    session_id: 5
    title: "Domain Adaptation Regularization for Spectral Pruning"
    authors:
      - author: "Laurent Dillard (None)"
      - author: "Yosuke Shinya (DENSO CORPORATION)"
      - author: "Taiji Suzuki (The University of Tokyo / RIKEN)"
    all_authors: "Laurent Dillard, Yosuke Shinya and Taiji Suzuki"
    code: ""
    keywords:
      - word: "model compression"
      - word: "domain adaptation"
      - word: "pruning"
      - word: ""
    paper: "papers/0453.pdf"
    supp: "supp/0453_supp.zip"
    abstract: "Deep Neural Networks (DNNs) have recently been achieving state-of-the-art performance on a variety of computer vision related tasks. However, their computational cost limits their ability to be implemented in embedded systems with restricted resources or strict latency constraints. Model compression has therefore been an active field of research to overcome this issue. Additionally, DNNs typically require massive amounts of labeled data to be trained. This represents a second limitation to their deployment. Domain Adaptation (DA) addresses this issue by allowing knowledge learned on one labeled source distribution to be transferred to a target distribution, possibly unlabeled. In this paper, we investigate on possible improvements of compression methods in DA setting. We focus on a compression method that was previously developed in the context of a single data distribution and show that, with a careful choice of data to use during compression and additional regularization terms directly related to DA objectives, it is possible to improve compression results. We also show that our method outperforms an existing compression method studied in the DA setting by a large margin for high compression rates. Although our work is based on one specific compression method, we also outline some general guidelines for improving compression in DA setting."
  - id: 473
    order: 58
    oral_session: 4
    poster_session: 2
    session_id: 4
    title: "Pano2Scene: 3D Indoor Semantic Scene Reconstruction from a Single Indoor Panorama Image"
    authors:
      - author: "Wei Zeng (University of Amsterdam)"
      - author: "Sezer Karaoglu (University of Amsterdam)"
      - author: "Theo Gevers (University of Amsterdam)"
    all_authors: "Wei Zeng, Sezer Karaoglu and Theo Gevers"
    code: ""
    keywords:
      - word: "3D point cloud"
      - word: "3D reconstruction"
      - word: "panorama"
      - word: "scene reconstruction"
      - word: "single image reconstruction"
      - word: "scene understanding"
      - word: "semantic scene reconstruction"
    paper: "papers/0473.pdf"
    supp: ""
    abstract: "3D indoor semantic scene reconstruction from 2D images is challenging as it requires both scene understanding and object reconstruction. Compared to perspective images, panoramas provide larger field of view and carry more scene information. In this paper, to reconstruct the 3D indoor semantic scene from a single panorama image, we propose a pipeline that jointly learns to predict the 3D scene layout, complete the object shapes and reconstruct the full scene point cloud. Experiments on the Stanford 2D-3D dataset demonstrate the generality and suitability of the proposed method."
  - id: 474
    order: 175
    poster_session: 4
    session_id: 11
    title: "The ADUULM-Dataset - a Semantic Segmentation Dataset for Sensor Fusion"
    authors:
      - author: "Andreas Pfeuffer (Ulm University)"
      - author: "Markus Schön (Ulm University)"
      - author: "Ditzel Carsten ()"
      - author: "Klaus Dietmayer (Ulm University)"
    all_authors: "Andreas Pfeuffer, Markus Schön, Ditzel Carsten and Klaus Dietmayer"
    code: "https://www.uni-ulm.de/in/iui-drive-u/projekte/aduulm-dataset/"
    keywords:
      - word: "semantic segmentation"
      - word: "sensor fusion"
      - word: "video segmentation"
      - word: "autonomous driving dataset"
      - word: ""
    paper: "papers/0474.pdf"
    supp: ""
    abstract: "One of the key challenges of today's semantic segmentation approaches is to obtain robust and reliable segmentation results not only in good weather conditions, but also in adverse weather conditions such as darkness, fog or heavy rain. For this purpose, multiple sensor data of several sensor types such as camera and lidar are required to compensate the weather sensitivity of individual sensors. Hence, a semantic segmentation dataset is necessary, which contains camera and lidar data, but until recently, no such dataset exists.  
Therefore, the ADUULM dataset was created, a semantic segmentation dataset which consists of fine-annotated camera data and pixel-wise labeled lidar data recorded in diverse weather conditions. Additionally, the corresponding GPS, IMU and stereo information are provided, and for each annotated data sample, a short video-sequence is available, too.
Furthermore, state-of-the-art semantic segmentation and drivable area detection approaches are evaluated on the proposed dataset, and it turned out that new methods are required to obtain robust and reliable results in adverse weather conditions. The ADUULM-dataset will be available online at https://www.uni-ulm.de/en/in/driveu/projects"
  - id: 476
    order: 135
    poster_session: 3
    session_id: 8
    title: "On Modality Bias in the TVQA Dataset"
    authors:
      - author: "Thomas Winterbottom (University of Durham)"
      - author: "Sarah Xiao (University of Durham)"
      - author: "Alistair McLean (Carbon DMP)"
      - author: "Noura Al Moubayed (University of Durham)"
    all_authors: "Thomas Winterbottom, Sarah Xiao, Alistair McLean and Noura Al Moubayed"
    code: "https://github.com/Jumperkables/tvqa_modality_bias"
    keywords:
      - word: "Multimodality"
      - word: "Unimodal Bias"
      - word: "Dataset Bias"
      - word: "TVQA"
      - word: "Video-QA"
      - word: "BERT"
      - word: "Bilinear Pooling"
      - word: "TVQA+"
    paper: "papers/0476.pdf"
    supp: "supp/0476_supp.zip"
    abstract: "TVQA is a large scale video question answering (video-QA) dataset based on popular TV shows. The questions were specifically designed to require \"both vision and language understanding to answer\". In this work, we demonstrate an inherent bias in the dataset towards the textual subtitle modality. We infer said bias both directly and indirectly, notably finding that models trained with subtitles learn, on-average, to suppress video feature contribution. Our results demonstrate that models trained on only the visual information can answer ~45% of the questions, while using only the subtitles achieves ~68%. We find that a bilinear pooling based joint representation of modalities damages model performance by 9% implying a reliance on modality specific information. We also show that TVQA fails to benefit from the RUBi modality bias reduction technique popularised in VQA. By simply improving text processing using BERT embeddings with the simple model first proposed for TVQA, we achieve state-of-the-art results (72.13%) compared to the highly complex STAGE model (70.50%). We recommend a multimodal evaluation framework that can highlight biases in models and isolate visual and textual reliant subsets of data. Using this framework we propose subsets of TVQA that respond exclusively to either or both modalities in order to facilitate multimodal modelling as TVQA originally intended."
  - id: 479
    order: 70
    poster_session: 2
    session_id: 5
    title: "Making L-BFGS Work with Industrial-Strength Nets"
    authors:
      - author: "Abhay Yadav (University of Maryland)"
      - author: "Tom Goldstein (University of Maryland, College Park)"
      - author: "David Jacobs (University of Maryland, USA)"
    all_authors: "Abhay Yadav, Tom Goldstein and David Jacobs"
    code: ""
    keywords:
      - word: "deep network training"
      - word: "efficient training"
      - word: "second-order optimization"
      - word: ""
    paper: "papers/0479.pdf"
    supp: "supp/0479_supp.pdf"
    abstract: "L-BFGS has been one of the most popular methods for convex optimization, but good performance by L-BFGS in deep learning has been elusive.
Recent work has modified L-BFGS for deep networks for classification tasks and been able to show performance competitive with SGD and Adam (the most popular current algorithms) when batch normalization is not used. 
However, this work cannot be applied with batch normalization. Since batch normalization is a defacto standard and important to good performance in deep networks, this still limits the use of L-BFGS. In this paper, we address this issue. Our proposed method can be used as a drop-in replacement without changing existing code. The proposed method performs consistently better than Adam and existing L-BFGS approaches, and comparable to carefully tuned SGD. We show results on three datasets, CIFAR-10, CIFAR-100, and STL-10 using three different popular deep networks ResNet, DenseNet and Wide ResNet. This work marks another significant step towards making L-BFGS 
competitive in the deep learning community."
  - id: 481
    order: 22
    poster_session: 1
    session_id: 2
    title: "Learning 3D Global Human Motion Estimation from Unpaired, Disjoint Datasets"
    authors:
      - author: "Julian Habekost (University of Edinburgh)"
      - author: "Takaaki Shiratori (Facebook Reality Labs)"
      - author: "Yuting Ye (Facebook Reality Labs)"
      - author: "Taku Komura (University of Edinburgh)"
    all_authors: "Julian Habekost, Takaaki Shiratori, Yuting Ye and Taku Komura"
    code: ""
    keywords:
      - word: "3D human pose estimation"
      - word: "3D global motion estimation"
      - word: "unpaired training"
      - word: "2d to 3d pose regression"
      - word: "monocular motion capture"
      - word: "human time sequence modeling"
      - word: ""
    paper: "papers/0481.pdf"
    supp: "supp/0481_supp.mp4"
    abstract: "We propose a novel method to compute both the local and global 3D motion of the human body from a 2D monocular video. Our approach only uses unpaired sets of 2D keypoints from target videos and 3D motion capture data for training. The estimation target video dataset is assumed to lack any ground truth and thus our supervision signal comes from motion datasets that are fully disjoint from the target datasets. For each time step, a temporal convolutional generator configures the human pose in the global space to satisfy both a reprojection loss and an adversarial loss. The translational and rotational global motion is then derived and converted into the egocentric representation in a differentiable manner for adversarial learning. We compare our system to state-of-the-art architectures that use the Human3.6M dataset for paired training, and demonstrate comparable precision even though our system is never trained on the ground truth Human3.6M 3D motion capture data. Due to its unpaired and disjoint nature in the training data, our system can be trained on a large set of videos and 3D motion capture data, which can considerably expand the domain of the applicable motion data types."
  - id: 493
    order: 115
    poster_session: 3
    session_id: 8
    title: "AOL: Adaptive Online Learning for Human Trajectory Prediction in Dynamic Video Scenes"
    authors:
      - author: "Manh Huynh (ucdenver)"
      - author: "Gita Alaghband (University of Colorado Denver)"
    all_authors: "Manh Huynh and Gita Alaghband"
    code: ""
    keywords:
      - word: "trajectory prediction"
      - word: "adaptive online learning"
    paper: "papers/0493.pdf"
    supp: ""
    abstract: "We present a novel adaptive online learning (AOL) framework to predict human movement trajectories in dynamic video scenes. Our framework learns and adapts to changes in the scene environment and generates best network weights for different scenarios. The framework can be applied to prediction models and improve their performance as it dynamically adjusts when it encounters changes in the scene and can apply the best training weights for predicting the next locations. We demonstrate this by integrating our framework with two existing prediction models: LSTM [3] and Future Person Location (FPL) [1]. Furthermore, we analyze the number of network weights for optimal performance and show that we can achieve real-time with a fixed number of networks using the least recently used (LRU) strategy for maintaining the most recently trained network weights. With extensive experiments, we show that our framework increases prediction accuracies of LSTM and FPL by ~17% and 28% on average, and up to ~50% for FPL on the worst case while achieving real-time (20fps)."
  - id: 512
    order: 161
    poster_session: 4
    session_id: 11
    title: "Real-time screen reading: reducing domain shift for one-shot learning"
    authors:
      - author: "James Charles (Cambridge University)"
      - author: "Stefano Bucciarelli (Cambridge University)"
      - author: "Roberto Cipolla (University of Cambridge)"
    all_authors: "James Charles, Stefano Bucciarelli and Roberto Cipolla"
    code: ""
    keywords:
      - word: "one shot learning"
      - word: "domain adaptation"
      - word: "domain transfer"
      - word: "domain shift"
      - word: "text recognition"
      - word: "word spotting"
      - word: "meter reading"
      - word: ""
    paper: "papers/0512.pdf"
    supp: "supp/0512_supp.pdf"
    supp2: "supp/0512_supp2.mp4"
    abstract: "Many digital meters such as those used for home health (e.g. blood pressure meters) or meters monitoring industrial equipment do not contain wireless connectivity. Hence, connecting these devices to phone tracking apps or control centres either requires cumbersome manual transcription or is not plausible due to costs. Our motivation is to cheaply retro-fit these types of meters with `smart' data transfer capabilities using a mobile phone app and limited training data. We demonstrate how one can use single training images of meter screens to build efficient custom meter readers targeted to chosen devices. To this end, we build a CNN based system which runs in real-time on mobile device with very high read accuracy (close to 100%). Our contributions include (i) introduction of an exciting new application domain, (ii) a method of training from purely synthetic data by reducing domain shift using a surprisingly simple approach which unlike adversarial training based methods does not even require unlabelled data; (iii) a highly accurate system for parsing digital meter screens and (iv) release of a new screen reading dataset. 
The system, although trained solely on synthetic data, transfers very well to the real-world. Our method of screen detection and text recognition also improves over the state of the art on our dataset."
  - id: 525
    order: 142
    poster_session: 3
    session_id: 8
    title: "Cascaded channel pruning using hierarchical self-distillation"
    authors:
      - author: "Roy Miles (Imperial College London)"
      - author: "Krystian Mikolajczyk (Imperial College London)"
    all_authors: "Roy Miles and Krystian Mikolajczyk"
    code: "https://github.com/roymiles/cascaded-channel-pruning"
    keywords:
      - word: "pruning"
      - word: "knowledge distillation"
      - word: "image classification"
      - word: ""
    paper: "papers/0525.pdf"
    supp: "supp/0525_supp.zip"
    abstract: "In this paper, we propose an approach for filter-level pruning with hierarchical knowledge distillation based on the teacher, teaching-assistant, and student framework.  Our method makes use of teaching assistants at intermediate pruning levels that share the same architecture and weights as the target student.  We propose to prune each model independently using the gradient information from its corresponding teacher. By considering the relative sizes of each student-teacher pair, this formulation provides a natural trade-off between the capacity gap for knowledge distillation and the bias of the filter saliency updates.  Our results show improvements in the attainable accuracy and model compression across the CIFAR10 and ImageNet classification tasks using the VGG16 and ResNet50 architectures.  We provide an extensive evaluation that demonstrates the benefits of using a varying number of teaching assistant models at different sizes."
  - id: 526
    order: 117
    poster_session: 3
    session_id: 8
    title: "RODEO: Replay for Online Object Detection"
    authors:
      - author: "Manoj Acharya (Rochester Institute of Technology)"
      - author: "Tyler Hayes (RIT)"
      - author: "Christopher Kanan (Rochester Institute of Technology)"
    all_authors: "Manoj Acharya, Tyler Hayes and Christopher Kanan"
    code: ""
    keywords:
      - word: "streaming learning"
      - word: "continual learning"
      - word: "object detection"
      - word: "lifelong learning"
      - word: "catastrophic forgetting"
      - word: "product quantization"
      - word: ""
    paper: "papers/0526.pdf"
    supp: "supp/0526_supp.zip"
    abstract: "Humans can incrementally learn to do new visual detection tasks, which is a huge challenge for today's computer vision systems. Incrementally trained deep learning models lack backwards transfer to previously seen classes and suffer from a phenomenon known as ``catastrophic forgetting.'' In this paper, we pioneer online streaming learning for object detection, where an agent must learn examples one at a time with severe memory and computational constraints. In object detection, a system must output all bounding boxes for an image with the correct label. Unlike earlier work, the system described in this paper can learn how to do this task in an online manner with new classes being introduced over time. We achieve this capability by using  a novel memory replay mechanism that replays entire scenes in an efficient manner. We achieve state-of-the-art results on both the PASCAL VOC 2007 and MS COCO datasets."
  - id: 528
    order: 31
    poster_session: 1
    session_id: 2
    title: "Inpainting Networks Learn to Separate Cells in Microscopy Images"
    authors:
      - author: "Steffen Wolf (Heidelberg University)"
      - author: "Fred Hamprecht (Heidelberg Collaboratory for Image Processing)"
      - author: "Jan Funke (HHMI Janelia Research Campus)"
    all_authors: "Steffen Wolf, Fred Hamprecht and Jan Funke"
    code: ""
    keywords:
      - word: "self-supervised learning"
      - word: "instance segmentation"
      - word: "inpainting"
      - word: "microscopy"
      - word: "cell segmentation"
    paper: "papers/0528.pdf"
    supp: "supp/0528_supp.pdf"
    abstract: "Deep neural networks trained to inpaint partially occluded images show a deep understanding of image composition and have even been shown to remove objects from images convincingly. In this work, we investigate how this implicit knowledge of image composition can be be used to separate cells in densely populated microscopy images. We propose a measure for the independence of two image regions given a fully self-supervised inpainting network and separate objects by maximizing this independence. We evaluate our method on two cell segmentation datasets and show that cells can be separated completely unsupervised. Furthermore, combined with simple foreground detection, our method yields instance segmentation of similar quality to fully supervised methods."
  - id: 532
    order: 190
    poster_session: 4
    session_id: 11
    title: "Robust Ensemble Model Training via Random Layer Sampling Against Adversarial Attack"
    authors:
      - author: "Hakmin Lee (KAIST)"
      - author: "Hong Joo Lee (KAIST)"
      - author: "Seong Tae Kim (Technical University of Munich)"
      - author: "Yong Man Ro (KAIST)"
    all_authors: "Hakmin Lee, Hong Joo Lee, Seong Tae Kim and Yong Man Ro"
    code: ""
    keywords:
      - word: "ensemble model"
      - word: "adversarial attack"
      - word: "adversarial defense"
      - word: "adversarial robustnessl"
      - word: "random layer sampling"
      - word: ""
    paper: "papers/0532.pdf"
    supp: ""
    abstract: "Deep neural networks have achieved substantial achievements in several computer vision areas, but have vulnerabilities that are often fooled by adversarial examples that are not recognized by humans. This is an important issue for security or medical applications. In this paper, we propose an ensemble model training framework with random layer sampling to improve the robustness of deep neural networks. In the proposed training framework, we generate various sampled model through the random layer sampling and update the weight of the sampled model. After the ensemble models are trained, it can hide the gradient efficiently and avoid the gradient-based attack by the random layer sampling method. To evaluate our proposed method, comprehensive and comparative experiments have been conducted on three datasets. Experimental results show that the proposed method improves the adversarial robustness. "
  - id: 534
    order: 80
    poster_session: 2
    session_id: 5
    title: "Anchor-free Small-scale Multispectral Pedestrian Detection"
    authors:
      - author: "Alexander Wolpert ( Karlsruhe Institute of Technology)"
      - author: "Michael Teutsch (Hensoldt Optronics)"
      - author: "Saquib Sarfraz (Karlsruhe Institute of Technology)"
      - author: "Rainer Stiefelhagen (Karlsruhe Institute of Technology)"
    all_authors: "Alexander Wolpert, Michael Teutsch, Saquib Sarfraz and Rainer Stiefelhagen"
    code: "https://github.com/HensoldtOptronicsCV/MultispectralPedestrianDetection"
    keywords:
      - word: "pedestrian detection"
      - word: "human recognition"
      - word: "multi-modal"
      - word: "thermal infrared"
      - word: "multispectral fusion"
      - word: "low object resolution"
      - word: "multispectral data augmentation"
      - word: "box-less object detection"
      - word: ""
    paper: "papers/0534.pdf"
    supp: "supp/0534_supp.pdf"
    abstract: "Multispectral images consisting of aligned visual-optical (VIS) and thermal infrared (IR) image pairs are well-suited for practical applications like autonomous driving or visual surveillance. Such data can be used to increase the performance of pedestrian detection especially for weakly illuminated, small-scaled, or partially occluded instances. The current state-of-the-art is based on variants of Faster R-CNN and thus passes through two stages: a proposal generator network with handcrafted anchor boxes for object localization and a classification network for verifying the object category. In this paper we propose a method for effective and efficient multispectral fusion of the two modalities in an adapted single-stage anchor box free base architecture. We aim at learning pedestrian representations based on object center and scale rather than direct bounding box predictions. In this way, we can both simplify the network architecture and achieve higher detection performance, especially for pedestrians under occlusion or at low object resolution. In addition, we provide a study on well-suited multispectral data augmentation techniques that improve the commonly used augmentations. The results show our method's effectiveness in detecting small-scaled pedestrians. We achieve 5.68 % log-average miss rate in comparison to the best current state-of-the-art of 7.49 % (~25 % improvement) on the challenging KAIST Multispectral Pedestrian Benchmark."
  - id: 539
    order: 130
    poster_session: 3
    session_id: 8
    title: "RecSal : Deep Recursive Supervision for Visual Saliency Prediction"
    authors:
      - author: "Oindrila  Saha (IIT Kharagpur)"
      - author: "Sandeep Mishra (IIT Kharagpur)"
    all_authors: "Oindrila  Saha and Sandeep Mishra"
    code: ""
    keywords:
      - word: "Saliency"
      - word: "Recursive Supervision"
      - word: "Deep Supervision"
      - word: "Convolutional Neural Networks"
      - word: "Deep Learning"
    paper: "papers/0539.pdf"
    supp: ""
    abstract: "State-of-the-art saliency prediction methods develop upon model architectures or loss functions; while training to generate one target saliency map. However, publicly available saliency prediction datasets can be utilized to create more information for each stimulus than just a final aggregate saliency map. This information when utilized in a biologically inspired fashion can contribute in better prediction performance without the use of models with huge number of parameters. In this light, we propose to extract and use the statistics of (a) region specific saliency and (b) temporal order of fixations, to provide additional context to our network. We show that extra supervision using spatially or temporally sequenced fixations results in achieving better performance in saliency prediction. Further, we also design novel architectures for utilizing this extra information and show that it achieves superior performance over a base model which is devoid of extra supervision. We show that our best method outperforms previous state-of-the-art methods with 50-80% fewer parameters. We also show that our models perform consistently well across all evaluation metrics unlike prior methods."
  - id: 541
    order: 122
    poster_session: 3
    session_id: 8
    title: "Tackling the Unannotated: Scene Graph Generation with Bias-Reduced Models"
    authors:
      - author: "Tzujui Wang (Aalto University)"
      - author: "Selen Pehlivan (Aalto University)"
      - author: "Jorma Laaksonen (Aalto University)"
    all_authors: "Tzujui Wang, Selen Pehlivan and Jorma Laaksonen"
    code: ""
    keywords:
      - word: "scene graph"
      - word: "online knowledge distillation"
      - word: "visual genome"
      - word: "visual relationship detection"
    paper: "papers/0541.pdf"
    supp: ""
    abstract: "Predicting a scene graph that captures visual entities and their interactions in an image has been considered a crucial step towards full scene comprehension. Recent scene graph generation (SGG) models have shown their capability of capturing the most frequent relations among visual entities. However, the state-of-the-art results are still far from satisfactory, e.g. models can obtain 31% in overall recall R@100, whereas the likewise important mean class-wise recall mR@100 is only around 8% on Visual Genome (VG). The discrepancy between R and mR results urges to shift the focus from pursuing a high R to a high mR with a still competitive R. We suspect that the observed discrepancy stems from both the annotation bias and sparse annotations in VG, in which many visual entity pairs are either not annotated at all or only with a single relation when multiple ones could be valid. To address this particular issue, we propose a novel SGG training scheme that capitalizes on self-learned knowledge. It involves two relation classifiers, one offering a less biased setting for the other to base on. The proposed scheme can be applied to most of the existing SGG models and is straightforward to implement. We observe significant relative improvements in mR (between +6.6% and +20.4%) and competitive or better R (between -2.4% and 0.3%) across all standard SGG tasks."
  - id: 547
    order: 54
    oral_session: 3
    poster_session: 2
    session_id: 3
    title: "Delving Deeper into Anti-aliasing in ConvNets"
    authors:
      - author: "Xueyan Zou (university of california, davis)"
      - author: "Fanyi Xiao (University of California Davis)"
      - author: "Zhiding Yu (NVIDIA)"
      - author: "Yong Jae Lee (University of California, Davis)"
    all_authors: "Xueyan Zou, Fanyi Xiao, Zhiding Yu and Yong Jae Lee"
    code: "https://maureenzou.github.io/ddac/"
    keywords:
      - word: "anti-aliasing"
      - word: "image classification"
      - word: "semantic segmentation"
      - word: "instance segmentation"
      - word: "consistency"
      - word: "shift consistency"
    paper: "papers/0547.pdf"
    supp: ""
    abstract: "Aliasing refers to the phenomenon that high frequency signals degenerate into completely different ones after sampling. It arises as a problem in the context of deep learning as downsampling layers are widely adopted in deep architectures to reduce parameters and computation. The standard solution is to apply a low-pass filter (e.g., Gaussian blur) before downsampling [Zhang.]. However, it can be suboptimal to apply the same filter across the entire content, as the frequency of feature maps can vary across both spatial locations and feature channels. To tackle this, we propose an adaptive content-aware low-pass filtering layer, which predicts separate filter weights for each spatial location and channel group of the input feature maps. We investigate the effectiveness and generalization of the proposed method across multiple tasks including ImageNet classification, COCO instance segmentation, and Cityscapes semantic segmentation. Qualitative and quantitative results demonstrate that our approach effectively adapts to the different feature frequencies to avoid aliasing while preserving useful information for recognition. "
  - id: 549
    order: 110
    poster_session: 3
    session_id: 8
    title: "Tripping through time: Efficient Localization of Activities in Videos"
    authors:
      - author: "Meera Hahn (Georgia Institute of Technology)"
      - author: "Asim Kadav (NEC Labs)"
      - author: "James Rehg (Georgia Institute of Technology)"
      - author: "Hans Peter Graf (NEC Labs)"
    all_authors: "Meera Hahn, Asim Kadav, James Rehg and Hans Peter Graf"
    code: ""
    keywords:
      - word: "Activity Localization"
      - word: "Reinforcement learning"
      - word: "Vision and Language"
    paper: "papers/0549.pdf"
    supp: "supp/0549_supp.pdf"
    abstract: "Localizing moments in untrimmed videos via language queries is a new and interesting task that requires the ability to accurately ground language into video. Previous works have approached this task by processing the entire video, often more than once, to localize relevant activities. In the real world applications that this task lends itself to, such as surveillance, efficiency is a pivotal trait of a system. In this paper, we present TripNet, an end-to-end system that uses a gated attention architecture to model fine-grained textual and visual representations in order to align text and video content. Furthermore, TripNet uses reinforcement learning to efficiently localize relevant activity clips in long videos, by learning how to intelligently skip around the video. It extracts visual features for few frames to perform activity classification. In our evaluation over Charades-STA, ActivityNet Captions and the TACoS dataset, we find that TripNet achieves high accuracy and saves process- ing time by only looking at 32-41% of the entire video. "
  - id: 550
    order: 45
    poster_session: 1
    session_id: 2
    title: "High-order Graph Convolutional Networks for 3D Human Pose Estimation"
    authors:
      - author: "Zhiming Zou (University of Illinois at Chicago)"
      - author: "Kenkun Liu ( University of Illinois at Chicago)"
      - author: "Le Wang (Xi'an Jiaotong University)"
      - author: "Wei Tang (University of Illinois at Chicago)"
    all_authors: "Zhiming Zou, Kenkun Liu, Le Wang and Wei Tang"
    code: ""
    keywords:
      - word: "human pose estimation"
      - word: "graph convolutional networks"
    paper: "papers/0550.pdf"
    supp: ""
    abstract: "Graph convolutional networks (GCNs) have been applied to 3D human pose estimation (HPE) from 2D body joint detections and have demonstrated promising performance. However, since the vanilla graph convolution is performed on the one-hop neighbors of each node, it is unable to capture the long-range dependencies between body joints. They can help reduce the uncertainty caused by occlusion or depth ambiguity. To resolve this issue, we propose a high-order GCN for 3D HPE. Its core building block, termed a high-order graph convolution, aggregates features of nodes at various distances. As a result, the network can model a wide range of interactions among body joints. Furthermore, we investigate different methods to fuse those multi-order features and compare how they affect the performance. Experimental results demonstrate the effectiveness of the proposed approach."
  - id: 552
    order: 144
    poster_session: 3
    session_id: 8
    title: "Black Magic in Deep Learning: How Human Skill Impacts  Network Training"
    authors:
      - author: "kanav anand (TU DELFT)"
      - author: "Ziqi Wang (Delft University of Technology)"
      - author: "Marco Loog (Delft University of Technology & University of Copenhagen)"
      - author: "Jan van Gemert (Delft University of Technology)"
    all_authors: "Kanav Anand, Ziqi Wang, Marco Loog and Jan van Gemert"
    code: ""
    keywords:
      - word: "deep learning"
      - word: "hyperparameter tuning"
      - word: "human factor"
      - word: "reproducibility"
    paper: "papers/0552.pdf"
    supp: "supp/0552_supp.pdf"
    abstract: "How does a user's prior experience with deep learning impact network performance?  We present an initial study based on 31 participants with different levels of experience. Their task is to perform hyperparameter optimization for a given deep learning architecture. The results show a strong positive correlation between the participant's experience and the final performance. They additionally indicate that an experienced participant finds better solutions using fewer resources on average. The data suggests furthermore that participants with no prior experience follow random strategies in their pursuit of optimal hyperparameters. Our study reveals the human factor in scientific reproducibility and in comparisons of state of the art results in deep learning."
  - id: 561
    order: 49
    poster_session: 1
    session_id: 2
    title: "Attribute Adaptive Margin Softmax Loss using  Privileged Information"
    authors:
      - author: "Seyed mehdi Iranmanesh (West virginia university)"
      - author: "Ali Dabouei (West Virginia university)"
      - author: "Nasser Nasrabadi (West Virginia University)"
    all_authors: "Seyed mehdi Iranmanesh, Ali Dabouei and Nasser Nasrabadi"
    code: ""
    keywords:
      - word: "Privileged Information"
      - word: "LUPI"
      - word: "Learning Using Privileged Information"
      - word: "Facial Attributes"
      - word: "Angular Softmax"
      - word: "Softmax"
      - word: "Unbalanced Data"
      - word: "Adaptive Margines"
      - word: "Face Recognition"
      - word: "Person re-identification"
    paper: "papers/0561.pdf"
    supp: ""
    abstract: "We present a novel framework to exploit privileged information for recognition which is provided only during the training phase. Here, we focus on recognition task where images are provided as the main view and soft biometric traits (attributes) are provided as the privileged data (only available during training phase). We demonstrate that more discriminative feature space can be learned by enforcing a deep network to adjust adaptive margins between classes utilizing attributes. This tight constraint also effectively reduces the class imbalance inherent in the local data neighborhood, thus carving more balanced class boundaries locally and using feature space more efficiently. Extensive experiments are performed on five different datasets and the results show the superiority of our method compared to the state-of-the-art models in both tasks of face recognition and person re-identification."
  - id: 566
    order: 6
    oral_session: 2
    poster_session: 1
    session_id: 1
    title: "Advancing weakly supervised cross-domain alignment with optimal transport"
    authors:
      - author: "Siyang Yuan (Duke University)"
      - author: "Ke Bai (Duke University)"
      - author: "Liqun Chen (Duke University)"
      - author: "Yizhe Zhang (Duke University)"
      - author: "Chenyang Tao (Duke University)"
      - author: "Chunyuan Li (Microsoft Research)"
      - author: "Guoyin Wang (Duke University)"
      - author: "Ricardo Henao (Duke University)"
      - author: "Lawrence Carin Duke (CS)"
    all_authors: "Siyang Yuan, Ke Bai, Liqun Chen, Yizhe Zhang, Chenyang Tao, Chunyuan Li, Guoyin Wang, Ricardo Henao and Lawrence Carin Duke"
    code: ""
    keywords:
      - word: "Optimal Transport"
      - word: "Cross Domain Alignment"
    paper: "papers/0566.pdf"
    supp: "supp/0566_supp.pdf"
    abstract: "Cross-domain alignment between image objects and text sequences is key to many visual-language tasks and it poses a fundamental challenge to both computer vision and natural language processing.
This study investigates a novel approach for the identification and optimization of  fine-grained semantic similarities between image and text entities, under a weakly-supervised setup, improving performance over state-of-the-art solutions.
Our method builds upon recent advances in optimal transport (OT) to resolve the cross-domain matching problem in a principled manner. Formulated as a drop-in regularizer, the proposed OT solution can be efficiently computed and used in combination with other existing approaches. 
We present empirical evidence to demonstrate the effectiveness of our approach, that enables simpler model architectures to outperform or be comparable with more sophisticated designs on a range of vision-language tasks."
  - id: 570
    order: 20
    poster_session: 1
    session_id: 2
    title: "First-Person View Hand Segmentation of Multi-Modal Hand Activity Video Dataset"
    authors:
      - author: "Sangpil Kim (Purdue University, USA)"
      - author: "Hyung-gun Chi (Purdue University)"
      - author: "Xiao Hu (Purdue University)"
      - author: "Anirudh Vegesana (Purdue University)"
      - author: "Karthik Ramani (Purdue University, USA)"
    all_authors: "Sangpil Kim, Hyung-gun Chi, Xiao Hu, Anirudh Vegesana and Karthik Ramani"
    code: ""
    keywords:
      - word: "hand segmentation"
      - word: "dataset"
      - word: "deep learning"
      - word: "pixel-wise segmentation"
      - word: "long-wave infraRed"
      - word: "multimodalities"
    paper: "papers/0570.pdf"
    supp: "supp/0570_supp.pdf"
    abstract: "First-person-view videos of hands interacting with tools are widely used in the computer vision industry. However, creating a dataset with pixel-wise segmentation of hands is challenging since most videos are captured with fingertips occluded by the hand dorsum and grasped tools. Current methods often rely on manually segmenting hands to create annotations, which is inefficient and costly. To relieve this challenge, we create a method that utilizes thermal information of hands for efficient pixel-wise hand segmentation to create a multi-modal activity video dataset. Our method is not affected by fingertip and joint occlusions and does not require hand pose ground truth. We show our method to be 24 times faster than the traditional polygon labeling method while maintaining high quality. With the segmentation method, we propose a multi-modal hand activity video dataset with 790 sequences and 401,765 frames of \"hands using tools\" videos captured by thermal and RGB-D cameras with hand segmentation data. We analyze multiple models for hand segmentation performance and benchmark four segmentation networks. We show that our multi-modal dataset with fusing Long-Wave InfraRed~(LWIR) and RGB-D frames achieves 5% better hand IoU performance than using RGB frames."
  - id: 578
    order: 170
    poster_session: 4
    session_id: 11
    title: "Learning Effectively from Noisy Supervision for Weakly Supervised Semantic Segmentation"
    authors:
      - author: "Wenbin Xie (Tsinghua University)"
      - author: "Qiaoqiao Wei (Tsinghua University)"
      - author: "Zheng Li (Tsinghua University)"
      - author: "Hui Zhang (Tsinghua University)"
    all_authors: "Wenbin Xie, Qiaoqiao Wei, Zheng Li and Hui Zhang"
    code: ""
    keywords:
      - word: "Semantic Segmentation"
      - word: "Weakly Supervised Semantic Segmentation"
      - word: "Self Attention"
    paper: "papers/0578.pdf"
    supp: "supp/0578_supp.pdf"
    abstract: "Semantic segmentation based on deep learning has undergone tremendous progress in recent years. However, it continues to depend heavily on massive densely annotated data.  In this paper, we propose a novel framework for weakly supervised semantic segmentation (WSSS) using bounding boxes to alleviate the need for pixel-wise annotations. We argue that the most important problem of WSSS should be learning effectively from noisy supervision. Therefore, we present a constrained foreground segmentation network (CFS) to generate high-quality dense annotations from noisy proposals. The network converts the segmentation task from multi-class classification to two-class classification and removes most of irrelevant regions, making the task easier to optimize. Besides, we introduce a loss-guided self-attention (LGSA) module to encourage self-correcting among intra-class pixels. Instead of allowing global information exchanges in existing non-local networks, our module imposes loss constraints on the information exchanges between different categories and learns a more reasonable affinity matrix which can be used for further random walk. Experiments indicate that our LGSA module has better performance and interpretability even with noisy supervision. We obtain state-of-the-art results on the Pascal VOC 2012 validation set by combining the two novel components."
  - id: 581
    order: 71
    poster_session: 2
    session_id: 5
    title: "Rethinking Curriculum Learning with Incremental Labels and Adaptive Compensation"
    authors:
      - author: "Madan Ravi Ganesh (University of Michigan)"
      - author: "Jason Corso (University of Michigan)"
    all_authors: "Madan Ravi Ganesh and Jason Corso"
    code: ""
    keywords:
      - word: "label smoothing"
      - word: "curriculum learning"
      - word: "incremental labels"
      - word: "adaptive compensation"
      - word: "negative mining"
    paper: "papers/0581.pdf"
    supp: "supp/0581_supp.pdf"
    abstract: "Like humans, deep networks have been shown to learn better when samples are organized and introduced in a meaningful order or curriculum. Conventional curriculum learning schemes introduce samples in their order of difficulty.  This forces models to begin learning from a subset of the available data while adding the external overhead of evaluating the difficulty of samples.  In this work, we propose Learning with Incremental Labels and Adaptive Compensation (LILAC), a two-phase method that incrementally increases the number of unique output labels rather than the difficulty of samples while consistently using the entire dataset throughout training.  In the first phase, Incremental Label Introduction, we partition data into mutually exclusive subsets, one that contains a subset of the ground-truth labels and another that contains the remaining data attached to a pseudo-label.  Throughout the training process, we recursively reveal unseen ground-truth labels in fixed increments until all the labels are known to the model. In the second phase, Adaptive Compensation, we optimize the loss function using altered target vectors for previously misclassified samples. The target vectors of such samples are modified to a smoother distribution to help models learn better. On evaluating across three standard image benchmarks, CIFAR-10, CIFAR-100, and STL-10, we show that LILAC outperforms all comparable baselines. Further, we detail the importance of pacing the introduction of new labels to a model as well as the impact of using a smooth target vector."
  - id: 614
    order: 52
    oral_session: 3
    poster_session: 2
    session_id: 3
    title: "Feature Binding with Category-Dependant MixUp for Semantic Segmentation and Adversarial Robustness"
    authors:
      - author: "Md Amirul Islam (Ryerson University)"
      - author: "Matthew Kowal (Ryerson University)"
      - author: "Konstantinos Derpanis (Ryerson University)"
      - author: "Neil D. B. Bruce (Ryerson University)"
    all_authors: "Md Amirul Islam, Matthew Kowal, Konstantinos Derpanis and Neil D. B. Bruce"
    code: ""
    keywords:
      - word: "Image Blending"
      - word: "Mixup"
      - word: "Feature Binding"
      - word: "Semantic Segmentation"
      - word: "Adversarial Robustness"
    paper: "papers/0614.pdf"
    supp: ""
    abstract: "In this paper, we present a strategy for training convolutional neural networks to effectively resolve interference arising from competing hypotheses relating to inter-categorical information throughout the network. The premise is based on the notion of feature binding, which is defined as the process by which activation's spread across space and layers in the network are successfully integrated to arrive at a correct inference decision. In our work, this is accomplished for the task of dense image labelling by blending images based on their class labels, and then training a feature binding network, which simultaneously segments and separates the blended images. Subsequent feature denoising to suppress noisy activations reveals additional desirable properties and high degrees of successful predictions. Through this process, we reveal a general mechanism, distinct from any prior methods, for boosting the performance of the base segmentation network while simultaneously increasing robustness to adversarial attacks. "
  - id: 618
    order: 156
    poster_session: 4
    session_id: 11
    title: "RNN-based Motion Prediction in Competitive Fencing Considering Interaction between Players"
    authors:
      - author: "Yutaro Honda (The University of Tokyo)"
      - author: "Rei Kawakami (Tokyo Institute of Technology)"
      - author: "Takeshi Naemura (The University of Tokyo)"
    all_authors: "Yutaro Honda, Rei Kawakami and Takeshi Naemura"
    code: ""
    keywords:
      - word: "pose prediction"
      - word: "sports analysis"
      - word: "interaction"
      - word: "fencing"
      - word: ""
    paper: "papers/0618.pdf"
    supp: ""
    abstract: "The ability to accurately predict the motion of fencing athletes will help to improve the competition techniques of the players and the viewing experience of the audience. Most human-motion prediction methods only consider a single person, but in fencing, the movement of the opponent greatly affects the future movements of the player. In this paper, we propose a motion prediction model that takes into account the interaction between the two players in the game by connecting the recurrent neural networks to each other. In experiments, our model improved the accuracy of predicting movements in response to the opposing player, such as retreating to avoid the opponent's thrusts."
  - id: 621
    order: 154
    oral_session: 8
    poster_session: 4
    session_id: 10
    title: "Learning from Counting: Leveraging Temporal Classification for Weakly Supervised Object Localization and Detection"
    authors:
      - author: "Chia-Yu Hsu (Arizona State University)"
      - author: "Wenwen Li (Arizona State University)"
    all_authors: "Chia-Yu Hsu and Wenwen Li"
    code: ""
    keywords:
      - word: "weakly supervised object detection"
      - word: "object detection"
      - word: "region proposal"
      - word: "weak supervision"
      - word: "MIL"
      - word: "WSOD"
      - word: ""
    paper: "papers/0621.pdf"
    supp: ""
    abstract: "This paper reports a new solution of leveraging temporal classification to support weakly supervised object detection (WSOD). Specifically, we introduce raster scan-order techniques to serialize 2D images into 1D sequence data, and then leverage a combined LSTM (Long, Short-Term Memory) and CTC (Connectionist Temporal Classification) network to achieve object localization based on a total count (of interested objects). We term our proposed network LSTM-CCTC (Count-based CTC). This “learning from counting” strategy differs from existing WSOD methods in that our approach automatically identifies critical points on or near a target object. This strategy significantly reduces the need of generating a large number of candidate proposals for object localization. Experiments show that our method yields state-of-the-art performance based on an evaluation on PASCAL VOC datasets. 
"
  - id: 631
    order: 150
    oral_session: 7
    poster_session: 4
    session_id: 9
    title: "Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs"
    authors:
      - author: "Ruigang Fu (National University of Defense Technology)"
      - author: "Qingyong Hu (University of Oxford)"
      - author: "Xiaohu Dong (National University of Defense Technology)"
      - author: "Yulan Guo (National University of Defense Technology)"
      - author: "Yinghui Gao (National University of Defense Technology)"
      - author: "Biao Li (National University of Defense Technology)"
    all_authors: "Ruigang Fu, Qingyong Hu, Xiaohu Dong, Yulan Guo, Yinghui Gao and Biao Li"
    code: "https://github.com/Fu0511/XGrad-CAM"
    keywords:
      - word: "CNN visualization"
      - word: "CNN explanation"
      - word: "image classification"
      - word: ""
    paper: "papers/0631.pdf"
    supp: "supp/0631_supp.zip"
    abstract: "To have a better understanding and usage of Convolution Neural Networks (CNNs), the visualization and interpretation of CNNs has attracted increasing attention in recent years. In particular, several Class Activation Mapping (CAM) methods have been proposed to discover the connection between CNN's decision and image regions. However, in spite of the reasonable visualization, most of these methods lack clear and sufficient theoretical support. In this paper, we introduce two axioms -- Sensitivity and Conservation -- to the visualization paradigm of the CAM methods. Meanwhile, a dedicated Axiom-based Grad-CAM (XGrad-CAM) is proposed to satisfy these axioms as much as possible. Experiments demonstrate that XGrad-CAM is an enhanced version of Grad-CAM in terms of sensitivity and conservation. It is able to achieve better visualization performance than Grad-CAM, while also be class-discriminative and easy-to-implement compared with Grad-CAM++ and Ablation-CAM. Code is available at https://github.com/Fu0511/XGrad-CAM."
  - id: 634
    order: 78
    poster_session: 2
    session_id: 5
    title: "Neural Network Quantization with Scale-Adjusted Training"
    authors:
      - author: "Qing Jin (Bytedance, Inc. & Texas A&M University)"
      - author: "Linjie Yang (ByteDance AI Lab)"
      - author: "Zhenyu Liao (Kwai Inc)"
      - author: "Xiaoning Qian (Texas A&M University)"
    all_authors: "Qing Jin, Linjie Yang, Zhenyu Liao and Xiaoning Qian"
    code: ""
    keywords:
      - word: "neural network quantization"
      - word: "over-fitting"
      - word: "regularization"
    paper: "papers/0634.pdf"
    supp: "supp/0634_supp.pdf"
    abstract: "Quantization has long been studied as a compression and accelerating technique for deep neural networks due to its potential on reducing model size and computational costs, for both general hardware, such as DSP, CPU or GPU, and customized devices with flexible bit-width configurations, including FPGA and ASIC. However, previous works generally achieve network quantization by sacrificing on prediction accuracy with respect to their full-precision counterparts. In this paper, we investigate the underlying mechanism of such performance degeneration based on previous work of parameterized clipping activation (PACT). We find that the key factor is the weight scale in the last layer. Instead of aligning weight distributions of quantized and full-precision models, as generally suggested in the literature, the main issue is that large scale can cause over-fitting problem. We propose a technique called scale-adjusted training (SAT) by directly scaling down weights in the last layer to alleviate such over-fitting. With the proposed technique, quantized networks can demonstrate better performance than their full-precision counter-parts, and we achieve state-of-the-art accuracy with consistent improvement over previous quantization methods for light weight models including MobileNet V1/V2 on ImageNet classification."
  - id: 638
    order: 155
    oral_session: 8
    poster_session: 4
    session_id: 10
    title: "Video Region Annotation with Sparse Bounding Boxes"
    authors:
      - author: "Yuzheng Xu (Kyoto University)"
      - author: "Yang Wu (Kyoto University)"
      - author: "Nur Sabrina binti Zuraimi (Kyoto University)"
      - author: "Shohei Nobuhara (Kyoto University)"
      - author: "Ko Nishino (Kyoto University)"
    all_authors: "Yuzheng Xu, Yang Wu, Nur Sabrina binti Zuraimi, Shohei Nobuhara and Ko Nishino"
    code: ""
    keywords:
      - word: "video annotation"
      - word: "semi-automatic annotation"
      - word: "graph convolutional network"
      - word: "region boundaries"
      - word: "sparse bounding boxes"
      - word: "automatic boundary finding"
      - word: ""
    paper: "papers/0638.pdf"
    supp: "supp/0638_supp.zip"
    abstract: "Video analysis has been moving towards more detailed interpretation (e.g. segmentation) with encouraging progresses. These tasks, however, increasingly rely on densely annotated training data both in space and time. Since such annotation is labour-intensive, few densely annotated video data with detailed region boundaries exist. This work aims to resolve this dilemma by learning to automatically generate region boundaries for all frames of a video from sparsely annotated bounding boxes of target regions. We achieve this with a Volumetric Graph Convolutional Network (VGCN), which learns to iteratively find keypoints on the region boundaries using the spatio-temporal volume of surrounding appearance and motion. The global optimization of VGCN makes it significantly stronger and generalize better than existing solutions. Experimental results using two latest datasets (one real and one synthetic), including ablation studies, demonstrate the effectiveness and superiority of our method."
  - id: 643
    order: 194
    poster_session: 4
    session_id: 11
    title: "Unsupervised and Semi-supervised Novelty Detection using Variational Autoencoders in Opportunistic Science Missions"
    authors:
      - author: "Lorenzo Sintini (University of Oxford)"
      - author: "Lars Kunze (University of Oxford)"
    all_authors: "Lorenzo Sintini and Lars Kunze"
    code: ""
    keywords:
      - word: "Variational Autoencoder"
      - word: "Novelty detection"
      - word: "Opportunistic Science"
      - word: "Space Exploration"
      - word: "Semi-supervised learning"
      - word: "Unsupervised learning"
    paper: "papers/0643.pdf"
    supp: ""
    abstract: "Scientific opportunities are missed in planetary explorations due to the lack of communication and/or long-time communication delays between rovers and ground stations. By enabling rovers to autonomously detect and explore targets the overall scientific outcome of extraterrestrial missions can be increased.

In this paper, we have designed, developed, and evaluated unsupervised as well as semi-supervised approaches to novelty detection based on Variational Autoencoders (VAE). Our VAE model was trained on typical data from previous missions and tested to infer the novelty of scientific targets.   
In an ablation study, we investigate the effectiveness of different types of loss functions. We compare losses based on reconstruction errors, losses obtained from the VAE's latent space as well as a combination of both. 
In our experiments, we have evaluated both unsupervised and semi-supervised approaches on datasets obtained from NASA's Mars Curiosity rover. Results show that our VAE-based approaches are not only robust but also comparable, or better, than the state-of-the-art."
  - id: 656
    order: 96
    poster_session: 2
    session_id: 5
    title: "Contrastively-reinforced Attention Convolutional Neural Network for Fine-grained Image Recognition"
    authors:
      - author: "Dichao Liu (Nagoya University)"
      - author: "Yu Wang (Ritsumeikan University)"
      - author: "Jien Kato (Ritsumeikan University)"
      - author: "Kenji Mase (Nagoya University)"
    all_authors: "Dichao Liu, Yu Wang, Jien Kato and Kenji Mase"
    code: ""
    keywords:
      - word: "attention learning"
      - word: "fine-grained recognition"
      - word: "deep learning"
      - word: ""
    paper: "papers/0656.pdf"
    supp: "supp/0656_supp.zip"
    abstract: "Fine-grained visual classification is inherently challenging because of its inter-class similarity and intra-class variance. However, by contrasting the images with same/different labels, a human can instinctively notice that the key clues lie in certain objects while other objects are ignorable. Inspired by this, we propose Contrastively-reinforced Attention Convolutional Neural Network (CRA-CNN), which reinforces the attention awareness of deep activations. CRA-CNN mainly contains two parts: the classification stream and attention regularization stream. The former classifies the input image and simultaneously divides the visual information of the input into attention and redundancy.  The latter evaluates the attention/redundancy proposal by classifying the attention and contrasting the attention/redundancy of various inputs. The evaluation information is backpropagated and forces the classification stream to improve its awareness of visual attention, which helps classification. Experimental results on CUB-Birds and Stanford Cars show that CRA-CNN distinctly outperforms the baselines and is comparable with state-of-art studies despite its simplicity."
  - id: 662
    order: 24
    poster_session: 1
    session_id: 2
    title: "Lifted Regression/Reconstruction Networks"
    authors:
      - author: "Rasmus Høier (Chalmers University of Technology)"
      - author: "Christopher Zach (Chalmers University)"
    all_authors: "Rasmus Høier and Christopher Zach"
    code: ""
    keywords:
      - word: "Lifted neural networks"
      - word: "Lipschitz continuity"
      - word: "adversarial robustness"
      - word: "energy-based models"
    paper: "papers/0662.pdf"
    supp: "supp/0662_supp.pdf"
    abstract: "In this work we propose lifted regression/reconstruction networks (LRRNs), which combine lifted neural networks with a guaranteed Lipschitz continuity property for the output layer. Lifted neural networks explicitly optimize an energy model to infer the unit activations and therefore---in contrast to standard feed-forward neural networks---allow bidirectional feedback between layers. So far lifted neural networks have been modelled around standard feed-forward architectures. We propose to take further advantage of the feedback property by letting the layers simultaneously perform regression and reconstruction. The resulting lifted network architecture allows to control the desired amount of Lipschitz continuity, which is an important feature to obtain adversarially robust regression and classification methods. We analyse and numerically demonstrate applications for unsupervised and supervised learning."
  - id: 673
    order: 67
    poster_session: 2
    session_id: 5
    title: "Zero-Shot Domain Generalization"
    authors:
      - author: "Udit Maniyar (Indian Institute of Technology Hyderabad)"
      - author: "Joseph K J (Indian Institute of Technology, Hyderabad)"
      - author: "Aniket Anand Deshmukh (Microsoft)"
      - author: "Urun Dogan (Microsoft)"
      - author: "Vineeth N Balasubramanian (Indian Institute of Technology, Hyderabad)"
    all_authors: "Udit Maniyar, Joseph K J, Aniket Anand Deshmukh, Urun Dogan and Vineeth N Balasubramanian"
    code: "https://github.com/aniketde/ZeroShotDG"
    keywords:
      - word: "Domain Generalization"
      - word: "zero-shot learning"
      - word: "semantic space"
      - word: "multi task learning"
      - word: "Learning with limited data"
      - word: "representation learning"
      - word: "classification"
    paper: "papers/0673.pdf"
    supp: "supp/0673_supp.zip"
    abstract: "Standard supervised learning setting assumes that training data and test data come from the same distribution (domain). Domain generalization (DG) methods try to learn a model that when trained on data from multiple domains, would generalize to a new unseen domain. We extend DG to an even more challenging setting, where the label space of the unseen domain could also change. We introduce this problem as Zero-Shot Domain Generalization (to the best of our knowledge, the first such effort), where the model generalizes across new domains and also across new classes in those domains.
We propose a simple strategy which effectively exploits semantic information of classes, to adapt existing DG methods to meet the demands of Zero-Shot Domain Generalization. We evaluate the proposed methods on CIFAR-10, CIFAR-100, F-MNIST and PACS datasets; establishing a strong baseline to foster interest in this new research direction."
  - id: 678
    order: 183
    poster_session: 4
    session_id: 11
    title: "Real-Time Semantic Segmentation via Multiply Spatial Fusion Network"
    authors:
      - author: "Haiyang Si (Beihang University)"
      - author: "Zhiqiang Zhang (HuaZhong univerisity of Science and Technology)"
      - author: "Feng Lu (Beihang University)"
    all_authors: "Haiyang Si, Zhiqiang Zhang and Feng Lu"
    code: ""
    keywords:
      - word: "real-time"
      - word: "semantic segmentation"
      - word: "boundary supervision"
    paper: "papers/0678.pdf"
    supp: ""
    abstract: "Real-time semantic segmentation plays a significant role in industry applications, such as autonomous driving, robotics and so on. It is a challenging task as both efficiency and performance need to be considered simultaneously. To address such a complex task, this paper proposes an efficient CNN called Multiply Spatial Fusion Network (MSFNet) to achieve fast and accurate perception. The proposed MSFNet uses Class Boundary Supervision to process the relevant boundary information based on our proposed Multi-features Fusion Module which can obtain spatial information and enlarge receptive field. Therefore, the final upsampling of the feature maps of 1/8 original image size can achieve impressive results while maintaining a high speed. Experiments on Cityscapes and Camvid datasets show an obvious advantage of the proposed approach compared with the existing approaches. Specifically, it achieves 77.1% Mean IOU on the Cityscapes test dataset with the speed of 41 FPS for a 1024*2048 input, and 75.4% Mean IOU with the speed of 91 FPS on the Camvid test dataset."
  - id: 680
    order: 30
    poster_session: 1
    session_id: 2
    title: "Semi-supervised semantic segmentation needs strong, varied perturbations"
    authors:
      - author: "Geoffrey French (University of East Anglia)"
      - author: "Samuli Laine (NVIDIA)"
      - author: "Timo Aila (NVIDIA)"
      - author: "Michal Mackiewicz (University of East Anglia)"
      - author: "Graham Finlayson (University of East Anglia)"
    all_authors: "Geoffrey French, Samuli Laine, Timo Aila, Michal Mackiewicz and Graham Finlayson"
    code: "https://github.com/Britefury/cutmix-semisup-seg"
    keywords:
      - word: "semantic segmentation"
      - word: "semi-supervised"
      - word: "deep learning"
      - word: "consistency regularization"
      - word: "data augmentation"
      - word: "deep learning"
      - word: "cluster assumption"
    paper: "papers/0680.pdf"
    supp: "supp/0680_supp.pdf"
    abstract: "Consistency regularization describes a class of approaches that have yielded ground breaking results in semi-supervised classification problems. Prior work has established the cluster assumption - under which the data distribution consists of uniform class clusters of samples separated by low density regions - as important to its success. We analyze the problem of semantic segmentation and find that its' distribution does not exhibit low density regions separating classes and offer this as an explanation for why semi-supervised segmentation is a challenging problem, with only a few reports of success.
We then identify choice of augmentation as key to obtaining reliable performance without such low-density regions.
We find that adapted variants of the recently proposed CutOut and CutMix augmentation techniques
yield state-of-the-art semi-supervised semantic segmentation results in standard datasets.
Furthermore, given its challenging nature we propose that semantic segmentation acts as an effective acid test for evaluating semi-supervised regularizers.
Implementation at: https://github.com/Britefury/cutmix-semisup-seg."
  - id: 688
    order: 106
    oral_session: 6
    poster_session: 3
    session_id: 7
    title: "Marginalized Graph Attention Hashing for Zero-Shot Image Retrieval"
    authors:
      - author: "Meixue Huang (Institute of Information Engineering, Chinese Academy of Sciences)"
      - author: "Dayan Wu (Institute of Information Engineering, Chinese Academy of Sciences)"
      - author: "Wanqian Zhang (Institute of Information Engineering, Chinese Academy of Sciences)"
      - author: "Zhi Xiong (Institute of Information Engineering, Chinese Academy of Sciences)"
      - author: "Bo Li (	Institute of Information Engineering, Chinese Academy of Sciences)"
      - author: "Weiping Wang (Institute of Information Engineering, CAS, China)"
    all_authors: "Meixue Huang, Dayan Wu, Wanqian Zhang, Zhi Xiong, Bo Li and Weiping Wang"
    code: ""
    keywords:
      - word: "zero-shot hashing"
      - word: "image retrieval"
      - word: "attention mechanism"
      - word: "marginalized strategy"
    paper: "papers/0688.pdf"
    supp: ""
    abstract: "Zero-shot image retrieval allows to precisely retrieve candidates relevant to unobserved queries, of which categories have never been seen during training. Recently, research interests arise in exploring hashing methods to solve this problem due to its storage and computational efficiency. However, existing methods only focus on leveraging semantic information, but omit to exploit the similarity structure of visual feature space for knowledge transfer. Besides, the domain shift problem across seen and unseen classes further degrades the performance. To tackle these issues, in this paper, we propose a novel deep zero-shot hashing method, named Marginalized Graph Attention Hashing (MGAH). MGAH introduces the masked attention mechanism to construct a joint-semantics similarity graph, which captures the intrinsic relationship from different metric spaces, making it competent to transfer knowledge from seen classes into unseen classes. Furthermore, we elaborately design an Energy Magnified Softmax (EM-Softmax) loss, which is capable to alleviate the domain shift problem and encourage the generalization ability of hash codes. By using marginalized strategy, EM-Softmax produces the shared decision margin for hard samples, thus can avoid overfitting on seen classes and meanwhile cover more knowledge for the unseen ones. Extensive experiments demonstrate that MGAH delivers superior performance over the state-of-the-art zero-shot hashing methods.
"
  - id: 689
    order: 148
    oral_session: 7
    poster_session: 4
    session_id: 9
    title: "Bipartite Graph Reasoning GANs for Person Image Generation"
    authors:
      - author: "Hao Tang (University of Trento)"
      - author: "Song Bai (University of Oxford)"
      - author: "Philip Torr (University of Oxford)"
      - author: "Nicu Sebe (University of Trento)"
    all_authors: "Hao Tang, Song Bai, Philip Torr and Nicu Sebe"
    code: "https://github.com/Ha0Tang/BiGraphGAN"
    keywords:
      - word: "person image generation"
      - word: "GANs"
      - word: "bipartite graph"
      - word: "graph reasoning"
      - word: ""
    paper: "papers/0689.pdf"
    supp: ""
    abstract: "We present a novel Bipartite Graph Reasoning GAN (BiGraphGAN) for the challenging person image generation task. The proposed graph generator mainly consists of two novel blocks that aim to model the pose-to-pose and pose-to-image relations, respectively. Specifically, the proposed Bipartite Graph Reasoning (BGR) block aims to reason the crossing long-range relations between the source pose and the target pose in a bipartite graph, which mitigates some challenges caused by pose deformation. Moreover, we propose a new Interaction-and-Aggregation (IA) block to effectively update and enhance the feature representation capability of both person's shape and appearance in an interactive way. Experiments on two challenging and public datasets,~emph{i.e.},~Market-1501 and DeepFashion, show the effectiveness of the proposed BiGraphGAN in terms of objective quantitative scores and subjective visual realness.
The source code and trained models are available at https://github.com/Ha0Tang/BiGraphGAN."
  - id: 694
    order: 51
    oral_session: 3
    poster_session: 2
    session_id: 3
    title: "Fast Convex Relaxations using Graph Discretizations"
    authors:
      - author: "Jonas Geiping (University of Siegen)"
      - author: "Fjedor Gaede (Westfälische Wilhelms Universität Münster)"
      - author: "Hartmut Bauermeister (University of Siegen)"
      - author: "Michael Moeller (University of Siegen)"
    all_authors: "Jonas Geiping, Fjedor Gaede, Hartmut Bauermeister and Michael Moeller"
    code: ""
    keywords:
      - word: "Minimal Partitions"
      - word: "Convex Relaxation"
      - word: "Mumford-Shah"
      - word: "Matching"
      - word: "Segmentation"
      - word: "Stereo Estimation"
      - word: "Superpixels"
      - word: ""
    paper: "papers/0694.pdf"
    supp: ""
    abstract: "   Matching and partitioning problems are fundamentals of computer vision applications with examples in multilabel segmentation, stereo estimation and optical-flow computation. These tasks can be posed as non-convex energy minimization problems and solved near-globally optimal by recent convex lifting approaches. Yet, applying these techniques comes with a significant computational effort, reducing their feasibility in practical applications. We discuss spatial discretization of continuous partitioning problems into a graph structure, generalizing discretization onto a Cartesian grid. This setup allows us to faithfully work on super-pixel graphs constructed by SLIC or Cut-Pursuit,
    massively decreasing the computational effort for lifted partitioning problems compared to a Cartesian grid, while optimal energy values remain similar: The global matching is still solved near-globally optimal.
   We discuss this methodology in detail and show examples in multi-label segmentation by minimal partitions and stereo estimation, where we demonstrate that the proposed graph discretization can reduce runtime as well as memory consumption of convex relaxations of matching problems by up to a factor of 10. "
  - id: 702
    order: 91
    poster_session: 2
    session_id: 5
    title: "Two-in-One Refinement for Interactive Segmentation"
    authors:
      - author: "Soumajit Majumder (University of Bonn)"
      - author: "Abhinav Rai (National University of Singapore)"
      - author: "Ansh Khurana (Indian Institute of Technology Bombay)"
      - author: "Angela Yao (National University of Singapore)"
    all_authors: "Soumajit Majumder, Abhinav Rai, Ansh Khurana and Angela Yao"
    code: ""
    keywords:
      - word: "instance segmentation"
      - word: "interactive segmentation"
    paper: "papers/0702.pdf"
    supp: ""
    abstract: "Deep convolutional neural networks are now mainstream for click-based interactive image segmentation. In majority of the frameworks, false negatives and false positive regions are refined via a succession of positive and negative clicks placed centrally in these regions.  We propose a simple yet intuitive two-in-one refinement strategy by using clicks placed on the boundary of the object of interest.  As boundary clicks are a very strong cue for extracting the object of interest and we find that they are much more effective in correcting wrong segmentation masks.  In addition, we propose a boundary-aware loss which encourages segmentation masks to respect instance boundaries.  We place our new refinement scheme and loss formulation within a task-specialized segmentation framework and achieve state-of-the-art performance on the standard datasets - Berkeley, Pascal VOC 2012, DAVIS and MS COCO. We exceed competing methods by 6.5 %, 9.4 %, 10.5 % and 2.5 % respectively."
  - id: 711
    order: 151
    oral_session: 7
    poster_session: 4
    session_id: 9
    title: "SketchHealer: A Graph-to-Sequence Network for Recreating Partial Human Sketches"
    authors:
      - author: "Guoyao Su (Beijing University of Posts and Telecommunications)"
      - author: "Yonggang Qi (Beijing University of Posts and Telecommunications)"
      - author: "Kaiyue Pang (Queen Mary University of London)"
      - author: "Jie Yang (BUPT)"
      - author: "Yi-Zhe Song (University of Surrey)"
    all_authors: "Guoyao Su, Yonggang Qi, Kaiyue Pang, Jie Yang and Yi-Zhe Song"
    code: "https://github.com/sgybupt/SketchHealer"
    keywords:
      - word: "sketch healing"
      - word: "sketch synthesis"
      - word: "graph-to-sequence network"
      - word: "GCN"
      - word: ""
    paper: "papers/0711.pdf"
    supp: ""
    abstract: "To perceive and create a whole from parts is a prime trait of the human visual system. In this paper, we teach machines to perform a similar task by recreating a vectorised human sketch from its incomplete parts. This is fundamentally different to prior work on image completion (i) sketches exhibit a severe lack of visual cue and are of a sequential nature, and more importantly (ii) we ask for an agent that does not just fill in a missing part, but to recreate a novel sketch that closely resembles the partial input from scratch. Central to our contribution is a graph model that encodes both the visual and structural features over multiple categories. A novel sketch graph construction module is proposed that leverages the sequential nature of sketches to associate key parts centred around stroke junctions. The intuition is then that message passing within the said graph will naturally provide the healing power when it comes to missing parts (nodes). Finally, an off-the-shelf LSTM-based decoder is employed to decode sketches in a vectorised fashion. Both qualitative and quantitative results show that the proposed model significantly outperforms state-of-the-art alternatives."
  - id: 725
    order: 176
    poster_session: 4
    session_id: 11
    title: "LSD_2 - Joint Denoising and Deblurring of Short and Long Exposure Images with CNNs"
    authors:
      - author: "Janne Mustaniemi (University of Oulu)"
      - author: "Juho	 Kannala (Aalto University, Finland)"
      - author: "Jiri Matas (CMP CTU FEE)"
      - author: "Simo Särkkä (Aalto University)"
      - author: "Janne Heikkila (University of Oulu, Finland)"
    all_authors: "Janne Mustaniemi, Juho	 Kannala, Jiri Matas, Simo Särkkä and Janne Heikkila"
    code: "https://github.com/jannemus/LSD2"
    keywords:
      - word: "computational photography"
      - word: "low light imaging"
      - word: "image noise"
      - word: "motion blur"
      - word: "image denoising"
      - word: "image deblurring"
      - word: "multi-image restoration"
    paper: "papers/0725.pdf"
    supp: "supp/0725_supp.pdf"
    abstract: "The paper addresses the problem of acquiring high-quality photographs with handheld smartphone cameras in low-light imaging conditions. We propose an approach based on capturing pairs of short and long exposure images in rapid succession and fusing them into a single high-quality photograph. Unlike existing methods, we take advantage of both images simultaneously and perform a joint denoising and deblurring using a convolutional neural network. A novel approach is introduced to generate realistic short-long exposure image pairs. The method produces good images in extremely challenging conditions and outperforms existing denoising and deblurring methods. It also enables exposure fusion in the presence of motion blur."
  - id: 737
    order: 177
    poster_session: 4
    session_id: 11
    title: "Reducing Label Noise in Anchor-Free Object Detection"
    authors:
      - author: "Nermin Samet (Middle East Technical University)"
      - author: "Samet Hicsonmez (Hacettepe University)"
      - author: "Emre Akbas (Middle East Technical University)"
    all_authors: "Nermin Samet, Samet Hicsonmez and Emre Akbas"
    code: "https://github.com/nerminsamet/ppdet"
    keywords:
      - word: "Object Detection"
      - word: "Anchor-Free"
      - word: "Prediction Pooling"
      - word: "Sum Pooling"
    paper: "papers/0737.pdf"
    supp: ""
    abstract: "Current anchor-free object detectors label all the features that spatially fall inside a predefined central region of a ground-truth box as positive. This approach causes label noise during training, since some of these positively labeled features may be on the background or an occluder object, or they are simply not discriminative features. In this paper, we propose a new labeling strategy aimed to reduce the label noise in anchor-free detectors. We sum-pool predictions stemming from individual features into a single prediction. This allows the model to reduce the contributions of non-discriminatory features during training. We develop a new one-stage, anchor-free object detector, PPDet, to employ this labeling strategy during training and a similar prediction pooling method during inference. On the COCO dataset, PPDet achieves the best performance among anchor-free top-down detectors and performs on-par with the other state-of-the-art methods. It also outperforms all major one-stage and two-stage methods in small object detection (APs 31.4). Code is available at https://github.com/nerminsamet/ppdet."
  - id: 742
    order: 173
    poster_session: 4
    session_id: 11
    title: "Spatial Feedback Learning to Improve Semantic Segmentation in Hot Weather"
    authors:
      - author: "Shyam Nandan Rai (IIIT-Hyderabad)"
      - author: "Vineeth N Balasubramanian (Indian Institute of Technology, Hyderabad)"
      - author: "Anbumani Subramanian (Intel)"
      - author: "C.V. Jawahar (IIIT-Hyderabad)"
    all_authors: "Shyam Nandan Rai, Vineeth N Balasubramanian, Anbumani Subramanian and C.V. Jawahar"
    code: "https://github.com/shyam671/Spatial-Feedback-Learning-to-ImproveSemantic-Segmentation-in-Hot-Weather"
    keywords:
      - word: "semantic segmentation"
      - word: "image restoration"
      - word: "adverse weather"
      - word: "feedback mechanism"
      - word: "iterative focal loss"
    paper: "papers/0742.pdf"
    supp: "supp/0742_supp.pdf"
    abstract: "High-temperature weather conditions induce geometrical distortions in images which can adversely affect the performance of a computer vision model performing downstream tasks such as semantic segmentation. The performance of such models has been shown to improve by adding a restoration network before a semantic segmentation network. The restoration network removes the geometrical distortions from the images and shows improved segmentation results. However, this approach suffers from a major architectural drawback that is the restoration network does not learn directly from the errors of the segmentation network. In other words, the restoration network is not task aware. In this work, we propose a semantic feedback learning approach, which improves the task of semantic segmentation giving a feedback response into the restoration network. This response works as an attend and fix mechanism by focusing on those areas of an image where restoration needs improvement. Also, we proposed loss functions: Iterative Focal Loss (iFL) and Class-Balanced Iterative Focal Loss (CB-iFL), which are specifically designed to improve the performance of the feedback network. These losses focus more on those samples that are continuously miss-classified over successive iterations. Our approach gives a gain of 17.41 mIoU over the standard segmentation model, including the additional gain of 1.9 mIoU with CB-iFL on the Cityscapes dataset."
  - id: 743
    order: 41
    poster_session: 1
    session_id: 2
    title: "Initial Classifier Weights Replay for Memoryless Class Incremental Learning"
    authors:
      - author: "Eden Belouadah (CEA LIST)"
      - author: "Adrian Popescu (CEA LIST)"
      - author: "Ioannis Kanellos (IMT Atlantique)"
    all_authors: "Eden Belouadah, Adrian Popescu and Ioannis Kanellos"
    code: "https://github.com/EdenBelouadah/class-incremental-learning/blob/master/siw/"
    keywords:
      - word: "Memoryless Incremental Learning"
      - word: "Catastrophic Forgetting"
      - word: "Deep Learning"
      - word: "Image classification"
    paper: "papers/0743.pdf"
    supp: "supp/0743_supp.pdf"
    abstract: "Incremental Learning (IL) is useful when artificial systems need to deal with streams of data and do not have access to all data at all times.
The most challenging setting requires a constant complexity of the deep model and an incremental model update without access to a bounded memory of past data.
Then, the representations of past classes are strongly affected by catastrophic forgetting.
To mitigate its negative effect, an adapted fine tuning which includes knowledge distillation is usually deployed.
We propose a different approach based on a vanilla fine tuning backbone.
It leverages initial classifier weights which provide a strong representation of past classes because they are trained with all class data.
However, the magnitude of classifiers learned in different states varies and normalization is needed for a fair handling of all classes.
Normalization is performed by standardizing the initial classifier weights, which are assumed to be normally distributed.
In addition, a calibration of prediction scores is done by using state level statistics to further improve classification fairness.
We conduct a thorough evaluation with four public datasets in a memoryless incremental learning setting. 
Results show that our method outperforms existing techniques by a large margin for large-scale datasets."
  - id: 744
    order: 179
    poster_session: 4
    session_id: 11
    title: "HASTE: multi-Hypothesis Asynchronous Speeded-up Tracking of Events"
    authors:
      - author: "Ignacio Alzugaray (ETH Zurich)"
      - author: "Margarita Chli (ETH Zurich)"
    all_authors: "Ignacio Alzugaray and Margarita Chli"
    code: ""
    keywords:
      - word: "event camera"
      - word: "event-driven"
      - word: "feature tracking"
      - word: "multi-hypothesis tracking"
      - word: "dynamic vision sensor"
      - word: "asynchronous optimization"
      - word: ""
    paper: "papers/0744.pdf"
    supp: "supp/0744_supp.zip"
    abstract: "Feature tracking using event cameras has experienced significant progress lately, with methods achieving comparable performance to feature trackers using traditional frame-based cameras, even outperforming them on certain challenging scenarios. Most of the event-based trackers, however, still operate on intermediate, frame-like representations generated from accumulated events, on which traditional frame-based techniques can be adopted. Attempting to harness the sparsity and asynchronicity of the event stream, other approaches have emerged to process each event individually, but they lack both in accuracy and efficiency in comparison to the event-based, frame-like alternatives.

Aiming to address this shortcoming of asynchronous approaches, in this paper, we propose an asynchronous patch-feature tracker that relies solely on events and processes each event individually as soon as it gets generated. We report significant improvements in tracking quality over the state of the art in publicly available datasets, while performing an order of magnitude more efficiently than similar asynchronous tracking approaches."
  - id: 754
    order: 17
    poster_session: 1
    session_id: 2
    title: "Accurate Parts Visualization for Explaining CNN Reasoning via Semantic Segmentation"
    authors:
      - author: "Ren Harada (Graduate School of Information Scienceand Technology, The University of Tokyo)"
      - author: "Antonio Tejero-de-Pablos (The University of Tokyo)"
      - author: "Tatsuya Harada (The University of Tokyo / RIKEN)"
    all_authors: "Ren Harada, Antonio Tejero-de-Pablos and Tatsuya Harada"
    code: ""
    keywords:
      - word: "explanation"
      - word: "semantic segmentation"
      - word: "deep learning"
      - word: ""
    paper: "papers/0754.pdf"
    supp: ""
    abstract: "Nowadays, neural networks are often used for image classification, but the structure of their decisions is difficult to understand because of their \"black-box\" nature. Different visualization techniques have been proposed to provide additional information on the reason of the classification results. Existing methods provide quantitative explanations by calculating heatmaps and interpretable components in the image. While the latter provides semantics on the image parts that contribute for the classification, the component areas are blurry due to the use of linear layers, which do not consider surrounding information. This makes hard to point out the specific reason for the classification and to evaluate quantitatively. In this paper, we introduce a novel method for explaining classification in neural networks, the Parts Detection Module. Unlike previous methods, ours is capable of determining the accurate position of the interpretable components in the image by performing upsampling and convolution stepwise, similarly to semantic segmentation. In addition to providing quantitative visual explanations, we also proposed a method to verify the validity of the quantitative explanations themselves. The experimental results prove the effectivity of our explanations."
  - id: 772
    order: 84
    poster_session: 2
    session_id: 5
    title: "From Saturation to Zero-Shot Visual Relationship Detection Using Local Context"
    authors:
      - author: "Nikolaos Gkanatsios (DeepLab)"
      - author: "Vassilis Pitsikalis (DeepLab)"
      - author: "Petros Maragos (National Technical University of Athens)"
    all_authors: "Nikolaos Gkanatsios, Vassilis Pitsikalis and Petros Maragos"
    code: "https://github.com/deeplab-ai/zs-vrd-bmvc20"
    keywords:
      - word: "Visual Relationship Detection"
      - word: "Scene Graph Generation"
      - word: "Zero-shot Classification"
      - word: "Local Context"
      - word: "Language Bias"
      - word: ""
    paper: "papers/0772.pdf"
    supp: ""
    abstract: "Visual relationship detection has been motivated by the ``insufficiency of objects to describe rich visual knowledge''. However, we find that training and testing on current popular datasets may not support such statements; most approaches can be outperformed by a naive image-agnostic baseline that fuses language and spatial features. We visualize the errors of numerous existing detectors, to discover that most of them are caused by the coexistence and penalization of antagonizing predicates that could describe the same interaction. Such annotations hurt the dataset's causality and models tend to overfit the dataset biases, resulting in a saturation of accuracy to artificially low levels.
		
We construct a simple architecture and explore the effect of using language on generalization. Then, we introduce adaptive local-context-aware classifiers, that are built on-the-fly based on the objects' categories. To improve context awareness, we mine and learn predicate synonyms, i.e. different predicates that could equivalently hold, and apply a distillation-like loss that forces synonyms to have similar classifiers and scores. The last also serves as a regularizer that mitigates the dominance of the most frequent classes, enabling zero-shot generalization. We evaluate predicate accuracy on existing and novel test scenarios to display state-of-the-art results over prior biased baselines."
  - id: 774
    order: 127
    poster_session: 3
    session_id: 8
    title: "Imitating Unknown Policies via Exploration"
    authors:
      - author: "Nathan Gavenski (PUCRS)"
      - author: "Juarez Monteiro (PUCRS)"
      - author: "Roger Granada (PUCRS)"
      - author: "Felipe Meneguzzi (PUCRS)"
      - author: "Rodrigo Barros (PUCRS)"
    all_authors: "Nathan Gavenski, Juarez Monteiro, Roger Granada, Felipe Meneguzzi and Rodrigo Barros"
    code: "https://github.com/NathanGavenski/IUPE"
    keywords:
      - word: "imitation learning"
      - word: "learning from demonstration"
      - word: "behavioral cloning"
    paper: "papers/0774.pdf"
    supp: "supp/0774_supp.zip"
    abstract: "Behavioral cloning is an imitation learning technique that teaches an agent how to behave through expert demonstrations. Recent approaches use self-supervision of fully-observable unlabeled snapshots of the states to decode state-pairs into actions.
However, the iterative learning scheme from these techniques are prone to getting stuck into bad local minima. We address these limitations incorporating a two-phase model into the original framework, which learns from unlabeled observations via exploration, substantially improving traditional behavioral cloning by exploiting (i) a sampling mechanism to prevent bad local minima, (ii) a sampling mechanism to improve exploration, and (iii) self-attention modules to capture global features. The resulting technique outperforms the previous state-of-the-art in four different environments by a large margin. "
  - id: 781
    order: 116
    poster_session: 3
    session_id: 8
    title: "MagnifierNet: Towards Semantic Adversary and Fusion for Person Re-identification"
    authors:
      - author: "Yushi Lan (Nanyang Technological University )"
      - author: "Yuan Liu (Nanyang Technological University)"
      - author: "xinchi zhou (Sensetime Limited)"
      - author: "Tian Maoqing (Sensetime Limited)"
      - author: "Xuesen Zhang (SenseTime)"
      - author: "Shuai Yi (SenseTime Group Limited)"
      - author: "Hongsheng Li (Chinese University of Hong Kong)"
    all_authors: "Yushi Lan, Yuan Liu, Xinchi Zhou, Tian Maoqing, Xuesen Zhang, Shuai Yi and Hongsheng Li"
    code: "https://github.com/NIRVANALAN/magnifiernet_reid"
    keywords:
      - word: "person re-identification"
      - word: "adversarial samples"
      - word: "metric learning"
      - word: "multi-task learning"
      - word: "image retrieval"
    paper: "papers/0781.pdf"
    supp: ""
    abstract: "Although person re-identification (ReID) has achieved significant improvement recently by enforcing part alignment, it is still a challenging task when it comes to distinguishing visually similar identities or identifying the occluded person. In these scenarios, magnifying details in each part features and selectively fusing them together may provide a feasible solution. In this work, we propose MagnifierNet, a triple-branch network which accurately mines details from whole to parts. Firstly, the holistic salient features are encoded by a global branch. Secondly, to enhance detailed representation for each semantic region, the \"Semantic Adversarial Branch\" is designed to learn from dynamically generated semantic-occluded samples during training. Meanwhile, we introduce \"Semantic Fusion Branch\" to filter out irrelevant noises by selectively fusing semantic region information sequentially. To further improve feature diversity, we introduce a novel loss function \"Semantic Diversity Loss\" to remove redundant overlaps across learned semantic representations. State-of-the-art performance has been achieved on three benchmarks by large margins. Specifically, the mAP score is improved by 6% and 5% on the most challenging CUHK03-L and CUHK03-D benchmarks."
  - id: 783
    order: 72
    poster_session: 2
    session_id: 5
    title: "Not all points are created equal - an anisotropic cost function for facial landmark location"
    authors:
      - author: "Farshid Rayhan (The University of Manchester)"
      - author: "Aphrodite Galata (The University of Manchester)"
      - author: "Timothy Cootes (University of Manchester)"
    all_authors: "Farshid Rayhan, Aphrodite Galata and Timothy Cootes"
    code: "https://github.com/farshidrayhan-uom/ananisotropic_loss"
    keywords:
      - word: "face allignment"
      - word: "facial landmark detection"
      - word: "facial keypoint detection"
    paper: "papers/0783.pdf"
    supp: "supp/0783_supp.pdf"
    abstract: "An effective approach to locating facial landmarks is to train a CNN to predict their positions directly from an image patch cropped around the face. Earlier work has shown that the choice of cost function comparing predicted with target points is important, but have tended to use the same weighting for each individual point. Since some points, such as those on boundaries, are less clearly defined than those at obvious corners, we propose an alternative cost function which uses anisotropic weights.  This penalises movement away from feature boundaries more than that along them. We demonstrate that using this cost function improves location performance and training convergence. We also address the problem of pose imbalance in datasets, suggesting a way of balancing the poses in the training samples. State of the art results on three public datasets (AFLW, WFLW and 300W) demonstrate the effectiveness of these techniques"
  - id: 789
    order: 25
    poster_session: 1
    session_id: 2
    title: "LiPo-LCD: Combining Lines and Points for Appearance-based Loop Closure Detection"
    authors:
      - author: "Joan Pep Company-Corcoles (University of the Balearic Islands)"
      - author: "Emilio Garcia-Fidalgo (University of the Balearic Islands)"
      - author: "Alberto Ortiz (University of the Balearic Islands, Spain)"
    all_authors: "Joan Pep Company-Corcoles, Emilio Garcia-Fidalgo and Alberto Ortiz"
    code: ""
    keywords:
      - word: "loop closure"
      - word: "SLAM"
      - word: "incremental bag of words"
      - word: "visual place recognition"
      - word: "BoW"
      - word: "robot vision"
      - word: "localisation"
      - word: "mapping"
      - word: ""
    paper: "papers/0789.pdf"
    supp: ""
    abstract: "Visual SLAM approaches typically depend on loop closure detection to correct the inconsistencies that may arise during the map and camera trajectory calculations, typically making use of point features for detecting and closing the existing loops. In low-textured scenarios, however, it is difficult to find enough point features and, hence, the performance of these solutions drops drastically. An alternative for human-made scenarios, due to their structural regularity, is the use of geometrical cues such as straight segments, frequently present within these environments. Under this context, in this paper we introduce LiPo-LCD, a novel appearance-based loop closure detection method that integrates lines and points. Adopting the idea of incremental Bag-of-Binary-Words schemes, we build separate BoW models for each feature, and use them to retrieve previously seen images using a late fusion strategy. Additionally, a simple but effective mechanism, based on the concept of island, groups similar images close in time to reduce the image candidate search effort. A final step validates geometrically the loop candidates by incorporating the detected lines by means of a process comprising a line feature matching stage, followed by a robust spatial verification stage, now combining both lines and points. As it is reported in the paper, LiPo-LCD compares well with several state-of-the-art solutions for a number of datasets involving different environmental conditions."
  - id: 790
    order: 171
    poster_session: 4
    session_id: 11
    title: "MDA-Net: Memorable Domain Adaptation Network for Monocular Depth Estimation"
    authors:
      - author: "Jing  Zhu (New York University)"
      - author: "Yunxiao Shi (New York University)"
      - author: "Mengwei Ren (New York University)"
      - author: "Yi Fang (New York University)"
    all_authors: "Jing  Zhu, Yunxiao Shi, Mengwei Ren and Yi Fang"
    code: ""
    keywords:
      - word: "depth estimation"
      - word: "LSTM"
      - word: "autonomous driving"
      - word: "visual perception"
      - word: ""
    paper: "papers/0790.pdf"
    supp: ""
    abstract: "Monocular depth estimation is a challenging task that aims to predict a corresponding depth map from a given single RGB image. Recent deep learning models have been proposed to predict the depth from the image by learning the alignment of deep features between the RGB image and the depth domains. In this paper, we present a novel approach, named Memorable Domain Adaptation Network (MDA-Net), to more effectively transfer domain features for monocular depth estimation by taking into account the common structure regularities (e.g., repetitive  structure  patterns,  planar  surfaces, symmetries) in domain adaptation. To this end, we introduce a new Structure-Oriented Memory (SOM) module to learn and memorize the structure-specific information between RGB image domain and the depth domain. More specifically, in the SOM module, we develop a Memorable Bank of Filters (MBF) unit to learn a set of filters that memorize the structure-aware image-depth residual pattern, and also an Attention Guided Controller (AGC) unit to control the filter selection in the MBF given image features queries. Given the query image feature, the trained SOM module is able to adaptively select the best customized filters for cross-domain feature transferring with an optimal structural disparity between image and depth. In summary, we focus on addressing this structure-specific domain adaption challenge by proposing a novel end-to-end multi-scale memorable network for monocular depth estimation. The experiments show that our MDA-Net demonstrates the superior performance compared to the existing supervised monocular depth estimation approaches on the challenging KITTI and NYU Depth V2 benchmarks."
  - id: 796
    order: 89
    poster_session: 2
    session_id: 5
    title: "Synchronous Bidirectional Learning for Multilingual Lip Reading"
    authors:
      - author: "Mingshuang Luo (Institute of Computing Technology, Chinese Academy of Sciences, University of Chinese Academy of Science)"
      - author: "Shuang Yang (ICT, CAS)"
      - author: "Xilin Chen (Institute of Computing Technology, Chinese Academy of Sciences)"
      - author: "Zitao Liu (TAL AI Lab)"
      - author: "Shiguang Shan (Institute of Computing Technology, Chinese Academy of Sciences)"
    all_authors: "Mingshuang Luo, Shuang Yang, Xilin Chen, Zitao Liu and Shiguang Shan"
    code: ""
    keywords:
      - word: "lip reading"
      - word: "multilingual"
      - word: "synchronous bidirectional learning"
      - word: "transformer"
    paper: "papers/0796.pdf"
    supp: ""
    abstract: "Lip reading has received increasing attention in recent years. This paper focuses on the synergy of multilingual lip reading. There are about as many as 7,000 languages in the world, which implies that it is impractical to train separate lip reading models with large-scale data for each language. Although each language has its own linguistic and pronunciation rules, the lip movements of all languages share similar patterns due to the common structures of human organs. Based on this idea,  we try to explore the synergized learning of multilingual lip reading in this paper, and further propose a synchronous bidirectional learning (SBL) framework for effective synergy of multilingual lip reading. We firstly introduce phonemes as our modeling units for the multilingual setting here. Phonemes are more closely related with the lip movements than the alphabet letters. At the same time, similar phonemes always lead to similar visual patterns no matter which type the target language is. Then, a novel SBL block is proposed to learn the rules for each language in a fill-in-the-blank way. Specifically, the model has to learn to infer the target unit given its bidirectional context, which could represent the composition rules of phonemes for each language. To make the learning process more targeted at each particular language, an extra task of predicting the language identity is introduced in the learning process. Finally, a thorough comparison on LRW (English) and LRW-1000 (Mandarin) is performed, which shows the promising benefits from the synergized learning of different languages and also reports a new state-of-the-art result on both datasets."
  - id: 803
    order: 191
    poster_session: 4
    session_id: 11
    title: "Robust Image Matching By Dynamic Feature Selection"
    authors:
      - author: "Hao Huang (New York University)"
      - author: "Jianchun Chen (New York University)"
      - author: "Xiang Li (New York University)"
      - author: "Lingjing Wang (New York University)"
      - author: "Yi Fang (New York University)"
    all_authors: "Hao Huang, Jianchun Chen, Xiang Li, Lingjing Wang and Yi Fang"
    code: ""
    keywords:
      - word: "image matching"
      - word: "feature selection"
      - word: "reinforcement learning"
    paper: "papers/0803.pdf"
    supp: ""
    abstract: "Estimating dense correspondences between images is a long-standing image understanding task. Most recent works introduce convolutional neural networks to extract high-level feature maps and find correspondences through feature matching. However, high-level feature maps are in low spatial resolution and therefore insufficient to provide accurate and fine-grained features to distinguish intra-class variations for correspondence matching.  To address this problem, we generate robust features by selecting and combining convolutional features at different levels/scales.  To resolve two critical issues in feature selection, i.e., how many and which levels of features to be selected, we frame the feature selection process as a sequential Markov decision-making process (MDP)and introduce an optimal selection strategy using reinforcement learning (RL) to select features.  Particularly, we define an RL environment for image matching in which individual actions are either requests for new features or terminate the selection episode by referring a matching score.  Deep neural networks are incorporated into our method and trained for decision making.  Experimental results show that our method achieves com-parable/superior performance with state-of-the-art methods on three public benchmarks, demonstrating the effectiveness of our proposed feature selection strategy."
  - id: 815
    order: 13
    poster_session: 1
    session_id: 2
    title: "Text and Style Conditioned GAN for the Generation of Offline-Handwriting Lines"
    authors:
      - author: "Brian Davis (Brigham Young University)"
      - author: "Bryan Morse (-)"
      - author: "Brian Price (Adobe)"
      - author: "Chris Tensmeyer (Adobe Research)"
      - author: "Curtis Wigington (Adobe Research)"
      - author: "Rajiv Jain (Adobe Research)"
    all_authors: "Brian Davis, Bryan Morse, Brian Price, Chris Tensmeyer, Curtis Wigington and Rajiv Jain"
    code: ""
    keywords:
      - word: "handwriting generation"
      - word: "conditional GAN"
      - word: "conditional image generation"
      - word: "few-shot image generation"
      - word: "handwriting recognition"
    paper: "papers/0815.pdf"
    supp: "supp/0815_supp.pdf"
    abstract: "This paper presents a GAN for generating images of handwritten lines 
conditioned on arbitrary text and latent style vectors. Unlike prior work, which produce stroke points or single-word images, this model generates entire lines of offline handwriting. The model produces variable-sized images by using style vectors to determine character widths. A generator network is trained with GAN and autoencoder techniques to learn style, and uses a pre-trained handwriting recognition network to induce legibility. A study using human evaluators demonstrates that the model produces images that appear to be written by a human. After training, the encoder network can extract a style vector from an image, allowing images in a similar style to be generated, but with arbitrary text."
  - id: 816
    order: 92
    poster_session: 2
    session_id: 5
    title: "LaDDer: Latent Data Distribution Modelling with a Generative Prior"
    authors:
      - author: "Shuyu Lin (University of Oxford)"
      - author: "Ronald Clark (Imperial College London)"
    all_authors: "Shuyu Lin and Ronald Clark"
    code: "https://github.com/lin-shuyu/ladder-latent-data-distribution-modelling"
    keywords:
      - word: "variational autoencoder"
      - word: "generative model"
      - word: "variational inference"
      - word: "representation learning"
      - word: "unsupervised learning"
      - word: "latent space interpolation"
    paper: "papers/0816.pdf"
    supp: "supp/0816_supp.pdf"
    abstract: "In this paper, we show that the performance of a learnt generative model is closely related to the model's ability to accurately represent the inferred latent data distribution, i.e. its topology and structural properties. We propose LaDDer to achieve accurate modelling of the latent data distribution in a variational autoencoder framework and to facilitate better representation learning. The central idea of LaDDer is a meta-embedding concept, which uses multiple VAE models to learn an embedding of the embeddings, forming a ladder of encodings. We use a non-parametric mixture as the hyper prior for the innermost VAE and learn all the parameters in a unified variational framework. From extensive experiments, we show that our LaDDer model is able to accurately estimate complex latent distribution and results in improvement in the representation quality. We also propose a novel latent space interpolation method that utilises the derived data distribution. The code and demos are available at https://github.com/lin-shuyu/ladder-latent-data-distribution-modelling."
  - id: 817
    order: 93
    poster_session: 2
    session_id: 5
    title: "Paying more Attention to Snapshots of Iterative Pruning: Improving Model Compression via Ensemble Distillation"
    authors:
      - author: "Duong Le (Ho Chi Minh City University of Technology)"
      - author: "Nhan Vo  (Ho Chi Minh City University of Technology	)"
      - author: "Nam Thoai (Ho Chi Minh City University of Technology	)"
    all_authors: "Duong Le, Nhan Vo and Nam Thoai"
    code: "https://github.com/lehduong/kesi"
    keywords:
      - word: "network pruning"
      - word: "knowledge distillation"
      - word: "ensemble learning"
      - word: ""
    paper: "papers/0817.pdf"
    supp: "supp/0817_supp.zip"
    abstract: "  Network pruning is one of the most dominant methods for reducing the heavy inference cost of deep neural networks. Existing methods often iteratively prune networks to attain high compression ratio without incurring significant loss in performance. However, we argue that conventional methods for retraining pruned networks (i.e., using small, fixed learning rate) are inadequate as they completely ignore the benefits from snapshots of iterative pruning. In this work, we show that strong ensembles can be constructed from snapshots of iterative pruning, which achieve competitive performance and vary in network structure.  Furthermore, we present a simple, general and effective pipeline that generates strong ensembles of networks during pruning with textit{large learning rate restarting}, and utilizes knowledge distillation with those ensembles to improve the predictive power of compact models. In standard image classification benchmarks such as CIFAR and Tiny-Imagenet, we advance state-of-the-art pruning ratio of structured pruning by integrating simple $ell_1$-norm filters pruning into our pipeline. Specifically, we reduce 75-80% of total parameters and 65-70% MACs of numerous variants of ResNet architectures while having comparable or better performance than that of original networks. "
  - id: 822
    order: 195
    poster_session: 4
    session_id: 11
    title: "Large Scale Photometric Bundle Adjustment"
    authors:
      - author: "Oliver J. Woodford (Snap, Inc., Santa Monica)"
      - author: "Edward Rosten  (Snap Group Ltd., London)"
    all_authors: "Oliver J. Woodford and Edward Rosten"
    code: ""
    keywords:
      - word: "bundle adjustment"
      - word: "structure from motion"
    paper: "papers/0822.pdf"
    supp: ""
    abstract: "Direct methods have shown promise on visual odometry and SLAM, leading to greater accuracy and robustness over feature-based methods. However, offline 3-d reconstruction from internet images has not yet benefited from a joint, photometric optimization over dense geometry and camera parameters. Issues such as the lack of brightness constancy, and the sheer volume of data, make this a more challenging task. This work presents a framework for jointly optimizing millions of scene points and hundreds of camera poses and intrinsics, using a photometric cost that is invariant to local lighting changes. The improvement in metric reconstruction accuracy that it confers over feature-based bundle adjustment is demonstrated on the large-scale Tanks & Temples benchmark. We further demonstrate qualitative reconstruction improvements on an internet photo collection, with challenging diversity in lighting and camera intrinsics."
  - id: 841
    order: 147
    oral_session: 7
    poster_session: 4
    session_id: 9
    title: "Weakly Paired Multi-Domain Image Translation"
    authors:
      - author: "Marc Yanlong Zhang (ETH Zurich)"
      - author: "Zhiwu Huang (ETH Zurich)"
      - author: "Danda Pani Paudel (ETH Zürich)"
      - author: "Janine Thoma (ETH Zurich)"
      - author: "Luc Van Gool (ETH Zurich)"
    all_authors: "Marc Yanlong Zhang, Zhiwu Huang, Danda Pani Paudel, Janine Thoma and Luc Van Gool"
    code: ""
    keywords:
      - word: "deep learning"
      - word: "image translation"
      - word: "weakly paired data"
      - word: "GAN"
    paper: "papers/0841.pdf"
    supp: "supp/0841_supp.pdf"
    abstract: "In this paper, we aim at studying the new problem of weakly paired multi-domain image translation. To this end, we collect a dataset that contains weakly paired images from multiple domains. Two images are considered to be weakly paired if they are captured from nearby locations and share an overlapping field of view. These images are possibly captured by two asynchronous cameras—often resulting in images from separate domains, e.g. summer and winter. Major motivations for using weakly paired images are: (i) performance improvement towards that of paired data; (ii) cheap labels and abundant data availability. For the first time in this paper, we propose a multi-domain image translation method specifically designed for weakly paired data. The proposed method consists of an attention-based generator and a two-stream discriminator that deals with misalignment between source and target images. Our method generates images in the target domain while preserving source image content, including foreground objects such as cars and pedestrians. Our extensive experiments demonstrate the superiority of the proposed method in comparison to the state-of-the-art. The new dataset and the source code are available at https://github.com/zhangma123/weaklypaired."
  - id: 859
    order: 145
    poster_session: 3
    session_id: 8
    title: "Adversarial Concurrent Training: Optimizing Robustness and Accuracy Trade-off of Deep Neural Networks"
    authors:
      - author: "Elahe Arani (Navinfo Europe )"
      - author: "Fahad Sarfraz (Navinfo Europe)"
      - author: "Bahram Zonooz (Navinfo Europe)"
    all_authors: "Elahe Arani, Fahad Sarfraz and Bahram Zonooz"
    code: ""
    keywords:
      - word: "Adversarial Robustness"
      - word: "Generalization"
      - word: "Adversarial Training"
      - word: "Deep Learning"
      - word: "Collaborative Learning"
    paper: "papers/0859.pdf"
    supp: ""
    abstract: "Adversarial training has been proven to be an effective technique for improving the adversarial robustness of models.  However, there seems to be an inherent trade-off be-tween optimizing the model for accuracy and robustness. To this end, we propose Adversarial Concurrent Training (ACT), which employs adversarial training in a collaborative learning framework whereby we train a robust model in conjunction with a natural model in a minimax game. ACT encourages the two models to align their feature space by using the task-specific decision boundaries and explore the input space more broadly. Furthermore, the natural model acts as a regularizer, enforcing priors on features that the robust model should learn.  Our analyses on the behavior of the models show that ACT leads to a robust model with lower model complexity, higher information compression in the learned representations, and high posterior entropy solutions indicative of convergence to a flatter minima.  We demonstrate the effectiveness of the proposed approach across different datasets and network architectures. On ImageNet, ACT achieves 68.20% standard accuracy and 44.29% robustness accuracy under a 100-iteration untargeted attack, improving upon the standard adversarial training method’s 65.70% standard accuracy and 42.36% robustness."
  - id: 861
    order: 193
    poster_session: 4
    session_id: 11
    title: "Self-Supervised Learning for Facial Action Unit Recognition through Temporal Consistency"
    authors:
      - author: "Liupei Lu (University of Southern California)"
      - author: "Leili Tavabi (University of Southern California)"
      - author: "Mohammad Soleymani (University of Southern California)"
    all_authors: "Liupei Lu, Leili Tavabi and Mohammad Soleymani"
    code: " https://git.io/JJSI6"
    keywords:
      - word: "self-supervised learning"
      - word: "facial action unit detection"
      - word: "temporal consistency"
      - word: "metric learning"
      - word: "representation learning"
      - word: "facial expression analysis"
    paper: "papers/0861.pdf"
    supp: "supp/0861_supp.pdf"
    abstract: "Facial expressions have inherent temporal dependencies that can be leveraged in automatic facial expression analysis from videos. In this paper, we propose a self-supervised representation learning method for facial Action Unit (AU) recognition through learning temporal consistencies in videos. To this end, we use a triplet-based ranking approach that learns to rank the frames based on their temporal distance from an anchor frame. Instead of manually labeling informative triplets, we randomly select an anchor frame along with two additional frames with predefined distances from the anchor as positive and negative. To develop an effective metric learning approach, we introduce an aggregate ranking loss by taking the sum of multiple triplet losses to allow pairwise comparisons between adjacent frames. A Convolutional Neural Network (CNN) is used as encoder to learn representations by minimizing the objective loss. We demonstrate that our encoder learns meaningful representations for AU recognition with no labels. The encoder is evaluated for AU detection on various detasets including BP4D, EmotioNet and DISFA. Our results are comparable or superior to the state-of-the-art AU recognition through self-supervised learning. "
  - id: 863
    order: 98
    poster_session: 2
    session_id: 5
    title: "Learning Non-Parametric Invariances from Data with Permanent Random Connectomes"
    authors:
      - author: "Dipan Pal (Carnegie Mellon University)"
      - author: "Akshay Chawla (CMU)"
      - author: "Marios Savvides (Carnegie Mellon University)"
    all_authors: "Dipan Pal, Akshay Chawla and Marios Savvides"
    code: ""
    keywords:
      - word: "random connections"
      - word: "invariant features"
      - word: "permanent random connectomes"
      - word: "prcns"
      - word: "nptns"
      - word: "learning invariances"
      - word: "bio-inspired networks"
    paper: "papers/0863.pdf"
    supp: "supp/0863_supp.zip"
    abstract: "Learning non-parametric invariances directly from data remains an important open problem. In this paper, we introduce a new architectural layer for convolutional networks which is capable of learning general invariances from data itself. This layer can learn invariance to non-parametric transformations and interestingly, motivates and incorporates permanent random connectomes, thereby being called Permanent Random Connectome Non-Parametric Transformation Networks (PRC-NPTN). PRC-NPTN networks are initialized with random connections (not just weights) which are a small subset of the connections in a fully connected convolution layer. Importantly, these connections in PRC-NPTNs once initialized remain permanent throughout training and testing.  Permanent random connectomes make these architectures loosely more biologically plausible than many other mainstream network architectures which require highly ordered structures. We motivate randomly initialized connections as a simple method to learn invariance from data itself while invoking invariance towards multiple nuisance transformations simultaneously. We find that these randomly initialized permanent connections have positive effects on generalization, outperform much larger ConvNet baselines and the recently proposed Non-Parametric Transformation Network (NPTN) on benchmarks such as augmented MNIST, ETH-80 and CIFAR10, that enforce learning invariances from the data itself."
  - id: 864
    order: 65
    poster_session: 2
    session_id: 5
    title: "Towards a Hypothesis on Visual Transformation based Self-Supervision"
    authors:
      - author: "Dipan Pal (Carnegie Mellon University)"
      - author: "Sreena Nallamothu (Carnegie Mellon University)"
      - author: "Marios Savvides (Carnegie Mellon University)"
    all_authors: "Dipan Pal, Sreena Nallamothu and Marios Savvides"
    code: ""
    keywords:
      - word: "self supervision"
      - word: "rotation transformation"
      - word: "rot net"
      - word: "visual transformation self supervision"
    paper: "papers/0864.pdf"
    supp: "supp/0864_supp.zip"
    abstract: "We propose the first qualitative hypothesis characterizing the behavior of visual transformation based self-supervision, called the VTSS hypothesis. Given a dataset upon which a self-supervised task is performed while predicting instantiations of a transformation, the hypothesis states that if the predicted instantiations of the transformations are already present in the dataset, then the representation learned will be less useful. The hypothesis was derived by observing a key constraint in the application of self-supervision using a particular transformation. This constraint, which we term the transformation conflict for this paper, forces a network to learn degenerative features thereby reducing the usefulness of the representation. The VTSS hypothesis helps us identify transformations that have the potential to be effective as a self-supervision task. Further, it helps to generally predict whether a particular transformation based self-supervision technique would be effective or not for a particular dataset. We provide extensive evaluations on CIFAR 10, CIFAR 100, SVHN and FMNIST confirming the hypothesis and the trends it predicts. We also propose  novel cost-effective self-supervision techniques based on translation and scale, which when combined with rotation outperform all transformations applied individually. Overall, the aim of this paper is to shed light on the phenomenon of visual transformation based self-supervision."
  - id: 877
    order: 37
    poster_session: 1
    session_id: 2
    title: "Two-Stream Spatiotemporal Compositional Attention Network for VideoQA"
    authors:
      - author: "Taiki Miyanishi (Advanced Telecommunications Research Institute International (ATR))"
      - author: "Takuya Maekawa (Osaka University)"
      - author: "Motoaki Kawanabe (Advanced Telecommunications Research Institute International (ATR))"
    all_authors: "Taiki Miyanishi, Takuya Maekawa and Motoaki Kawanabe"
    code: ""
    keywords:
      - word: "video question answering"
    paper: "papers/0877.pdf"
    supp: ""
    abstract: "This study tackles a video question answering (VideoQA), which requires spatiotemporal video reasoning. VideoQA aims to return an appropriate answer about textual questions referring to image frames in the video. In this paper, based on the observation that multiple entities and their movements in the video can be important clues for deriving the correct answer, we propose a two-stream spatiotemporal compositional attention network that achieves sophisticated multi-step spatiotemporal reasoning by using both motion and detailed appearance features. In contrast to the existing video reasoning approach that uses frame-level or clip-level appearance and motion features, our method simultaneously attends detailed appearance features of multiple entities as well as motion features guided by attending words in the textual question. Furthermore, it progressively refines internal representation and infers the answer via multiple reasoning steps. We evaluate our method on short- and long-form VideoQA benchmarks: MSVD-QA, MSRVTT-QA, and ActivityNet-QA and achieve state-of-the-art accuracy on these datasets."
  - id: 890
    order: 86
    poster_session: 2
    session_id: 5
    title: "E2ETag: An End-to-End Trainable Method for Generating and Detecting Fiducial Markers"
    authors:
      - author: "John Peace (University of Nebraska-Lincoln)"
      - author: "Eric Psota (University of Nebraska-Lincoln)"
      - author: "Yanfeng Liu (University of Nebraska-Lincoln)"
      - author: "Lance Pérez (University of Nebraska-Lincoln)"
    all_authors: "John Peace, Eric Psota, Yanfeng Liu and Lance Pérez"
    code: "https://github.com/jbpeace/E2ETag"
    keywords:
      - word: "E2ETag"
      - word: "Fiducial Marker"
      - word: "Pose Estimation"
      - word: "Superimposition"
      - word: "Simulation"
      - word: "Augmentation"
    paper: "papers/0890.pdf"
    supp: "supp/0890_supp.zip"
    supp2: "supp/0890_supp2.zip"
    abstract: "Existing fiducial markers solutions are designed for efficient detection and decoding, however, their ability to stand out in natural environments is difficult to infer from relatively limited analysis.  Furthermore, worsening performance in challenging image capture  scenarios  -  such  as  poor  exposure,  motion  blur,  and  off-axis  viewing  -  sheds light on their limitations. E2ETag introduces an end-to-end trainable method for designing fiducial markers and a complimentary detector.  By introducing back-propagatable marker augmentation and superimposition into training, the method learns to generate markers that can be detected and classified in challenging real-world environments using a fully convolutional detector network.  Results demonstrate that E2ETag outperforms existing methods in ideal conditions and performs much better in the presence of motion blur, contrast fluctuations, noise, and off-axis viewing angles.  "
  - id: 899
    order: 138
    poster_session: 3
    session_id: 8
    title: "Constrained Video Face Clustering using1NN Relations"
    authors:
      - author: "Vicky Kalogeiton (University of Oxford)"
      - author: "Andrew Zisserman (University of Oxford)"
    all_authors: "Vicky Kalogeiton and Andrew Zisserman"
    code: "http://www.robots.ox.ac.uk/~vgg/research/c1c"
    keywords:
      - word: "video face clustering"
      - word: "constrained clustering"
      - word: "constrained video head clustering"
      - word: "self-supervised constraints"
      - word: "hierachical clustering"
      - word: "first neighbor relations"
      - word: "must-link and cannot-link constraints"
    paper: "papers/0899.pdf"
    supp: ""
    abstract: "In this work, we introduce the Constrained first nearest neighbour Clustering (C1C) method for video face clustering. Using the premise that the first nearest neighbour (1NN) of an instance is sufficient to discover large chains and groupings, C1C builds upon the hierarchical clustering method FINCH by imposing must-link and cannot-link constraints acquired in a self-supervised manner. We show that adding these constraints leads to performance improvements with a low computational cost. C1C is easily scalable and does not require any training. Additionally, we introduce a new Friends dataset for evaluating the performance of face clustering algorithms. Given that most video datasets for face clustering are saturated or emphasize only the main characters, the Friends dataset is larger, contains identities for several main and secondary characters, and tackles more challenging cases as it labels also the `back of the head’. We evaluate C1C on the Big Bang Theory, Buffy, and Sherlock datasets for video face clustering, and show that it achieves the new state of the art whilst setting the baseline on Friends."
  - id: 901
    order: 44
    poster_session: 1
    session_id: 2
    title: "Bayesian Geodesic Regression on Riemannian Manifolds"
    authors:
      - author: "Youshan Zhang (Lehigh University)"
    all_authors: "Youshan Zhang"
    code: ""
    keywords:
      - word: "Bayesian"
      - word: "geodesic regression"
      - word: "Riemannian manifolds"
      - word: "shape analysis"
    paper: "papers/0901.pdf"
    supp: ""
    abstract: " Geodesic regression has been proposed for fitting the geodesic curve. However, it cannot automatically choose the dimensionality of data. In this paper, we develop a Bayesian geodesic regression model on Riemannian manifolds (BGRM) model. To avoid the overfitting problem, we add a regularization term to control the effectiveness of the model. To automatically select the dimensionality, we develop a prior for the geodesic regression model, which can automatically select the number of relevant dimensions by driving unnecessary tangent vectors to zero. To show the validation of our model, we first apply it in the 3D synthetic sphere and 2D pentagon data. We then demonstrate the effectiveness of our model in reducing the dimensionality and analyzing shape variations of human corpus callosum and mandible data."
  - id: 905
    order: 0
    oral_session: 1
    poster_session: 1
    session_id: 0
    title: "Is Face Recognition Sexist? No, Gendered Hairstyles and Biology Are"
    authors:
      - author: "Vítor Albiero (University of Notre Dame)"
      - author: "Kevin Bowyer (University of Notre Dame)"
    all_authors: "Vítor Albiero and Kevin Bowyer"
    code: ""
    keywords:
      - word: "face recognition"
      - word: "bias"
      - word: "biology"
      - word: "hairstyle"
      - word: "CNN"
      - word: "deep learning"
      - word: "arcface"
      - word: "matcher"
      - word: ""
    paper: "papers/0905.pdf"
    supp: ""
    abstract: "Recent news articles have accused face recognition of being “biased”, “sexist” or “racist”. There is consensus in the research literature that face recognition accuracy is lower for females, who often have both a higher false match rate and a higher false non- match rate. However, there is little published research aimed at identifying the cause of lower accuracy for females. For instance, the 2019 Face Recognition Vendor Test that documents lower female accuracy across a broad range of algorithms and datasets also lists “Analyze cause and effect” under the heading “What we did not do”. We present the first experimental analysis to identify major causes of lower face recognition accuracy for females on datasets where previous research has observed this result. Controlling for equal amount of visible face in the test images reverses the apparent higher false non-match rate for females. Also, principal component analysis indicates that images of two different females are inherently more similar than of two different males, potentially accounting for a difference in false match rates."
  - id: 906
    order: 18
    poster_session: 1
    session_id: 2
    title: "Intrinsic Decomposition of Document Images In-the-Wild"
    authors:
      - author: "Sagnik Das (Stony Brook University)"
      - author: "Hassan Sial (Computer Vision Center)"
      - author: "Ke Ma (Stony Brook University)"
      - author: "Ramón Baldrich (Computer Vision Center (Ph.D.))"
      - author: "Maria Vanrell (Computer Vision Center (Ph.D.))"
      - author: "Dimitris   Samaras (Stony Brook University)"
    all_authors: "Sagnik Das, Hassan Sial, Ke Ma, Ramón Baldrich, Maria Vanrell and Dimitris   Samaras"
    code: "https://github.com/cvlab-stonybrook/DocIIW"
    keywords:
      - word: "Intrinsic"
      - word: "Shading Removal"
      - word: "Shadows"
      - word: "Documents"
      - word: "OCR"
    paper: "papers/0906.pdf"
    supp: ""
    abstract: "Automatic document content processing is affected by artifacts caused by the shape of the paper, non-uniform and diverse color of lighting conditions. Fully-supervised methods on real data are impossible due to the large amount of data needed. Hence, the current state of the art deep learning models are trained on fully or partially synthetic images. However, document shadow or shading removal results still suffer because: (a) prior methods rely on uniformity of local color statistics, which limit their application on real-scenarios with complex document shapes and textures and; (b) synthetic or hybrid datasets with non-realistic, simulated lighting conditions are used to train the models.  In this paper we tackle these problems with our two main contributions. First, a physically constrained learning-based method that directly estimates document reflectance based on intrinsic image formation which generalizes to challenging illumination conditions. Second, a new dataset that clearly  improves previous synthetic ones, by adding a large range of realistic shading and diverse multi-illuminant conditions, uniquely customized to deal with documents in-the-wild.
The proposed architecture works in two steps. First, a white balancing module neutralizes the color of the illumination on the input image. Based on the proposed multi-illuminant dataset we achieve a good white-balancing in really difficult conditions. Second, the shading separation module accurately disentangles the shading and paper material in a self-supervised manner where only the synthetic texture is used as a weak training signal (obviating the need for very costly ground truth with disentangled  versions of shading and reflectance). The proposed approach leads to significant generalization of document reflectance estimation in real scenes with challenging illumination. We extensively evaluate on the real benchmark datasets available for intrinsic image decomposition and document shadow removal tasks. Our reflectance estimation scheme, when used as a pre-processing step of an OCR pipeline, shows a 21 % improvement of character error rate (CER), thus, proving the practical applicability."
  - id: 907
    order: 38
    poster_session: 1
    session_id: 2
    title: "WHENet: Real-time Fine-Grained Estimation for Wide Range Head Pose"
    authors:
      - author: "Yijun Zhou (Huawei Technologies Canada)"
      - author: "James Gregson (Huawei Technologies Canada)"
    all_authors: "Yijun Zhou and James Gregson"
    code: "https://github.com/Ascend-Research/HeadPoseEstimation-WHENet"
    keywords:
      - word: "head pose"
      - word: "face"
      - word: "head"
      - word: "rotation"
      - word: ""
    paper: "papers/0907.pdf"
    supp: "supp/0907_supp.zip"
    abstract: "We present an end-to-end head-pose estimation network designed to predict Euler
angles through the full range head yaws from a single RGB image. Existing ethods
perform well for frontal views but few target head pose from all viewpoints. This has applications in autonomous driving and retail. Our network builds on multi-loss approaches with changes to loss functions and training strategies adapted to wide range estimation. Additionally, we extract ground truth labelings of anterior views from a current panoptic dataset for the first time. The resulting Wide Headpose Estimation Network (WHENet) is the first fine-grained modern method applicable to the full-range of head yaws (hence wide) yet also meets or beats state-of-the-art methods for frontal head pose estimation. Our network is compact and efficient for mobile devices and applications. We will release the trained model to aid future researchers in this important topic."
  - id: 918
    order: 184
    poster_session: 4
    session_id: 11
    title: "POMP: Pomcp-based Online Motion Planning for active visual search in indoor environments"
    authors:
      - author: "Yiming Wang (IIT)"
      - author: "Francesco  Giuliari (University of Verona)"
      - author: "Riccardo Berra (University of Verona)"
      - author: "Alberto Castellini (University of Verona)"
      - author: "Alessio Del Bue (Istituto Italiano di Tecnologia (IIT))"
      - author: "Alessandro Farinelli (University of Verona, Italy)"
      - author: "Marco Cristani (University of Verona)"
      - author: "Francesco Setti (University of Verona)"
    all_authors: "Yiming Wang, Francesco  Giuliari, Riccardo Berra, Alberto Castellini, Alessio Del Bue, Alessandro Farinelli, Marco Cristani and Francesco Setti"
    code: ""
    keywords:
      - word: "Object Recognition Active Visual Search Partially Observable Markov Decision Process Monte Carlo Tree Search"
    paper: "papers/0918.pdf"
    supp: ""
    abstract: "In this paper we focus on the problem of learning an optimal policy for Active Visual Search (AVS) of objects in known indoor environments with an online setup. Our POMP method uses as input the current pose of an agent (e.g. a robot) and a RGB-D frame. The task is to plan the next move that brings the agent closer to the target object. We model this problem as a Partially Observable Markov Decision Process solved by a Monte-Carlo planning approach. This allows us to make decisions on the next moves by iterating over the known scenario at hand, exploring the environment and searching for the object at the same time.  Differently from the current state of the art in Reinforcement Learning, POMP does not require extensive and expensive (in time and computation) labelled data so being very agile in solving AVS in small and medium real scenarios. We only require the information of the floormap of the environment, an information usually available or that can be easily extracted from an a priori single exploration run. We validate our method on the publicly available AVD benchmark, achieving an average success rate of 0.76 with an average path length of 17.1, performing close to the state of the art but without any training needed. Additionally, we show experimentally the robustness of our method when the quality of the object detection goes from ideal to faulty."
  - id: 928
    order: 90
    poster_session: 2
    session_id: 5
    title: "Mish: A Self Regularized Non-Monotonic Activation Function"
    authors:
      - author: "Diganta Misra (Kalinga Institute of Industrial Technology)"
    all_authors: "Diganta Misra"
    code: "https://github.com/digantamisra98/Mish"
    keywords:
      - word: "activation functions"
      - word: "non-linear dynamics"
      - word: "loss landscapes"
      - word: ""
    paper: "papers/0928.pdf"
    supp: ""
    abstract: "We propose Mish, a novel self-regularized non-monotonic activation function which can be mathematically defined as: f(x) = xtanh(softplus(x)). As activation functions play a crucial role in the performance and training dynamics in neural networks, we validated experimentally on several well-known benchmarks against the best combinations of architectures and activation functions. We also observe that data augmentation techniques have a favorable effect on benchmarks like ImageNet-1k and MS-COCO across multiple architectures. For example, Mish outperformed Leaky ReLU on YOLOv4 with a CSP-DarkNet-53 backbone on average precision (AP-50 val) by 2.1% in MS-COCO object detection and ReLU on ResNet-50 on ImageNet-1k in Top-1 accuracy by 1% while keeping all other network parameters and hyperparameters constant. Furthermore, we explore the mathematical formulation of Mish in relation with the Swish family of functions and propose an intuitive understanding on how the first derivative behavior may be acting as a regularizer helping the optimization of deep neural networks. Code is publicly available at https://github.com/digantamisra98/Mish."
  - id: 962
    order: 139
    poster_session: 3
    session_id: 8
    title: "Thoracic Disease Identification and Localization using Distance Learning and Region Verification"
    authors:
      - author: "Cheng Zhang (The Ohio State University)"
      - author: "Francine Chen (FX Palo Alto Laboratory)"
      - author: "Yan-Ying Chen (FX Pal)"
    all_authors: "Cheng Zhang, Francine Chen and Yan-Ying Chen"
    code: ""
    keywords:
      - word: "Chest X-ray analysis"
      - word: "disease classification"
      - word: "disease localization"
      - word: "triplet learning"
      - word: "region verification"
      - word: "multi-class activation map."
    paper: "papers/0962.pdf"
    supp: ""
    abstract: "The identification and localization of diseases in medical images using deep learning models have recently attracted significant interest. Existing methods only consider training the networks with each image independently and most leverage an activation map for disease localization. In this paper, we propose an alternative approach that learns discriminative features among triplets of images and cyclically trains on region features to verify whether attentive regions contain information indicative of a disease. Concretely, we adapt a distance learning framework for multi-label disease classification to differentiate subtle disease features. Additionally, we feed back the features of the predicted class-specific regions to a separate classifier during training to better verify the localized diseases. Our model can achieve state-of-the-art classification performance on the challenging Chest-Xray14 dataset, and our ablation studies indicate that both distance learning and region verification contribute to overall classification performance. Moreover, the distance learning and region verification modules can capture essential information for better localization than baseline models without these modules. "
  - id: 975
    order: 181
    poster_session: 4
    session_id: 11
    title: "Unsupervised Monocular Depth Estimation with Multi-Baseline Stereo"
    authors:
      - author: "Saad Imran (Korea Advanced Institute of Science and Technology)"
      - author: "Muhammad Umar Karim Khan (Korea Advanced Institute of Science and Technology)"
      - author: "Sikander Mukaram (Korea Advanced Institute of Science and Technology (KAIST))"
      - author: "Chong-Min Kyung (Korea Advanced Institute of Science and Technology)"
    all_authors: "Saad Imran, Muhammad Umar Karim Khan, Sikander Mukaram and Chong-Min Kyung"
    code: "https://github.com/saadi297/MultiBaselineDepth"
    keywords:
      - word: "Unsupervised Monocular Depth"
      - word: "Small-Baseline"
      - word: "Wide-Baseline"
      - word: "Multi-Baseline"
      - word: "Stereo"
      - word: ""
    paper: "papers/0975.pdf"
    supp: "supp/0975_supp.zip"
    abstract: "Unsupervised deep learning methods have shown promising performance for single-image depth estimation. Since most of these methods use binocular stereo pairs for self-supervision, the depth range is generally limited. Small-baseline stereo pairs provide small depth range but handle occlusions well. On the other hand, stereo images acquired with a wide-baseline rig cause occlusions-related errors in the near range but estimate depth well in the far range. In this work, we propose to integrate the advantages of the small and wide baselines. By training the network using three horizontally aligned views, we obtain accurate depth predictions for both close and far ranges. Our strategy allows to infer multi-baseline depth from a single image. This is unlike previous multi-baseline systems which employ more than two cameras. The qualitative and quantitative results show the superior performance of multi-baseline approach over previous stereo-based monocular methods. For 0.1 to 80 meters depth range, our approach decreases the absolute relative error of depth by 24% compared to Monodepth2. Our approach provides 21 frames per second on a single Nvidia1080 GPU, making it useful for practical applications."
  - id: 977
    order: 159
    poster_session: 4
    session_id: 11
    title: "Robust Unsupervised Cleaning of Underwater Bathymetric Point Cloud Data"
    authors:
      - author: "Cong Chen (908945500)"
      - author: "Abel  Gawel (ETH Zurich)"
      - author: "Stephen Krauss (Virginia Tech)"
      - author: "Yuliang Zou (Virginia Tech)"
      - author: "Amos Abbott (Virginia Tech)"
      - author: "Daniel Stilwell (Virginia Tech.)"
    all_authors: "Cong Chen, Abel  Gawel, Stephen Krauss, Yuliang Zou, Amos Abbott and Daniel Stilwell"
    code: ""
    keywords:
      - word: "Point cloud cleaning"
      - word: "underwater bathymetric data"
      - word: "point cloud tensorization"
      - word: "variational Bayesian inference"
      - word: ""
    paper: "papers/0977.pdf"
    supp: ""
    abstract: "This paper presents a novel unified one-stage unsupervised learning framework forpoint cloud cleaning of noisy partial data from underwater side-scan sonars. By combining a swath-based point cloud tensor representation, an adaptive multi-scale feature encoder, and a generative Bayesian framework, the proposed method provides robust sonarpoint cloud denoising, completion, and outlier removal simultaneously. The condensed swath-based tensor representation preserves point cloud of underlying three-dimensionalgeometry of point cloud by reconstructing spatial and temporal correlation of sonar data.The adaptive multi-scale feature encoder distinguishes noisy  partial  tensor  data without handcrafted feature labeling by utilizing CANDECOMP/PARAFAC tensor factorization. Each local embedded outlier feature under various scales is aggregated into aglobal context by a generative Bayesian framework. The model is automatically inferredby a variational Bayesian, without parameter tuning and model pre-training. Extensive experiments on large scale synthetic and real data demonstrates the robustness against environmental perturbation. The proposed algorithm compares favourably with existing methods."
  - id: 979
    order: 82
    poster_session: 2
    session_id: 5
    title: "Learning to Abstract and Predict Human Actions"
    authors:
      - author: "Romero Morais (Deakin University)"
      - author: "Vuong Le (Deakin University)"
      - author: "Truyen Tran (Deakin University)"
      - author: "Svetha Venkatesh (Deakin University)"
    all_authors: "Romero Morais, Vuong Le, Truyen Tran and Svetha Venkatesh"
    code: "https://github.com/RomeroBarata/hierarchical_action_prediction"
    keywords:
      - word: "action prediction"
      - word: "hierarchical"
      - word: "dataset"
      - word: ""
    paper: "papers/0979.pdf"
    supp: "supp/0979_supp.zip"
    abstract: "Human activities are naturally structured as hierarchies unrolled over time. For action prediction, temporal relations in event sequences are widely exploited by current methods while their semantic coherence across different levels of abstraction has not been well explored. In this work we model the hierarchical structure of human activities in videos and demonstrate the power of such structure in action prediction. We propose Hierarchical Encoder-Refresher-Anticipator, a multi-level neural machine that can learn the structure of human activities by observing a partial hierarchy of events and roll-out such structure into a future prediction in multiple levels of abstraction. We also introduce a new coarse-to-fine action annotation on the Breakfast Actions videos to create a comprehensive, consistent, and cleanly structured video hierarchical activity dataset. Through our experiments, we examine and rethink the settings and metrics of activity prediction tasks toward unbiased evaluation of prediction systems, and demonstrate the role of hierarchical modeling toward reliable and detailed long-term action forecasting."
  - id: 1025
    order: 137
    poster_session: 3
    session_id: 8
    title: "DALE : Dark Region-Aware Low-light Image Enhancement"
    authors:
      - author: "DoKyeong Kwon (Chung-Ang Univ., Korea)"
      - author: "Guisik Kim (Chung-Ang Univ., Korea)"
      - author: "Junseok Kwon (Chung-Ang Univ., Korea)"
    all_authors: "DoKyeong Kwon, Guisik Kim and Junseok Kwon"
    code: ""
    keywords:
      - word: "Low-light Image Enhancement"
      - word: "Dark Region-Aware"
      - word: "Visual Attention"
      - word: ""
    paper: "papers/1025.pdf"
    supp: ""
    abstract: "In this paper, we present a novel low-light image enhancement method called dark region-aware low-light image enhancement (DALE), where dark regions are accurately recognized by the proposed visual attention module and their brightness are intensively enhanced. Our method can estimate the visual attention in an efficient manner using super-pixels without any complicated process. Thus, the method can preserve the color, tone, and brightness of original images and prevents normally illuminated areas of the images from being saturated and distorted. Experimental results show that our method accurately identifies dark regions via the proposed visual attention, and qualitatively and quantitatively outperforms state-of-the-art methods."
