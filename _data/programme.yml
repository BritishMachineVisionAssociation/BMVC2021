keynotes:
  - name: Prof. Davide Scaramuzza
    id: keynote-3
    img: /assets/images/mugshots/davide_scaramuzza.jpg
    url: "http://rpg.ifi.uzh.ch/people_scaramuzza.html"
    bio: "Davide Scaramuzza is a Professor of Robotics and Perception at the University of Zurich, where he does research at the intersection of robotics, computer vision, and machine learning, aiming to enable autonomous, agile navigation of micro drones using both standard and neuromorphic event-based cameras. He pioneered autonomous, vision-based navigation of drones, which inspired the NASA Mars helicopter and has served as a consultant for the United Nations’ International Atomic Energy Agency’s Fukushima Action Plan on Nuclear Safety. For his research contributions, he won prestigious awards, such as a European Research Council (ERC) Consolidator Grant, the IEEE Robotics and Automation Society Early Career Award, an SNF-ERC Starting Grant, a Google Research Award, and several paper awards. In 2015, he co-founded Zurich-Eye, today Facebook Zurich, which developed the world leading virtual-reality headset, Oculus Quest. Many aspects of his research have been prominently featured in wider media, such as The New York Times, BBC News, Forbes, Discovery Channel."
    title: ""
    job: ETH Zürich
    abstract: ""
    slides-id: 
  - name: Prof. Andrew Zisserman
    id: keynote-1
    img: /assets/images/mugshots/andrew_zisserman.jpg
    url: "https://www.robots.ox.ac.uk/~az/"
    bio: "Andrew Zisserman, FRS, is the Professor of Computer Vision Engineering and the Royal Society Research Professor in the Department of Engineering Science at the University of Oxford, and the founder of the Visual Geometry Group (VGG). Since 2014, he is also affiliated with DeepMind. Prof. Zisserman is one of the principal architects of modern computer vision, and is known internationally for his pioneering work in multiple view geometry, visual recognition, and large scale retrieval in images and video. His papers have won many best paper awards at international conferences (such as CVPR, ICCV and BMVC), including the IEEE Marr prize three times. Prof. Zisserman received a Technical Emmy Award (with the company 2d3) for camera-tracking software in 2002. He also received test of time awards - Longuet-Higgins and Helmholtz prizes - on four occasions, the BMVA Distinguished Fellowship (2008), the IEEE PAMI Distinguished Researcher award (2013), Royal Society Milner Award (2017)."
    title: ""
    job: University of Oxford
    abstract: ""
    slides-id: 
  - name: Prof. Daphne Koller
    id: keynote-2
    img: /assets/images/mugshots/daphne_koller.jpg
    url: "https://ai.stanford.edu/~koller/index.html"
    bio: "Daphne Koller is CEO and Founder of insitro, a machine-learning enabled drug discovery company. Daphne is also co-founder of Engageli, was the Rajeev Motwani Professor of Computer Science at Stanford University, where she served on the faculty for 18 years, the co-CEO and President of Coursera, and the Chief Computing Officer of Calico, an Alphabet company in the healthcare space. She is the author of over 200 refereed publications appearing in venues such as Science, Cell, and Nature Genetics. Daphne was recognized as one of TIME Magazine’s 100 most influential people in 2012. She received the MacArthur Foundation Fellowship in 2004 and the ACM Prize in Computing in 2008. She was inducted into the National Academy of Engineering in 2011 and elected a fellow of the American Association for Artificial Intelligence in 2004, the American Academy of Arts and Sciences in 2014, and the International Society of Computational Biology in 2017."
    title: ""
    job: insitro
    abstract: ""
    slides-id: 
  - name: Prof. Katerina Fragkiadaki
    id: keynote-4
    img: /assets/images/mugshots/katerina_fragkiadaki.jpg
    url: "https://www.cs.cmu.edu/~katef/"
    bio: "Katerina Fragkiadaki is an Assistant Professor in the Machine
Learning Department in Carnegie Mellon University. She received her
Ph.D. from University of Pennsylvania and was a postdoctoral fellow in
UC Berkeley and Google research after that.  Her work is on learning
visual representations with little supervision and on combining
spatial reasoning in deep visual learning.  Her  group develops
algorithms for mobile computer vision,  learning of physics and common
sense for agents that move around and interact with the world.  Her
work has been awarded with a best Ph.D. thesis award,  an NSF CAREER
award, AFOSR YIP award, a DARPA Young Investigator award, Google, TRI, Amazon and Sony faculty research awards."
    title: ""
    job: Carnegie Mellon University
    abstract: ""
    slides-id: 

old-keynotes:
  - name: Prof. Sanja Fidler
    id: keynote-1
    img: /assets/images/mugshots/sanja_fidler.jpg
    url: "https://www.cs.utoronto.ca/~fidler/"
    title: "AI for 3D Content Creation"
    job: Faculty at University of Toronto and a Director of AI at NVIDIA
    abstract: "3D content is key in several domains such as architecture, film, gaming, and robotics. However, creating 3D content can be very time consuming -- the artists need to sculpt high quality 3d assets, compose them into large worlds, and bring these worlds to life by writing behaviour models that “drives” the characters around in the world. In this talk, I’ll discuss some of our recent efforts on introducing automation in the 3D content creation process using A.I."
    slides-id: 38934739
  - name: Prof. Laura Leal-Taixé
    id: keynote-4
    img: /assets/images/mugshots/lealtaixe.jpg
    url: "https://dvl.in.tum.de/team/lealtaixe/"
    title: "Multiple Object Tracking: Promising Directions and Data Privacy"
    job: Faculty at the Technical University of Munich
    abstract: "Is the performance on multiple object tracking benchmarks saturating? What seem to be promising new research directions? I will first analyze the performance of recent methods, and show how powerful a simple object detector regressor can be for MOT. This analysis will uncover the main challenges that as a community we are still not tackling.
I will then move towards end-to-end learning for MOT, and explain how graph neural networks can open the path to new research directions. Finally, I will show how computer vision tools can help us in handling data privacy in MOT benchmarks."
    slides-id: 38934740
  - name: Prof. Kate Saenko
    id: keynote-3 
    img: /assets/images/mugshots/kate_saenko.png
    url: "http://ai.bu.edu/ksaenko.html"
    title: "Mitigating Dataset Bias"
    job: Faculty at Boston University and MIT-IBM Watson AI Lab
    slides-id: 38934741
    abstract: "Deep Learning has made exciting progress on many computer vision problems such as object recognition in images and video. However, it has relied on large datasets that can be expensive and time-consuming to collect and label. Datasets can also suffer from “dataset bias,” which happens when the training data is not representative of the future deployment domain. Dataset bias is a major problem in computer vision -- even the most powerful deep neural networks fail to generalize to out-of-sample data. A classic example of this is when a network trained to classify handwritten digits fails to recognize typed digits, but this problem happens in many situations, such as new geographic locations, changing demographics, and simulation-to-real learning. Can we solve dataset bias with only a limited amount of supervision? Yes, this is possible under some assumptions. I will give an overview of current solutions based on domain adaptation of deep learning models and point out several assumptions they make and situations they fail to handle. I will also describe recent efforts to improve adaptation by using unlabeled data to learn better features, with ideas from self-supervised learning."
  - name: Prof. Andrew Davison
    id: keynote-2
    img: /assets/images/mugshots/andy_davison.jpg
    url: "https://www.doc.ic.ac.uk/~ajd/"
    title: "Towards Graph-Based Spatial AI"
    job: Faculty at Imperial College and Head of the Dyson Robitics Lab
    slides-id: 38934742
    abstract: "To enable the next generation of smart robots and devices which can
truly interact with their environments, Simultaneous Localisation and
Mapping (SLAM) will progressively develop into a vision-driven
geometric and semantic `Spatial AI' perception capability which gives
devices the real-time dynamic model in which to reason in intuitive
and intelligent ways about their actions.  A fundamental issue is the
algorithmic architecture which will enable estimation and machine
learning components to come together to enable efficient, incremental
updating of scene representation, and I believe that graph strucures
where storage and computation come together will be the key. New
computing and sensing hardware is now becoming available which makes
research in this direction a reality."


